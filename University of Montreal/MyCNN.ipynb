{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction:\n",
    "\n",
    "This notebook implements the LeNet5 neural network architecture using Theano to calssify MNIST dataset.  \n",
    "\n",
    "LeNet5 is a convolutional neural network, good for classifying images. \n",
    "\n",
    "CNN consists of thre main phases:\n",
    "1. Convolutional layer\n",
    "2. RELU\n",
    "3. Pooling (used for dimensionality reduction)\n",
    "\n",
    "==> Finally, the fully conntected layer has the ability to connect the whole results to be able to classify the whole image\n",
    "\n",
    "One of the main disadvantages of CNN is it belongs to unsupervised learning approaches hence it requires huge number of images to be well trained .. \n",
    "\n",
    "### Model assumptions:\n",
    "\n",
    " - LeNetConvPool doesn't implement location-specific gain and bias parameters\n",
    " - LeNetConvPool doesn't implement pooling by average, it implements pooling\n",
    "   by max.\n",
    " - Digit classification is implemented with a logistic regression rather than\n",
    "   an RBF network\n",
    " - LeNet5 was not fully-connected convolutions at second layer\n",
    " \n",
    "As a prerequisite you are expected to classify MNIST dataset using Logistic regresion (utilized in layer3 - to generate the output) and MLP as we gonna utilize these previous implementations .. \n",
    "\n",
    "The topology of the constructed network is as follows (one input layer, single hidden layer and one output layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.nnet import conv2d\n",
    "\n",
    "from logistic_sgd import LogisticRegression, load_data\n",
    "from mlp import HiddenLayer\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling will be implemented in this notebook .. \n",
    "in general the pooling step is to select the optimal (minimum) number of features from the whole number of input features .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Pool step .. \n",
    "\n",
    "class LeNetConvPoolLayer(object):\n",
    "    def __init__(self, rng, input, filter_shape, image_shape, poolsize=(2, 2)):\n",
    "        ## filter_shape: (number of filters, num input feature maps,filter height, filter width)\n",
    "        ## image_shape: (batch size, num input feature maps, image height, image width)\n",
    "        assert image_shape[1] == filter_shape[1]  ## assert statement helps you find bugs more quickly and with less pain. \n",
    "        self.input = input\n",
    "        \n",
    "        # inputs to each hidden unit ==> \"num input feature maps * filter height * filter width\"\n",
    "        fan_in = np.prod(filter_shape[1:])\n",
    "        \n",
    "        # each unit in the lower layer receives a gradient from:\n",
    "        # \"num output feature maps * filter height * filter width\" / pooling size\n",
    "        \n",
    "        fan_out = (filter_shape[0] * np.prod(filter_shape[2:]) // np.prod(poolsize))\n",
    "        \n",
    "        # initialize weights with random weights .. tanh activation function is utilized in generating the weights\n",
    "        W_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        self.W = theano.shared(\n",
    "            np.asarray(rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "                dtype=theano.config.floatX),borrow=True)\n",
    "        \n",
    "        \n",
    "         # the bias is a 1D tensor -- one bias per output feature map\n",
    "        b_values = np.zeros((filter_shape[0],), dtype=theano.config.floatX)\n",
    "        self.b = theano.shared(value=b_values, borrow=True)\n",
    "\n",
    "         # convolve input feature maps with filters\n",
    "        conv_out = conv2d(\n",
    "            input=input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            input_shape=image_shape)\n",
    "        \n",
    "        \n",
    "         # pool each feature map individually, using maxpooling\n",
    "        pooled_out = pool.pool_2d(\n",
    "            input=conv_out,\n",
    "            ds=poolsize,\n",
    "            ignore_border=True)\n",
    "        \n",
    "         # pool each feature map individually, using maxpooling\n",
    "        pooled_out = pool.pool_2d(   ## pool from theano lib\n",
    "            input=conv_out,\n",
    "            ds=poolsize,\n",
    "            ignore_border=True\n",
    "        )\n",
    "        \n",
    "         # add the bias term. Since the bias is a vector (1D array), we first\n",
    "        # reshape it to a tensor of shape (1, n_filters, 1, 1). Each bias will\n",
    "        # thus be broadcasted across mini-batches and feature map width & height\n",
    "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "\n",
    "        # store parameters of this layer\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower-layers are composed to alternating convolution and max-pooling layers. The upper-layers however are fully-connected and correspond to a traditional MLP (hidden layer + logistic regression)..\n",
    "\n",
    "From an implementation point of view, this means lower-layers operate on 4D tensors. These are then\n",
    "flattened to a 2D matrix of rasterized feature maps, to be compatible with our previous MLP implementation .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_lenet5(learning_rate=0.1, n_epochs=200, dataset='mnist.pkl.gz', nkerns=[20, 50], batch_size=500):\n",
    "    rng = np.random.RandomState(23455)\n",
    "    datasets = load_data(dataset)\n",
    "    \n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    \n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "    n_train_batches //= batch_size\n",
    "    n_valid_batches //= batch_size\n",
    "    n_test_batches //= batch_size\n",
    "    \n",
    "     ###############\n",
    "    # MODEL BUILDING #\n",
    "    ###############\n",
    "    \n",
    "    \n",
    "     # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    x = T.matrix('x')   # the data is presented as rasterized images\n",
    "    y = T.ivector('y')  # the labels are presented as 1D vector of [int] labels\n",
    "    \n",
    "    \n",
    "    # Reshape matrix of rasterized images of shape (batch_size, 28 * 28)\n",
    "    # to a 4D tensor, compatible with our LeNetConvPoolLayer\n",
    "    # (28, 28) is the size of MNIST images.\n",
    "    layer0_input = x.reshape((batch_size, 1, 28, 28))\n",
    "    \n",
    "     # Construct the first convolutional pooling layer:\n",
    "    # filtering reduces the image size to (28-5+1 , 28-5+1) = (24, 24)\n",
    "    # maxpooling reduces this further to (24/2, 24/2) = (12, 12)\n",
    "    # 4D output tensor is thus of shape (batch_size, nkerns[0], 12, 12)\n",
    "    layer0 = LeNetConvPoolLayer(\n",
    "        rng,\n",
    "        input=layer0_input,\n",
    "        image_shape=(batch_size, 1, 28, 28),\n",
    "        filter_shape=(nkerns[0], 1, 5, 5),\n",
    "        poolsize=(2, 2))\n",
    "\n",
    "    \n",
    "    # Construct the second convolutional pooling layer\n",
    "    # filtering reduces the image size to (12-5+1, 12-5+1) = (8, 8)\n",
    "    # maxpooling reduces this further to (8/2, 8/2) = (4, 4)\n",
    "    # 4D output tensor is thus of shape (batch_size, nkerns[1], 4, 4)\n",
    "    layer1 = LeNetConvPoolLayer(\n",
    "        rng,\n",
    "        input=layer0.output,\n",
    "        image_shape=(batch_size, nkerns[0], 12, 12),\n",
    "        filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
    "        poolsize=(2, 2))\n",
    "    \n",
    "    # the HiddenLayer being fully-connected, it operates on 2D matrices of\n",
    "    # shape (batch_size, num_pixels) (i.e matrix of rasterized images).\n",
    "    # This will generate a matrix of shape (batch_size, nkerns[1] * 4 * 4),\n",
    "    # or (500, 50 * 4 * 4) = (500, 800) with the default values.\n",
    "    layer2_input = layer1.output.flatten(2)\n",
    "\n",
    "      # construct a fully-connected sigmoidal layer\n",
    "    layer2 = HiddenLayer(\n",
    "        rng,\n",
    "        input=layer2_input,\n",
    "        n_in=nkerns[1] * 4 * 4,\n",
    "        n_out=500,\n",
    "        activation=T.tanh)\n",
    "    \n",
    "    # classify the values of the fully-connected sigmoidal layer\n",
    "    layer3 = LogisticRegression(input=layer2.output, n_in=500, n_out=10)\n",
    "    \n",
    "    # the cost we minimize during training is the -ve log likelihood of the model\n",
    "    cost = layer3.negative_log_likelihood(y)\n",
    "\n",
    "    \n",
    "     # create a function to compute the mistakes that are made by the model\n",
    "    test_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]})\n",
    "    \n",
    "    ## model validation\n",
    "    validate_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]})\n",
    "    \n",
    "    ## hence gradients will be calculated in terms of ALL model parameters .. \n",
    "    # create a list of all model parameters to be fit by gradient descent\n",
    "    params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "    \n",
    "     # create a list of gradients for all model parameters\n",
    "    grads = T.grad(cost, params)\n",
    "    \n",
    "      # train_model is a function that updates the model parameters by\n",
    "    # SGD Since this model has many parameters, it would be tedious to\n",
    "    # manually create an update rule for each model parameter. We thus\n",
    "    # create the updates list by automatically looping over all\n",
    "    # (params[i], grads[i]) pairs.\n",
    "    updates = [(param_i, param_i - learning_rate * grad_i) for param_i, grad_i in zip(params, grads)]\n",
    "    \n",
    "    train_model = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]})\n",
    "    \n",
    "     ###############\n",
    "    # MODEL TRAINING #\n",
    "    ###############\n",
    "    \n",
    "    # early-stopping parameters\n",
    "    patience = 10000  # look as this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is\n",
    "                           # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = np.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "    this_validation_loss = 0\n",
    "    \n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if iter % 100 == 0:\n",
    "                print('training @ iter = ', iter)\n",
    "            cost_ij = train_model(minibatch_index)\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i) for i in range(n_valid_batches)]\n",
    "                this_validation_loss = np.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                      (epoch, minibatch_index + 1, n_train_batches,\n",
    "                       this_validation_loss * 100.))\n",
    "            \n",
    "             # if we got the best validation score until now\n",
    "            if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "                    \n",
    "                    # test it on the test set\n",
    "                    test_losses = [test_model(i) for i in range(n_test_batches)]\n",
    "                    test_score = np.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches, test_score * 100.))\n",
    "            \n",
    "            ## stopping condition\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "            \n",
    "    end_time = timeit.default_timer()\n",
    "    print('Best validation score of %f %% obtained at iteration %i, '\n",
    "          'with test performance %f %%' % (best_validation_loss * 100., best_iter + 1, test_score * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "('training @ iter = ', 0)\n",
      "     epoch 1, minibatch 1/100, test error of best model 61.990000 %\n",
      "epoch 1, minibatch 100/100, validation error 9.230000 %\n",
      "('training @ iter = ', 100)\n",
      "epoch 2, minibatch 100/100, validation error 6.180000 %\n",
      "('training @ iter = ', 200)\n",
      "epoch 3, minibatch 100/100, validation error 4.640000 %\n",
      "('training @ iter = ', 300)\n",
      "epoch 4, minibatch 100/100, validation error 3.500000 %\n",
      "('training @ iter = ', 400)\n",
      "epoch 5, minibatch 100/100, validation error 3.020000 %\n",
      "('training @ iter = ', 500)\n",
      "epoch 6, minibatch 100/100, validation error 2.780000 %\n",
      "('training @ iter = ', 600)\n",
      "epoch 7, minibatch 100/100, validation error 2.480000 %\n",
      "('training @ iter = ', 700)\n",
      "epoch 8, minibatch 100/100, validation error 2.290000 %\n",
      "('training @ iter = ', 800)\n",
      "epoch 9, minibatch 100/100, validation error 2.160000 %\n",
      "('training @ iter = ', 900)\n",
      "epoch 10, minibatch 100/100, validation error 1.970000 %\n",
      "('training @ iter = ', 1000)\n",
      "epoch 11, minibatch 100/100, validation error 1.880000 %\n",
      "('training @ iter = ', 1100)\n",
      "epoch 12, minibatch 100/100, validation error 1.790000 %\n",
      "('training @ iter = ', 1200)\n",
      "epoch 13, minibatch 100/100, validation error 1.760000 %\n",
      "('training @ iter = ', 1300)\n",
      "epoch 14, minibatch 100/100, validation error 1.710000 %\n",
      "('training @ iter = ', 1400)\n",
      "epoch 15, minibatch 100/100, validation error 1.680000 %\n",
      "('training @ iter = ', 1500)\n",
      "epoch 16, minibatch 100/100, validation error 1.620000 %\n",
      "('training @ iter = ', 1600)\n",
      "epoch 17, minibatch 100/100, validation error 1.590000 %\n",
      "('training @ iter = ', 1700)\n",
      "epoch 18, minibatch 100/100, validation error 1.560000 %\n",
      "('training @ iter = ', 1800)\n",
      "epoch 19, minibatch 100/100, validation error 1.530000 %\n",
      "('training @ iter = ', 1900)\n",
      "epoch 20, minibatch 100/100, validation error 1.520000 %\n",
      "('training @ iter = ', 2000)\n",
      "epoch 21, minibatch 100/100, validation error 1.490000 %\n",
      "('training @ iter = ', 2100)\n",
      "epoch 22, minibatch 100/100, validation error 1.460000 %\n",
      "('training @ iter = ', 2200)\n",
      "epoch 23, minibatch 100/100, validation error 1.430000 %\n",
      "('training @ iter = ', 2300)\n",
      "epoch 24, minibatch 100/100, validation error 1.410000 %\n",
      "('training @ iter = ', 2400)\n",
      "epoch 25, minibatch 100/100, validation error 1.380000 %\n",
      "('training @ iter = ', 2500)\n",
      "epoch 26, minibatch 100/100, validation error 1.340000 %\n",
      "('training @ iter = ', 2600)\n",
      "epoch 27, minibatch 100/100, validation error 1.320000 %\n",
      "('training @ iter = ', 2700)\n",
      "epoch 28, minibatch 100/100, validation error 1.300000 %\n",
      "('training @ iter = ', 2800)\n",
      "epoch 29, minibatch 100/100, validation error 1.270000 %\n",
      "('training @ iter = ', 2900)\n",
      "epoch 30, minibatch 100/100, validation error 1.260000 %\n",
      "('training @ iter = ', 3000)\n",
      "epoch 31, minibatch 100/100, validation error 1.260000 %\n",
      "('training @ iter = ', 3100)\n",
      "epoch 32, minibatch 100/100, validation error 1.250000 %\n",
      "('training @ iter = ', 3200)\n",
      "epoch 33, minibatch 100/100, validation error 1.250000 %\n",
      "('training @ iter = ', 3300)\n",
      "epoch 34, minibatch 100/100, validation error 1.220000 %\n",
      "('training @ iter = ', 3400)\n",
      "epoch 35, minibatch 100/100, validation error 1.220000 %\n",
      "('training @ iter = ', 3500)\n",
      "epoch 36, minibatch 100/100, validation error 1.190000 %\n",
      "('training @ iter = ', 3600)\n",
      "epoch 37, minibatch 100/100, validation error 1.190000 %\n",
      "('training @ iter = ', 3700)\n",
      "epoch 38, minibatch 100/100, validation error 1.180000 %\n",
      "('training @ iter = ', 3800)\n",
      "epoch 39, minibatch 100/100, validation error 1.180000 %\n",
      "('training @ iter = ', 3900)\n",
      "epoch 40, minibatch 100/100, validation error 1.170000 %\n",
      "('training @ iter = ', 4000)\n",
      "epoch 41, minibatch 100/100, validation error 1.150000 %\n",
      "('training @ iter = ', 4100)\n",
      "epoch 42, minibatch 100/100, validation error 1.150000 %\n",
      "('training @ iter = ', 4200)\n",
      "epoch 43, minibatch 100/100, validation error 1.140000 %\n",
      "('training @ iter = ', 4300)\n",
      "epoch 44, minibatch 100/100, validation error 1.130000 %\n",
      "('training @ iter = ', 4400)\n",
      "epoch 45, minibatch 100/100, validation error 1.130000 %\n",
      "('training @ iter = ', 4500)\n",
      "epoch 46, minibatch 100/100, validation error 1.120000 %\n",
      "('training @ iter = ', 4600)\n",
      "epoch 47, minibatch 100/100, validation error 1.110000 %\n",
      "('training @ iter = ', 4700)\n",
      "epoch 48, minibatch 100/100, validation error 1.090000 %\n",
      "('training @ iter = ', 4800)\n",
      "epoch 49, minibatch 100/100, validation error 1.090000 %\n",
      "('training @ iter = ', 4900)\n",
      "epoch 50, minibatch 100/100, validation error 1.090000 %\n",
      "('training @ iter = ', 5000)\n",
      "epoch 51, minibatch 100/100, validation error 1.100000 %\n",
      "('training @ iter = ', 5100)\n",
      "epoch 52, minibatch 100/100, validation error 1.090000 %\n",
      "('training @ iter = ', 5200)\n",
      "epoch 53, minibatch 100/100, validation error 1.080000 %\n",
      "('training @ iter = ', 5300)\n",
      "epoch 54, minibatch 100/100, validation error 1.070000 %\n",
      "('training @ iter = ', 5400)\n",
      "epoch 55, minibatch 100/100, validation error 1.070000 %\n",
      "('training @ iter = ', 5500)\n",
      "epoch 56, minibatch 100/100, validation error 1.080000 %\n",
      "('training @ iter = ', 5600)\n",
      "epoch 57, minibatch 100/100, validation error 1.080000 %\n",
      "('training @ iter = ', 5700)\n",
      "epoch 58, minibatch 100/100, validation error 1.070000 %\n",
      "('training @ iter = ', 5800)\n",
      "epoch 59, minibatch 100/100, validation error 1.060000 %\n",
      "('training @ iter = ', 5900)\n",
      "epoch 60, minibatch 100/100, validation error 1.070000 %\n",
      "('training @ iter = ', 6000)\n",
      "epoch 61, minibatch 100/100, validation error 1.070000 %\n",
      "('training @ iter = ', 6100)\n",
      "epoch 62, minibatch 100/100, validation error 1.070000 %\n",
      "('training @ iter = ', 6200)\n",
      "epoch 63, minibatch 100/100, validation error 1.070000 %\n",
      "('training @ iter = ', 6300)\n",
      "epoch 64, minibatch 100/100, validation error 1.070000 %\n",
      "('training @ iter = ', 6400)\n",
      "epoch 65, minibatch 100/100, validation error 1.070000 %\n",
      "('training @ iter = ', 6500)\n",
      "epoch 66, minibatch 100/100, validation error 1.060000 %\n",
      "('training @ iter = ', 6600)\n",
      "epoch 67, minibatch 100/100, validation error 1.060000 %\n",
      "('training @ iter = ', 6700)\n",
      "epoch 68, minibatch 100/100, validation error 1.070000 %\n",
      "('training @ iter = ', 6800)\n",
      "epoch 69, minibatch 100/100, validation error 1.070000 %\n",
      "('training @ iter = ', 6900)\n",
      "epoch 70, minibatch 100/100, validation error 1.050000 %\n",
      "('training @ iter = ', 7000)\n",
      "epoch 71, minibatch 100/100, validation error 1.030000 %\n",
      "('training @ iter = ', 7100)\n",
      "epoch 72, minibatch 100/100, validation error 1.030000 %\n",
      "('training @ iter = ', 7200)\n",
      "epoch 73, minibatch 100/100, validation error 1.020000 %\n",
      "('training @ iter = ', 7300)\n",
      "epoch 74, minibatch 100/100, validation error 1.000000 %\n",
      "('training @ iter = ', 7400)\n",
      "epoch 75, minibatch 100/100, validation error 1.000000 %\n",
      "('training @ iter = ', 7500)\n",
      "epoch 76, minibatch 100/100, validation error 0.980000 %\n",
      "('training @ iter = ', 7600)\n",
      "epoch 77, minibatch 100/100, validation error 0.970000 %\n",
      "('training @ iter = ', 7700)\n",
      "epoch 78, minibatch 100/100, validation error 0.970000 %\n",
      "('training @ iter = ', 7800)\n",
      "epoch 79, minibatch 100/100, validation error 0.990000 %\n",
      "('training @ iter = ', 7900)\n",
      "epoch 80, minibatch 100/100, validation error 0.990000 %\n",
      "('training @ iter = ', 8000)\n",
      "epoch 81, minibatch 100/100, validation error 1.000000 %\n",
      "('training @ iter = ', 8100)\n",
      "epoch 82, minibatch 100/100, validation error 1.000000 %\n",
      "('training @ iter = ', 8200)\n",
      "epoch 83, minibatch 100/100, validation error 0.990000 %\n",
      "('training @ iter = ', 8300)\n",
      "epoch 84, minibatch 100/100, validation error 0.990000 %\n",
      "('training @ iter = ', 8400)\n",
      "epoch 85, minibatch 100/100, validation error 0.980000 %\n",
      "('training @ iter = ', 8500)\n",
      "epoch 86, minibatch 100/100, validation error 0.980000 %\n",
      "('training @ iter = ', 8600)\n",
      "epoch 87, minibatch 100/100, validation error 0.980000 %\n",
      "('training @ iter = ', 8700)\n",
      "epoch 88, minibatch 100/100, validation error 0.970000 %\n",
      "('training @ iter = ', 8800)\n",
      "epoch 89, minibatch 100/100, validation error 0.960000 %\n",
      "('training @ iter = ', 8900)\n",
      "epoch 90, minibatch 100/100, validation error 0.970000 %\n",
      "('training @ iter = ', 9000)\n",
      "epoch 91, minibatch 100/100, validation error 0.970000 %\n",
      "('training @ iter = ', 9100)\n",
      "epoch 92, minibatch 100/100, validation error 0.950000 %\n",
      "('training @ iter = ', 9200)\n",
      "epoch 93, minibatch 100/100, validation error 0.940000 %\n",
      "('training @ iter = ', 9300)\n",
      "epoch 94, minibatch 100/100, validation error 0.940000 %\n",
      "('training @ iter = ', 9400)\n",
      "epoch 95, minibatch 100/100, validation error 0.940000 %\n",
      "('training @ iter = ', 9500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96, minibatch 100/100, validation error 0.940000 %\n",
      "('training @ iter = ', 9600)\n",
      "epoch 97, minibatch 100/100, validation error 0.930000 %\n",
      "('training @ iter = ', 9700)\n",
      "epoch 98, minibatch 100/100, validation error 0.930000 %\n",
      "('training @ iter = ', 9800)\n",
      "epoch 99, minibatch 100/100, validation error 0.940000 %\n",
      "('training @ iter = ', 9900)\n",
      "epoch 100, minibatch 100/100, validation error 0.940000 %\n",
      "('training @ iter = ', 10000)\n",
      "Best validation score of 0.000000 % obtained at iteration 1, with test performance 61.990000 %\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    evaluate_lenet5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment:\n",
    "The running time of CNN is even higher than MLP and simple logistic regression .. \n",
    "this is due to the large number of parameters (although shared variables concept is there) and the complexity of the network topology .. \n",
    "\n",
    "The number of filters used in CNNs is typically much smaller than the number of hidden units in MLPs\n",
    "and depends on the size of the feature maps (itself a function of input image size and filter shapes).\n",
    "\n",
    "The test error deceases rapidely from 61.9% in the first 100 itrs to only 9.23% in the 2nd 100 itrs .. and cont. decreasing (but with lower rate) up to about 0.94%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
