{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "This notebook is devoted to diagonsing autoencoders using Theano .. \n",
    "\n",
    "A denoising autoencoders tries to reconstruct the input from a corrupted version of it by projecting it first in a latent space and reprojecting it afterwards back in the input space (i.e. train the autoencoder to reconstruct the input\n",
    "from a corrupted version of it)\n",
    "\n",
    "It assumes an implementation of simple logistic regression and MLP on MNIST dataset .. \n",
    "\n",
    " If x is the input then equation (1) computes a partially\n",
    "    destroyed version of x by means of a stochastic mapping q_D. Equation (2)\n",
    "    computes the projection of the input into the latent space. Equation (3)\n",
    "    computes the reconstruction of the input, while equation (4) computes the\n",
    "    reconstruction error.\n",
    "\n",
    "        \\tilde{x} ~ q_D(\\tilde{x}|x)                                     (1)\n",
    "\n",
    "        y = s(W \\tilde{x} + b)                                           (2)\n",
    "\n",
    "        x = s(W' y  + b')                                                (3)\n",
    "\n",
    "        L(x,z) = -sum_{k=1}^d [x_k \\log z_k + (1-x_k) \\log( 1-z_k)]      (4)\n",
    "        \n",
    "The below stems from the following Deep Learning tutorial .. \n",
    "http://deeplearning.net/tutorial/deeplearning.pdf\n",
    "\n",
    "Check out more about auto-encoders from this tutorial ==>\n",
    "https://www.youtube.com/watch?v=b99UVkWzYTQ\n",
    "\n",
    "__Note__:\n",
    "- The reconstruction is a fundmental step here .. where the inputs from the backprob (after cost calc) is compared to the original inputs ..\n",
    "\n",
    "- These auto-encoders are sometimes called (Restricted Boltezman Machines -RBM-) \n",
    "\n",
    "- Auto-encoders are used for dimensionality reduction due to its encoding (at the input), decoding (at the output) steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "from logistic_sgd import load_data\n",
    "import utils  #import tile_raster_images\n",
    "\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization:\n",
    "When dealing with SdAs this always happens, the dA on layer 2 gets as input the output of the dA on layer 1, and the weights of the dA are used in the second stage of training to construct an MLP.\n",
    "\n",
    "The following class contains the whole functions .. collectively represent the implementation of DA .. \n",
    "\n",
    "It starts with the init function .. followed by that one for corrupted inputs that contains cost calculation and updates .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class diagAutoEncod(object):     \n",
    "    def __init__(self, numpy_rng, theano_rng, input, n_visible, n_hidden): \n",
    "        W=None\n",
    "        bhid=None\n",
    "        bvis=None\n",
    "#         W_prime = None\n",
    "#         n_visible=784\n",
    "#         n_hidden=500\n",
    "\n",
    "            ## bhid:biases to hidden units, bvis: biases to visible units\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        # create a Theano random generator that gives symbolic random values\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "        \n",
    "        # note that W' was written as `W_prime` and b' as `b_prime`\n",
    "        if not W:\n",
    "            # W is initialized with `initial_W` which is uniformely sampled\n",
    "            # from [-4*sqrt(6./(n_visible+n_hidden)), 4*sqrt(6./(n_hidden+n_visible))]the output of uniform if\n",
    "            # converted using asarray to dtype theano.config.floatX so that the code is runable on GPU\n",
    "            initial_W = np.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)\n",
    "                ),dtype=theano.config.floatX)\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "            \n",
    "            ## init biases \n",
    "            if not bvis:\n",
    "                bvis = theano.shared(value=np.zeros(n_visible, dtype=theano.config.floatX),borrow=True)\n",
    "            \n",
    "            if not bhid:\n",
    "                bhid = theano.shared(value=np.zeros(n_hidden, dtype=theano.config.floatX),name='b', borrow=True)\n",
    "            \n",
    "            self.W = W\n",
    "            # b corresponds to the bias of the hidden\n",
    "            self.b = bhid\n",
    "            # b_prime corresponds to the bias of the visible\n",
    "            self.b_prime = bvis\n",
    "            \n",
    "            # tied weights, therefore W_prime is W transpose\n",
    "#             W_prime_ = np.asarray(\n",
    "#                 numpy_rng.uniform(\n",
    "#                     low=-4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
    "#                     high=4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
    "#                     size=(n_visible, n_hidden)\n",
    "#                 ),dtype=theano.config.floatX)\n",
    "            \n",
    "#             W_prime = theano.shared(value=W_prime_, name='W', borrow=True)\n",
    "            self.W_prime = T.DimShuffle(initial_W)  #T.transpose(initial_W)\n",
    "\n",
    "#             self.W_prime = self.W.T\n",
    "            self.theano_rng = theano_rng\n",
    "\n",
    "            # if no input is given, generate a variable representing the input\n",
    "            if input is None:\n",
    "                # we use a matrix because we expect a minibatch of several examples, each example being a row\n",
    "                self.x = T.dmatrix(name='input')\n",
    "            else:\n",
    "                self.x = input\n",
    "\n",
    "            self.params = [self.W, self.b, self.b_prime]\n",
    "    # end of init \n",
    "    \n",
    "        #   The following function depends on theano binomial function \n",
    "        #   The binomial function return int64 data type by default.  \n",
    "        #   int64 multiplicated by the input type(floatX) always return float64.  \n",
    "        #   To keep all data in floatX when floatX is float32, we set the dtype of\n",
    "        #   the binomial to floatX. As in our case the value of the binomial is always 0 or 1, this don't change the\n",
    "        #   result. This is needed to allow the gpu to work correctly as it only support float32 for now.\n",
    "     # this method converts auto-encoder to diagonsing auto-encoder\n",
    "    def get_corrupted_input(self, input, corruption_level):\n",
    "                 # this function produces an array of 0s and 1s \n",
    "                # where 1 has a probability of 1 - ``corruption_level`` and 0 with ``corruption_level``\n",
    "            return self.theano_rng.binomial(size=input.shape, n=1, \n",
    "                                                p=1 - corruption_level, dtype=theano.config.floatX) * input\n",
    "\n",
    "    def get_hidden_values(self, input):\n",
    "            # Computes the values of the hidden layer\n",
    "            return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
    "\n",
    "\n",
    "    def get_reconstructed_input(self, hidden):\n",
    "              ## Computes the reconstructed input given the values of the hidden layer\n",
    "            return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n",
    "\n",
    "            ## cost function .. \n",
    "            ## This function computes the cost and the updates (SGD) for one trainng step of the dA\n",
    "    def get_cost_updates(self, corruption_level, learning_rate):\n",
    "            tilde_x = self.get_corrupted_input(self.x, corruption_level)\n",
    "            y = self.get_hidden_values(tilde_x)  # y is a function of x\n",
    "            z = self.get_reconstructed_input(y)  # z is a function of y\n",
    "                # note : we sum over the size of a datapoint; if we are using minibatches\n",
    "                # L will be a vector, with one entry per example in minibatch\n",
    "            L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1) # cross entropy cost\n",
    "                # note : L is now a vector, where each element is the\n",
    "                #        cross-entropy cost of the reconstruction of the\n",
    "                #        corresponding example of the minibatch. We need to\n",
    "                #        compute the average of all these to get the cost of\n",
    "                #        the minibatch\n",
    "            cost = T.mean(L)\n",
    "\n",
    "                # compute the gradients of the cost of the `dA` with respect\n",
    "                # to its parameters\n",
    "            gparams = T.grad(cost, self.params)\n",
    "                # generate the list of updates\n",
    "            updates = [(param, param - learning_rate * gparam) for param, gparam in zip(self.params, gparams)]\n",
    "            return (cost, updates)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test_dA:\n",
    "this function is devoted to building, training and testing the whole model ..\n",
    "\n",
    "MNIST dataset is used in testing the model .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_dA():\n",
    "    \n",
    "    learning_rate=0.1\n",
    "    training_epochs=15, \n",
    "    dataset='mnist.pkl.gz'\n",
    "    batch_size=20\n",
    "    output_folder='dA_plots'\n",
    "    \n",
    "#     if not os.path.isdir(output_folder):\n",
    "#         os.makedirs(output_folder)\n",
    "#         os.chdir('/home/eman/PhD/Deep Learning Practice/Myown practice')  # Changes the current working directory to the given path.It returns None in all the cases.\n",
    "\n",
    "    \n",
    "    ## load data \n",
    "    datasets = load_data(dataset)\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    \n",
    "        # compute number of minibatches for training\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    \n",
    "#     # allocate symbolic variables for the data\n",
    "    index = T.lscalar()    # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "    \n",
    "    \n",
    "    ###################################\n",
    " ##   BUILDING THE MODEL NO CORRUPTION ##\n",
    "    ###################################\n",
    "    \n",
    "    rng = np.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "        \n",
    "    da_ = diagAutoEncod(numpy_rng=rng, theano_rng=theano_rng, input=x, n_visible=28 * 28, n_hidden=500)\n",
    "    \n",
    "    # cost calc and update\n",
    "    cost, updates = da.get_cost_updates(corruption_level=0, learning_rate=learning_rate)\n",
    "    \n",
    "#     ## training the model \n",
    "    train_da = theano.function([index], cost, updates=updates,\n",
    "        givens={x: train_set_x[index * batch_size: (index + 1) * batch_size]})\n",
    "\n",
    "#     ##start timer\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    \n",
    "#      ################\n",
    "#     # Model TRAINING #\n",
    "#     ################\n",
    "\n",
    "#     # go through training epochs\n",
    "    for epoch in range(training_epochs):\n",
    "        # go through trainng set\n",
    "        c = []\n",
    "        for batch_index in range(n_train_batches):\n",
    "            c.append(train_da(batch_index))\n",
    "\n",
    "        print('Training epoch %d, cost ' % epoch, numpy.mean(c, dtype='float64'))\n",
    "\n",
    "#     ## end timer ..\n",
    "    end_time = timeit.default_timer()\n",
    "    \n",
    "    training_time = (end_time - start_time)  # calc the duration of the training step .. \n",
    "    \n",
    "    print(('The no corruption code for file ' + os.path.split(__file__)[1] +\n",
    "           ' ran for %.2fm' % ((training_time) / 60.)), sys.stderr)\n",
    "    \n",
    "    image = Image.fromarray(tile_raster_images(X=da.W.get_value(borrow=True).T,\n",
    "                           img_shape=(28, 28), tile_shape=(10, 10), tile_spacing=(1, 1)))\n",
    "    image.save('filters_corruption_0.png')\n",
    "\n",
    "#       #####################################\n",
    "#     # BUILDING THE MODEL CORRUPTION 30% #\n",
    "#     #####################################\n",
    "\n",
    "    rng = np.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    da = diagAutoEncod(numpy_rng=rng, theano_rng=theano_rng, input=x, n_visible=28 * 28, n_hidden=500)\n",
    "    \n",
    "    cost, updates = da.get_cost_updates(corruption_level=0.3, learning_rate=learning_rate)\n",
    "    \n",
    "    train_da = theano.function([index], cost, updates=updates, givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size]})\n",
    "    \n",
    "\n",
    "    ## start training time .. \n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "     ################\n",
    "    # MODEL TRAINING #\n",
    "    ################\n",
    "    \n",
    "     # go through training epochs\n",
    "    for epoch in range(training_epochs):\n",
    "        # go through trainng set\n",
    "        c = []\n",
    "        for batch_index in range(n_train_batches):\n",
    "            c.append(train_da(batch_index))\n",
    "\n",
    "        print('Training epoch %d, cost ' % epoch, np.mean(c, dtype='float64'))\n",
    "\n",
    "        ## end training \n",
    "        end_time = timeit.default_timer()\n",
    "        \n",
    "        training_time = (end_time - start_time)  ## training time ..\n",
    "        print(('The 30% corruption code for file ' + os.path.split(__file__)[1] +\n",
    "           ' ran for %.2fm' % (training_time / 60.)), sys.stderr)\n",
    "    \n",
    "    image = Image.fromarray(tile_raster_images(X=da.W.get_value(borrow=True).T,\n",
    "        img_shape=(28, 28), tile_shape=(10, 10), tile_spacing=(1, 1)))\n",
    "    image.save('filters_corruption_30.png')\n",
    "    os.chdir('/home/eman/PhD/Deep Learning Practice/Myown practice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes at least 3 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-876273045146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtest_dA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-4be1d002405f>\u001b[0m in \u001b[0;36mtest_dA\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtheano_rng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomStreams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mda_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiagAutoEncod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtheano_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_visible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# cost calc and update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-032df1903188>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, numpy_rng, theano_rng, input, n_visible, n_hidden)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#             W_prime = theano.shared(value=W_prime_, name='W', borrow=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDimShuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_W\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#T.transpose(initial_W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#             self.W_prime = self.W.T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes at least 3 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_dA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side Note:\n",
    "The following commands to check the current working directory and to change it to whatever directory you wanna work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/eman/PhD/Deep Learning Practice/Myown practice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:\n",
    "\n",
    "- Experiments reported in [Bengio07] suggest that, in practice, when trained with stochastic gradient descent, non-linear auto-encoders with more hidden units than inputs (called overcomplete) yield useful representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this implementation contains an error in calling __init__ ==> \n",
    "\n",
    "__init__() takes at least 3 arguments (2 given)\n",
    "\n",
    "although I checked the parameters! .. this might be checked ! .."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
