{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "This notebook is devoted to diagonsing autoencoders using Theano .. \n",
    "\n",
    "A denoising autoencoders tries to reconstruct the input from a corrupted version of it by projecting it first in a latent space and reprojecting it afterwards back in the input space.\n",
    "\n",
    "It assumes an implementation of simple logistic regression and MLP on MNIST dataset .. \n",
    "\n",
    " If x is the input then equation (1) computes a partially\n",
    "    destroyed version of x by means of a stochastic mapping q_D. Equation (2)\n",
    "    computes the projection of the input into the latent space. Equation (3)\n",
    "    computes the reconstruction of the input, while equation (4) computes the\n",
    "    reconstruction error.\n",
    "\n",
    "        \\tilde{x} ~ q_D(\\tilde{x}|x)                                     (1)\n",
    "\n",
    "        y = s(W \\tilde{x} + b)                                           (2)\n",
    "\n",
    "        x = s(W' y  + b')                                                (3)\n",
    "\n",
    "        L(x,z) = -sum_{k=1}^d [x_k \\log z_k + (1-x_k) \\log( 1-z_k)]      (4)\n",
    "        \n",
    "The below stems from the following Deep Learning tutorial .. \n",
    "http://deeplearning.net/tutorial/deeplearning.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "from logistic_sgd import load_data\n",
    "import utils  #tile_raster_images\n",
    "\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization:\n",
    "When dealing with SdAs this always happens, the dA on layer 2 gets as input the output of the dA on layer 1, and the weights of the dA are used in the second stage of training to construct an MLP.\n",
    "\n",
    "The following class contains the whole functions .. collectively represent the implementation of DA .. \n",
    "\n",
    "It starts with the init function .. followed by that one for corrupted inputs that contains cost calculation and updates .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class diagAutoEncod(object):\n",
    "     def __init__(self, numpy_rng, theano_rng=None, input=None, n_visible=784, n_hidden=500, W=None, bhid=None, bvis=None):\n",
    "            ## bhid:biases to hidden units, bvis: biases to visible units\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        # create a Theano random generator that gives symbolic random values\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "        \n",
    "        # note that W' was written as `W_prime` and b' as `b_prime`\n",
    "        if not W:\n",
    "            # W is initialized with `initial_W` which is uniformely sampled\n",
    "            # from [-4*sqrt(6./(n_visible+n_hidden)), 4*sqrt(6./(n_hidden+n_visible))]the output of uniform if\n",
    "            # converted using asarray to dtype theano.config.floatX so that the code is runable on GPU\n",
    "            initial_W = np.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)\n",
    "                ),dtype=theano.config.floatX)\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "            \n",
    "            ## init biases \n",
    "            if not bvis:\n",
    "                bvis = theano.shared(value=np.zeros(n_visible, dtype=theano.config.floatX),borrow=True)\n",
    "            \n",
    "            if not bhid:\n",
    "                bhid = theano.shared(value=np.zeros(n_hidden, dtype=theano.config.floatX),name='b', borrow=True)\n",
    "            \n",
    "            self.W = W\n",
    "            # b corresponds to the bias of the hidden\n",
    "            self.b = bhid\n",
    "            # b_prime corresponds to the bias of the visible\n",
    "            self.b_prime = bvis\n",
    "            # tied weights, therefore W_prime is W transpose\n",
    "            self.W_prime = self.W.T\n",
    "            self.theano_rng = theano_rng\n",
    "\n",
    "            # if no input is given, generate a variable representing the input\n",
    "            if input is None:\n",
    "                # we use a matrix because we expect a minibatch of several examples, each example being a row\n",
    "                self.x = T.dmatrix(name='input')\n",
    "            else:\n",
    "                self.x = input\n",
    "\n",
    "            self.params = [self.W, self.b, self.b_prime]\n",
    "    ## end of init \n",
    "    \n",
    "        #   The following function depends on theano binomial function \n",
    "        #   The binomial function return int64 data type by default.  \n",
    "        #   int64 multiplicated by the input type(floatX) always return float64.  \n",
    "        #   To keep all data in floatX when floatX is float32, we set the dtype of\n",
    "        #   the binomial to floatX. As in our case the value of the binomial is always 0 or 1, this don't change the\n",
    "        #   result. This is needed to allow the gpu to work correctly as it only support float32 for now.\n",
    "\n",
    "        def get_corrupted_input(self, input, corruption_level):\n",
    "             # this function produces an array of 0s and 1s \n",
    "            # where 1 has a probability of 1 - ``corruption_level`` and 0 with ``corruption_level``\n",
    "            return self.theano_rng.binomial(size=input.shape, n=1, \n",
    "                                            p=1 - corruption_level, dtype=theano.config.floatX) * input\n",
    "            \n",
    "        def get_hidden_values(self, input):\n",
    "        # Computes the values of the hidden layer\n",
    "            return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
    "        \n",
    "        \n",
    "        def get_reconstructed_input(self, hidden):\n",
    "          ## Computes the reconstructed input given the values of the hidden layer\n",
    "            return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n",
    "\n",
    "        ## cost function .. \n",
    "        ## This function computes the cost and the updates for one trainng step of the dA\n",
    "        def get_cost_updates(self, corruption_level, learning_rate):\n",
    "            tilde_x = self.get_corrupted_input(self.x, corruption_level)\n",
    "            y = self.get_hidden_values(tilde_x)  # y is a function of x\n",
    "            z = self.get_reconstructed_input(y)  # z is a function of y\n",
    "            # note : we sum over the size of a datapoint; if we are using minibatches\n",
    "            # L will be a vector, with one entry per example in minibatch\n",
    "            L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1) # cross entropy cost\n",
    "            # note : L is now a vector, where each element is the\n",
    "            #        cross-entropy cost of the reconstruction of the\n",
    "            #        corresponding example of the minibatch. We need to\n",
    "            #        compute the average of all these to get the cost of\n",
    "            #        the minibatch\n",
    "            cost = T.mean(L)\n",
    "\n",
    "            # compute the gradients of the cost of the `dA` with respect\n",
    "            # to its parameters\n",
    "            gparams = T.grad(cost, self.params)\n",
    "            # generate the list of updates\n",
    "            updates = [(param, param - learning_rate * gparam) for param, gparam in zip(self.params, gparams)]\n",
    "            return (cost, updates)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test_dA:\n",
    "this function is devoted to building, training and testing the whole model ..\n",
    "\n",
    "MNIST dataset is used in testing the model .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dA(learning_rate=0.1, training_epochs=15, \n",
    "            dataset='mnist.pkl.gz', batch_size=20, \n",
    "            output_folder='/home/eman/PhD/Deep Learning Practice/Myown practice/dA_plots'):\n",
    "    \n",
    "#     if not os.path.isdir(output_folder):\n",
    "#         os.makedirs(output_folder)\n",
    "#         os.chdir(output_folder)  # Changes the current working directory to the given path.It returns None in all the cases.\n",
    "\n",
    "    \n",
    "    ## load data \n",
    "    datasets = load_data(dataset)\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    \n",
    "        # compute number of minibatches for training\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    \n",
    "#     # allocate symbolic variables for the data\n",
    "    index = T.lscalar()    # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "    \n",
    "    \n",
    "    ###################################\n",
    " ##   BUILDING THE MODEL NO CORRUPTION ##\n",
    "    ###################################\n",
    "    \n",
    "    rng = np.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    da = diagAutoEncod(numpy_rng=rng, theano_rng=theano_rng, input=x, n_visible=28*28, n_hidden=500)\n",
    "    \n",
    "    ## cost calc and update\n",
    "    cost, updates = da.get_cost_updates(corruption_level=0, learning_rate=learning_rate)\n",
    "    \n",
    "    ## training the model \n",
    "    train_da = theano.function([index], cost, updates=updates,\n",
    "        givens={x: train_set_x[index * batch_size: (index + 1) * batch_size]})\n",
    "\n",
    "    ##start timer\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    \n",
    "     ################\n",
    "    # Model TRAINING #\n",
    "    ################\n",
    "\n",
    "    # go through training epochs\n",
    "    for epoch in range(training_epochs):\n",
    "        # go through trainng set\n",
    "        c = []\n",
    "        for batch_index in range(n_train_batches):\n",
    "            c.append(train_da(batch_index))\n",
    "\n",
    "        print('Training epoch %d, cost ' % epoch, numpy.mean(c, dtype='float64'))\n",
    "\n",
    "    ## end timer ..\n",
    "    end_time = timeit.default_timer()\n",
    "    \n",
    "    training_time = (end_time - start_time)  # calc the duration of the training step .. \n",
    "    \n",
    "    print(('The no corruption code for file ' + os.path.split(__file__)[1] +\n",
    "           ' ran for %.2fm' % ((training_time) / 60.)), sys.stderr)\n",
    "    \n",
    "    image = Image.fromarray(tile_raster_images(X=da.W.get_value(borrow=True).T,\n",
    "                           img_shape=(28, 28), tile_shape=(10, 10), tile_spacing=(1, 1)))\n",
    "    image.save('filters_corruption_0.png')\n",
    "\n",
    "      #####################################\n",
    "    # BUILDING THE MODEL CORRUPTION 30% #\n",
    "    #####################################\n",
    "\n",
    "    rng = np.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    da = diagAutoEncod(numpy_rng=rng, theano_rng=theano_rng, input=x, n_visible=28 * 28, n_hidden=500)\n",
    "    \n",
    "    cost, updates = da.get_cost_updates(corruption_level=0.3, learning_rate=learning_rate)\n",
    "    \n",
    "    train_da = theano.function([index], cost, updates=updates, givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size]})\n",
    "    \n",
    "\n",
    "    ## start training time .. \n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "     ################\n",
    "    # MODEL TRAINING #\n",
    "    ################\n",
    "    \n",
    "     # go through training epochs\n",
    "    for epoch in range(training_epochs):\n",
    "        # go through trainng set\n",
    "        c = []\n",
    "        for batch_index in range(n_train_batches):\n",
    "            c.append(train_da(batch_index))\n",
    "\n",
    "        print('Training epoch %d, cost ' % epoch, np.mean(c, dtype='float64'))\n",
    "\n",
    "        ## end training \n",
    "        end_time = timeit.default_timer()\n",
    "        \n",
    "        training_time = (end_time - start_time)  ## training time ..\n",
    "        print(('The 30% corruption code for file ' + os.path.split(__file__)[1] +\n",
    "           ' ran for %.2fm' % (training_time / 60.)), sys.stderr)\n",
    "    \n",
    "    image = Image.fromarray(tile_raster_images(X=da.W.get_value(borrow=True).T,\n",
    "        img_shape=(28, 28), tile_shape=(10, 10), tile_spacing=(1, 1)))\n",
    "    image.save('filters_corruption_30.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_dA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side Note:\n",
    "The following commands to check the current working directory and to change it to whatever directory you wanna work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/eman/PhD/Deep Learning Practice/Myown practice')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
