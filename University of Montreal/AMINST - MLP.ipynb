{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "In this notebook, we gonna apply MLP to classify MNIST dataset .. We gonna start with simple example .. then train MNIST ..\n",
    "\n",
    "MLP is a feed forward neural network model that depends on one hidden layer and non linear activation function (usually sigmoid or tanh) \n",
    "\n",
    "The implementation is guided by this tutorial ==> \n",
    "http://deeplearning.net/tutorial/deeplearning.pdf\n",
    "\n",
    "- Typical hidden layer of a MLP: units are fully-connected and have sigmoidal activation function. Weight matrix W is of shape (n_in,n_out) and the bias vector b is of shape (n_out,)..\n",
    "- Tanh activiation fun. will be used .. \n",
    "- Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "- each neuron operates in a regime of its activation function where information can easily be propagated both upward (activations flowing from inputs to outputs) and backward (gradients flowing from outputs to inputs)\n",
    "\n",
    "- In this implementation, intermediate layers usually have as activation function tanh or the\n",
    "   sigmoid function (defined here by a ``HiddenLayer`` class)  while the\n",
    "   top layer is a softmax layer (defined here by a ``LogisticRegression`` class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "from theano import *\n",
    "import theano.tensor as T\n",
    "from logistic_sgd import LogisticRegression, load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP:\n",
    "\n",
    "Let's construct the class of MLP that utilizes the previous implementation .. \n",
    "\n",
    "MLP class consists of two main functions .. \n",
    "1. Init: \n",
    "    contains the initialization of the model paramters and loss functions .. \n",
    "2. Test MLP:\n",
    "    train and test the model on MNIST dataset, optimize and update param. using SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
    "                 activation=T.tanh):\n",
    "    \n",
    "        self.input = input\n",
    "      \n",
    "        ## initialize the weights according to tanh activication function .. \n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            \n",
    "            ## if activiation function is sigmoid .. \n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "\n",
    "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "        \n",
    "        ## init bias vector .. \n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        ## defining the shape of the output .. \n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "  \n",
    "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
    "       ## init hidden layer with tanh activation fun.\n",
    "        self.hiddenLayer = HiddenLayer(\n",
    "            rng=rng,\n",
    "            input=input,\n",
    "            n_in=n_in,\n",
    "            n_out=n_hidden,\n",
    "            activation=T.tanh\n",
    "        )\n",
    "        # The logistic regression layer gets as input the hidden units of the hidden layer    \n",
    "        self.logRegressionLayer = LogisticRegression(\n",
    "            input=self.hiddenLayer.output,\n",
    "            n_in=n_hidden,\n",
    "            n_out=n_out\n",
    "        )\n",
    "        \n",
    "        ## defining loss function .. L1 norm \n",
    "        self.L1 = (\n",
    "            abs(self.hiddenLayer.W).sum()\n",
    "            + abs(self.logRegressionLayer.W).sum()\n",
    "        )\n",
    "\n",
    "       ## L2 norm .. is just L1 square :) \n",
    "        self.L2_sqr = (\n",
    "            (self.hiddenLayer.W ** 2).sum()\n",
    "            + (self.logRegressionLayer.W ** 2).sum()\n",
    "        )\n",
    "\n",
    "       # negative log likelihood of the MLP is given by the negative\n",
    "        # log likelihood of the output of the model, computed in the\n",
    "        # logistic regression layer\n",
    "        self.negative_log_likelihood = (\n",
    "            self.logRegressionLayer.negative_log_likelihood\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # same holds for the function computing the number of errors\n",
    "        self.errors = self.logRegressionLayer.errors\n",
    "\n",
    "        # the parameters of the model are the parameters of the two layer it is made out of (hidden, output)\n",
    "         self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test_MLP:\n",
    "\n",
    "This function is to train and test MLP on MNIST dataset with the use of SGD .. \n",
    "\n",
    "This function contains three main steps ..\n",
    "1. Model Building (includes the update formula)\n",
    "2. Model training \n",
    "3. Model testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000,\n",
    "             dataset='mnist.pkl.gz', batch_size=20, n_hidden=500):\n",
    "    \n",
    "    ## load the data .. \n",
    "    datasets = load_data(dataset)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "    \n",
    "      ######################\n",
    "     #  MODEL BUILDING #\n",
    "     ######################\n",
    "    \n",
    "     # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
    "                        # [int] labels\n",
    "\n",
    "    rng = numpy.random.RandomState(1234)\n",
    "\n",
    "    # construct the MLP class\n",
    "    classifier = MLP(\n",
    "        rng=rng,\n",
    "        input=x,\n",
    "        n_in=28 * 28,\n",
    "        n_hidden=n_hidden,\n",
    "        n_out=10\n",
    "    )\n",
    "\n",
    "     # construct the MLP class \n",
    "    cost = (\n",
    "        classifier.negative_log_likelihood(y)\n",
    "        + L1_reg * classifier.L1\n",
    "        + L2_reg * classifier.L2_sqr\n",
    "    )\n",
    "   \n",
    "     # compiling a Theano function that computes the mistakes that are made by the model on a minibatch\n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ## model validation \n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # compute the gradient of cost with respect to theta (sorted in params)\n",
    "    # the resulting gradients will be stored in a list gparams\n",
    "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "\n",
    "   \n",
    "     ## model updates .. \n",
    "    # given two lists of the same length, A = [a1, a2, a3, a4] and\n",
    "    # B = [b1, b2, b3, b4], zip generates a list C of same size, where each\n",
    "    # element is a pair formed from the two lists :\n",
    "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
    "    updates = [\n",
    "        (param, param - learning_rate * gparam)\n",
    "        for param, gparam in zip(classifier.params, gparams)\n",
    "    ]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but\n",
    "#     # in the same time updates the parameter of the model based on the rules\n",
    "#     # defined in `updates`\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "  \n",
    "      ######################\n",
    "     #  MODEL TRIANING    #\n",
    "     ######################\n",
    "\n",
    "    patience = 10000  # look as this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is\n",
    "                           # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i) for i\n",
    "                                     in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (\n",
    "                        this_validation_loss < best_validation_loss *\n",
    "                        improvement_threshold\n",
    "                    ):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = [test_model(i) for i\n",
    "                                   in range(n_test_batches)]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "    ## print the output ..\n",
    "    end_time = timeit.default_timer()\n",
    "    print(('Optimization complete. Best validation score of %f %% '\n",
    "           'obtained at iteration %i, with test performance %f %%') %\n",
    "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the function .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... building the model\n",
      "... training\n",
      "epoch 1, minibatch 2500/2500, validation error 9.620000 %\n",
      "     epoch 1, minibatch 2500/2500, test error of best model 10.090000 %\n",
      "epoch 2, minibatch 2500/2500, validation error 8.610000 %\n",
      "     epoch 2, minibatch 2500/2500, test error of best model 8.740000 %\n",
      "epoch 3, minibatch 2500/2500, validation error 8.000000 %\n",
      "     epoch 3, minibatch 2500/2500, test error of best model 8.160000 %\n",
      "epoch 4, minibatch 2500/2500, validation error 7.600000 %\n",
      "     epoch 4, minibatch 2500/2500, test error of best model 7.790000 %\n",
      "epoch 5, minibatch 2500/2500, validation error 7.300000 %\n",
      "     epoch 5, minibatch 2500/2500, test error of best model 7.590000 %\n",
      "epoch 6, minibatch 2500/2500, validation error 7.020000 %\n",
      "     epoch 6, minibatch 2500/2500, test error of best model 7.200000 %\n",
      "epoch 7, minibatch 2500/2500, validation error 6.680000 %\n",
      "     epoch 7, minibatch 2500/2500, test error of best model 6.990000 %\n",
      "epoch 8, minibatch 2500/2500, validation error 6.260000 %\n",
      "     epoch 8, minibatch 2500/2500, test error of best model 6.730000 %\n",
      "epoch 9, minibatch 2500/2500, validation error 5.970000 %\n",
      "     epoch 9, minibatch 2500/2500, test error of best model 6.510000 %\n",
      "epoch 10, minibatch 2500/2500, validation error 5.740000 %\n",
      "     epoch 10, minibatch 2500/2500, test error of best model 6.190000 %\n",
      "epoch 11, minibatch 2500/2500, validation error 5.400000 %\n",
      "     epoch 11, minibatch 2500/2500, test error of best model 5.900000 %\n",
      "epoch 12, minibatch 2500/2500, validation error 5.080000 %\n",
      "     epoch 12, minibatch 2500/2500, test error of best model 5.600000 %\n",
      "epoch 13, minibatch 2500/2500, validation error 4.820000 %\n",
      "     epoch 13, minibatch 2500/2500, test error of best model 5.400000 %\n",
      "epoch 14, minibatch 2500/2500, validation error 4.690000 %\n",
      "     epoch 14, minibatch 2500/2500, test error of best model 5.280000 %\n",
      "epoch 15, minibatch 2500/2500, validation error 4.470000 %\n",
      "     epoch 15, minibatch 2500/2500, test error of best model 5.140000 %\n",
      "epoch 16, minibatch 2500/2500, validation error 4.350000 %\n",
      "     epoch 16, minibatch 2500/2500, test error of best model 4.890000 %\n",
      "epoch 17, minibatch 2500/2500, validation error 4.130000 %\n",
      "     epoch 17, minibatch 2500/2500, test error of best model 4.700000 %\n",
      "epoch 18, minibatch 2500/2500, validation error 4.040000 %\n",
      "     epoch 18, minibatch 2500/2500, test error of best model 4.540000 %\n",
      "epoch 19, minibatch 2500/2500, validation error 3.910000 %\n",
      "     epoch 19, minibatch 2500/2500, test error of best model 4.420000 %\n",
      "epoch 20, minibatch 2500/2500, validation error 3.780000 %\n",
      "     epoch 20, minibatch 2500/2500, test error of best model 4.260000 %\n",
      "epoch 21, minibatch 2500/2500, validation error 3.700000 %\n",
      "     epoch 21, minibatch 2500/2500, test error of best model 4.150000 %\n",
      "epoch 22, minibatch 2500/2500, validation error 3.610000 %\n",
      "     epoch 22, minibatch 2500/2500, test error of best model 4.000000 %\n",
      "epoch 23, minibatch 2500/2500, validation error 3.540000 %\n",
      "     epoch 23, minibatch 2500/2500, test error of best model 3.910000 %\n",
      "epoch 24, minibatch 2500/2500, validation error 3.480000 %\n",
      "     epoch 24, minibatch 2500/2500, test error of best model 3.790000 %\n",
      "epoch 25, minibatch 2500/2500, validation error 3.400000 %\n",
      "     epoch 25, minibatch 2500/2500, test error of best model 3.710000 %\n",
      "epoch 26, minibatch 2500/2500, validation error 3.350000 %\n",
      "     epoch 26, minibatch 2500/2500, test error of best model 3.650000 %\n",
      "epoch 27, minibatch 2500/2500, validation error 3.260000 %\n",
      "     epoch 27, minibatch 2500/2500, test error of best model 3.570000 %\n",
      "epoch 28, minibatch 2500/2500, validation error 3.270000 %\n",
      "epoch 29, minibatch 2500/2500, validation error 3.240000 %\n",
      "     epoch 29, minibatch 2500/2500, test error of best model 3.440000 %\n",
      "epoch 30, minibatch 2500/2500, validation error 3.190000 %\n",
      "     epoch 30, minibatch 2500/2500, test error of best model 3.370000 %\n",
      "epoch 31, minibatch 2500/2500, validation error 3.160000 %\n",
      "     epoch 31, minibatch 2500/2500, test error of best model 3.330000 %\n",
      "epoch 32, minibatch 2500/2500, validation error 3.090000 %\n",
      "     epoch 32, minibatch 2500/2500, test error of best model 3.250000 %\n",
      "epoch 33, minibatch 2500/2500, validation error 3.050000 %\n",
      "     epoch 33, minibatch 2500/2500, test error of best model 3.170000 %\n",
      "epoch 34, minibatch 2500/2500, validation error 3.030000 %\n",
      "     epoch 34, minibatch 2500/2500, test error of best model 3.160000 %\n",
      "epoch 35, minibatch 2500/2500, validation error 3.020000 %\n",
      "     epoch 35, minibatch 2500/2500, test error of best model 3.150000 %\n",
      "epoch 36, minibatch 2500/2500, validation error 2.970000 %\n",
      "     epoch 36, minibatch 2500/2500, test error of best model 3.090000 %\n",
      "epoch 37, minibatch 2500/2500, validation error 2.940000 %\n",
      "     epoch 37, minibatch 2500/2500, test error of best model 3.040000 %\n",
      "epoch 38, minibatch 2500/2500, validation error 2.910000 %\n",
      "     epoch 38, minibatch 2500/2500, test error of best model 3.000000 %\n",
      "epoch 39, minibatch 2500/2500, validation error 2.880000 %\n",
      "     epoch 39, minibatch 2500/2500, test error of best model 2.980000 %\n",
      "epoch 40, minibatch 2500/2500, validation error 2.850000 %\n",
      "     epoch 40, minibatch 2500/2500, test error of best model 2.920000 %\n",
      "epoch 41, minibatch 2500/2500, validation error 2.810000 %\n",
      "     epoch 41, minibatch 2500/2500, test error of best model 2.860000 %\n",
      "epoch 42, minibatch 2500/2500, validation error 2.800000 %\n",
      "     epoch 42, minibatch 2500/2500, test error of best model 2.840000 %\n",
      "epoch 43, minibatch 2500/2500, validation error 2.780000 %\n",
      "     epoch 43, minibatch 2500/2500, test error of best model 2.820000 %\n",
      "epoch 44, minibatch 2500/2500, validation error 2.760000 %\n",
      "     epoch 44, minibatch 2500/2500, test error of best model 2.780000 %\n",
      "epoch 45, minibatch 2500/2500, validation error 2.720000 %\n",
      "     epoch 45, minibatch 2500/2500, test error of best model 2.730000 %\n",
      "epoch 46, minibatch 2500/2500, validation error 2.680000 %\n",
      "     epoch 46, minibatch 2500/2500, test error of best model 2.720000 %\n",
      "epoch 47, minibatch 2500/2500, validation error 2.630000 %\n",
      "     epoch 47, minibatch 2500/2500, test error of best model 2.690000 %\n",
      "epoch 48, minibatch 2500/2500, validation error 2.610000 %\n",
      "     epoch 48, minibatch 2500/2500, test error of best model 2.690000 %\n",
      "epoch 49, minibatch 2500/2500, validation error 2.580000 %\n",
      "     epoch 49, minibatch 2500/2500, test error of best model 2.680000 %\n",
      "epoch 50, minibatch 2500/2500, validation error 2.560000 %\n",
      "     epoch 50, minibatch 2500/2500, test error of best model 2.630000 %\n",
      "epoch 51, minibatch 2500/2500, validation error 2.560000 %\n",
      "epoch 52, minibatch 2500/2500, validation error 2.510000 %\n",
      "     epoch 52, minibatch 2500/2500, test error of best model 2.560000 %\n",
      "epoch 53, minibatch 2500/2500, validation error 2.490000 %\n",
      "     epoch 53, minibatch 2500/2500, test error of best model 2.530000 %\n",
      "epoch 54, minibatch 2500/2500, validation error 2.490000 %\n",
      "epoch 55, minibatch 2500/2500, validation error 2.480000 %\n",
      "     epoch 55, minibatch 2500/2500, test error of best model 2.520000 %\n",
      "epoch 56, minibatch 2500/2500, validation error 2.480000 %\n",
      "epoch 57, minibatch 2500/2500, validation error 2.460000 %\n",
      "     epoch 57, minibatch 2500/2500, test error of best model 2.500000 %\n",
      "epoch 58, minibatch 2500/2500, validation error 2.450000 %\n",
      "     epoch 58, minibatch 2500/2500, test error of best model 2.460000 %\n",
      "epoch 59, minibatch 2500/2500, validation error 2.430000 %\n",
      "     epoch 59, minibatch 2500/2500, test error of best model 2.440000 %\n",
      "epoch 60, minibatch 2500/2500, validation error 2.420000 %\n",
      "     epoch 60, minibatch 2500/2500, test error of best model 2.450000 %\n",
      "epoch 61, minibatch 2500/2500, validation error 2.390000 %\n",
      "     epoch 61, minibatch 2500/2500, test error of best model 2.410000 %\n",
      "epoch 62, minibatch 2500/2500, validation error 2.400000 %\n",
      "epoch 63, minibatch 2500/2500, validation error 2.400000 %\n",
      "epoch 64, minibatch 2500/2500, validation error 2.400000 %\n",
      "epoch 65, minibatch 2500/2500, validation error 2.350000 %\n",
      "     epoch 65, minibatch 2500/2500, test error of best model 2.350000 %\n",
      "epoch 66, minibatch 2500/2500, validation error 2.350000 %\n",
      "epoch 67, minibatch 2500/2500, validation error 2.320000 %\n",
      "     epoch 67, minibatch 2500/2500, test error of best model 2.350000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 68, minibatch 2500/2500, validation error 2.320000 %\n",
      "epoch 69, minibatch 2500/2500, validation error 2.320000 %\n",
      "epoch 70, minibatch 2500/2500, validation error 2.300000 %\n",
      "     epoch 70, minibatch 2500/2500, test error of best model 2.330000 %\n",
      "epoch 71, minibatch 2500/2500, validation error 2.290000 %\n",
      "     epoch 71, minibatch 2500/2500, test error of best model 2.330000 %\n",
      "epoch 72, minibatch 2500/2500, validation error 2.280000 %\n",
      "     epoch 72, minibatch 2500/2500, test error of best model 2.300000 %\n",
      "epoch 73, minibatch 2500/2500, validation error 2.260000 %\n",
      "     epoch 73, minibatch 2500/2500, test error of best model 2.300000 %\n",
      "epoch 74, minibatch 2500/2500, validation error 2.250000 %\n",
      "     epoch 74, minibatch 2500/2500, test error of best model 2.300000 %\n",
      "epoch 75, minibatch 2500/2500, validation error 2.230000 %\n",
      "     epoch 75, minibatch 2500/2500, test error of best model 2.300000 %\n",
      "epoch 76, minibatch 2500/2500, validation error 2.230000 %\n",
      "epoch 77, minibatch 2500/2500, validation error 2.240000 %\n",
      "epoch 78, minibatch 2500/2500, validation error 2.250000 %\n",
      "epoch 79, minibatch 2500/2500, validation error 2.220000 %\n",
      "     epoch 79, minibatch 2500/2500, test error of best model 2.290000 %\n",
      "epoch 80, minibatch 2500/2500, validation error 2.210000 %\n",
      "     epoch 80, minibatch 2500/2500, test error of best model 2.260000 %\n",
      "epoch 81, minibatch 2500/2500, validation error 2.200000 %\n",
      "     epoch 81, minibatch 2500/2500, test error of best model 2.250000 %\n",
      "epoch 82, minibatch 2500/2500, validation error 2.190000 %\n",
      "     epoch 82, minibatch 2500/2500, test error of best model 2.240000 %\n",
      "epoch 83, minibatch 2500/2500, validation error 2.190000 %\n",
      "epoch 84, minibatch 2500/2500, validation error 2.190000 %\n",
      "epoch 85, minibatch 2500/2500, validation error 2.180000 %\n",
      "     epoch 85, minibatch 2500/2500, test error of best model 2.220000 %\n",
      "epoch 86, minibatch 2500/2500, validation error 2.180000 %\n",
      "epoch 87, minibatch 2500/2500, validation error 2.160000 %\n",
      "     epoch 87, minibatch 2500/2500, test error of best model 2.220000 %\n",
      "epoch 88, minibatch 2500/2500, validation error 2.150000 %\n",
      "     epoch 88, minibatch 2500/2500, test error of best model 2.230000 %\n",
      "epoch 89, minibatch 2500/2500, validation error 2.150000 %\n",
      "epoch 90, minibatch 2500/2500, validation error 2.160000 %\n",
      "epoch 91, minibatch 2500/2500, validation error 2.160000 %\n",
      "epoch 92, minibatch 2500/2500, validation error 2.170000 %\n",
      "epoch 93, minibatch 2500/2500, validation error 2.160000 %\n",
      "epoch 94, minibatch 2500/2500, validation error 2.160000 %\n",
      "epoch 95, minibatch 2500/2500, validation error 2.150000 %\n",
      "epoch 96, minibatch 2500/2500, validation error 2.150000 %\n",
      "epoch 97, minibatch 2500/2500, validation error 2.150000 %\n",
      "epoch 98, minibatch 2500/2500, validation error 2.140000 %\n",
      "     epoch 98, minibatch 2500/2500, test error of best model 2.190000 %\n",
      "epoch 99, minibatch 2500/2500, validation error 2.140000 %\n",
      "epoch 100, minibatch 2500/2500, validation error 2.130000 %\n",
      "     epoch 100, minibatch 2500/2500, test error of best model 2.180000 %\n",
      "epoch 101, minibatch 2500/2500, validation error 2.130000 %\n",
      "epoch 102, minibatch 2500/2500, validation error 2.130000 %\n",
      "epoch 103, minibatch 2500/2500, validation error 2.130000 %\n",
      "epoch 104, minibatch 2500/2500, validation error 2.120000 %\n",
      "     epoch 104, minibatch 2500/2500, test error of best model 2.170000 %\n",
      "epoch 105, minibatch 2500/2500, validation error 2.120000 %\n",
      "epoch 106, minibatch 2500/2500, validation error 2.130000 %\n",
      "epoch 107, minibatch 2500/2500, validation error 2.130000 %\n",
      "epoch 108, minibatch 2500/2500, validation error 2.130000 %\n",
      "epoch 109, minibatch 2500/2500, validation error 2.130000 %\n",
      "epoch 110, minibatch 2500/2500, validation error 2.130000 %\n",
      "epoch 111, minibatch 2500/2500, validation error 2.130000 %\n",
      "epoch 112, minibatch 2500/2500, validation error 2.130000 %\n",
      "epoch 113, minibatch 2500/2500, validation error 2.120000 %\n",
      "epoch 114, minibatch 2500/2500, validation error 2.110000 %\n",
      "     epoch 114, minibatch 2500/2500, test error of best model 2.130000 %\n",
      "epoch 115, minibatch 2500/2500, validation error 2.100000 %\n",
      "     epoch 115, minibatch 2500/2500, test error of best model 2.130000 %\n",
      "epoch 116, minibatch 2500/2500, validation error 2.100000 %\n",
      "epoch 117, minibatch 2500/2500, validation error 2.100000 %\n",
      "epoch 118, minibatch 2500/2500, validation error 2.090000 %\n",
      "     epoch 118, minibatch 2500/2500, test error of best model 2.120000 %\n",
      "epoch 119, minibatch 2500/2500, validation error 2.080000 %\n",
      "     epoch 119, minibatch 2500/2500, test error of best model 2.110000 %\n",
      "epoch 120, minibatch 2500/2500, validation error 2.060000 %\n",
      "     epoch 120, minibatch 2500/2500, test error of best model 2.120000 %\n",
      "epoch 121, minibatch 2500/2500, validation error 2.060000 %\n",
      "epoch 122, minibatch 2500/2500, validation error 2.060000 %\n",
      "epoch 123, minibatch 2500/2500, validation error 2.060000 %\n",
      "epoch 124, minibatch 2500/2500, validation error 2.040000 %\n",
      "     epoch 124, minibatch 2500/2500, test error of best model 2.120000 %\n",
      "epoch 125, minibatch 2500/2500, validation error 2.060000 %\n",
      "epoch 126, minibatch 2500/2500, validation error 2.060000 %\n",
      "epoch 127, minibatch 2500/2500, validation error 2.060000 %\n",
      "epoch 128, minibatch 2500/2500, validation error 2.040000 %\n",
      "epoch 129, minibatch 2500/2500, validation error 2.040000 %\n",
      "epoch 130, minibatch 2500/2500, validation error 2.030000 %\n",
      "     epoch 130, minibatch 2500/2500, test error of best model 2.090000 %\n",
      "epoch 131, minibatch 2500/2500, validation error 2.030000 %\n",
      "epoch 132, minibatch 2500/2500, validation error 2.030000 %\n",
      "epoch 133, minibatch 2500/2500, validation error 2.040000 %\n",
      "epoch 134, minibatch 2500/2500, validation error 2.040000 %\n",
      "epoch 135, minibatch 2500/2500, validation error 2.040000 %\n",
      "epoch 136, minibatch 2500/2500, validation error 2.040000 %\n",
      "epoch 137, minibatch 2500/2500, validation error 2.040000 %\n",
      "epoch 138, minibatch 2500/2500, validation error 2.040000 %\n",
      "epoch 139, minibatch 2500/2500, validation error 2.050000 %\n",
      "epoch 140, minibatch 2500/2500, validation error 2.050000 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-b242833312f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtest_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-9ef8873ad6c3>\u001b[0m in \u001b[0;36mtest_mlp\u001b[0;34m(learning_rate, L1_reg, L2_reg, n_epochs, dataset, batch_size, n_hidden)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mminibatch_avg_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;31m# iteration number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_train_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mminibatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_mlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment:\n",
    "\n",
    "MLP algorithm takes much bigger running time than simple logistic regression  (LR)code .. \n",
    "\n",
    "This might be due to the large number of input paramters used in MLP as well as the number of hidden units (i.e. 500) .. hence, the structure of MLP network is more complex .. However, it is expected to give us less classification error than LR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
