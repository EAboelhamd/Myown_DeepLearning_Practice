{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "This is an implementation for Deep Belief Networks or for short (DBN) on MNIST dataset using Theano .. \n",
    "\n",
    "The principle of greedy layer-wise unsupervised training can be applied to DBNs with RBMs as the building\n",
    "blocks for each layer\n",
    "\n",
    "Algorithm steps:\n",
    "1. Train the first layer as an RBM that models the raw input x = h(0) as its visible layer.\n",
    "2. Use that first layer to obtain a representation of the input that will be used as data for the second layer. Two\n",
    "common solutions exist. This representation can be chosen as being the mean activations p(h(1) = 1jh(0))\n",
    "or samples of p(h(1)jh(0)).\n",
    "3. Train the second layer as an RBM, taking the transformed data (samples or mean activations) as training\n",
    "examples (for the visible layer of that RBM).\n",
    "4. Iterate (2 and 3) for the desired number of layers, each time propagating upward either samples or mean\n",
    "values.\n",
    "5. Fine-tune all the parameters of this deep architecture with respect to a proxy for the DBN log- likelihood,\n",
    "or with respect to a supervised training criterion (after adding extra learning machinery to convert the learned\n",
    "representation into supervised predictions, e.g. a linear classifier).\n",
    "\n",
    "In this notebook, the focus is on fine-tuning via supervised gradient descent. Specifically, we use a logistic\n",
    "regression classifier to classify the input x based on the output of the last hidden layer h(l) of the DBN. (stms from the following tutrial ==> http://deeplearning.net/tutorial/deeplearning.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "\n",
    "from logistic_sgd import LogisticRegression, load_data\n",
    "from mlp import HiddenLayer\n",
    "from rbm import RBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBN:\n",
    "\n",
    "A deep belief network is obtained by stacking several RBMs on top of each other. \n",
    "\n",
    "The hidden layer of the RBM at layer `i` becomes the input of the RBM at layer `i+1`. \n",
    "\n",
    "The first layer RBM gets as input the input of the network, and the hidden layer of the last RBM represents the output. \n",
    "\n",
    "When used for classification, the DBN is treated as an MLP, by adding a logistic regression layer on top.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DBN(object):\n",
    "    \n",
    "    def __init__(self, numpy_rng, theano_rng=None, n_ins=784,\n",
    "                 hidden_layers_sizes=[500, 500], n_outs=10):\n",
    "        \n",
    "        self.sigmoid_layers = []\n",
    "        self.rbm_layers = []\n",
    "        self.params = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "\n",
    "        if not theano_rng:\n",
    "            theano_rng = MRG_RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        # allocate symbolic variables for the data\n",
    "\n",
    "        # the data is presented as rasterized images\n",
    "        self.x = T.matrix('x')\n",
    "\n",
    "        # the labels are presented as 1D vector of [int] labels\n",
    "        self.y = T.ivector('y')\n",
    "\n",
    "        # The DBN is an MLP, for which all weights of intermediate\n",
    "        # layers are shared with a different RBM.  We will first\n",
    "        # construct the DBN as a deep multilayer perceptron, and when\n",
    "        # constructing each sigmoidal layer we also construct an RBM\n",
    "        # that shares weights with that layer. During pretraining we\n",
    "        # will train these RBMs (which will lead to chainging the\n",
    "        # weights of the MLP as well) During finetuning we will finish\n",
    "        # training the DBN by doing stochastic gradient descent on the MLP.\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            # construct the sigmoidal layer\n",
    "\n",
    "            # the size of the input is either the number of hidden\n",
    "            # units of the layer below or the input size if we are on the first layer\n",
    "            \n",
    "            if i == 0:\n",
    "                input_size = n_ins\n",
    "            else:\n",
    "                input_size = hidden_layers_sizes[i - 1]\n",
    "\n",
    "            # the input to this layer is either the activation of the\n",
    "            # hidden layer below or the input of the DBN if you are on the first layer\n",
    "            \n",
    "            if i == 0:\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                layer_input = self.sigmoid_layers[-1].output\n",
    "\n",
    "            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n",
    "                                        input=layer_input,\n",
    "                                        n_in=input_size,\n",
    "                                        n_out=hidden_layers_sizes[i],\n",
    "                                        activation=T.nnet.sigmoid)\n",
    "\n",
    "            # add the layer to our list of layers\n",
    "            self.sigmoid_layers.append(sigmoid_layer)\n",
    "\n",
    "            # its arguably a philosophical question...  but we are\n",
    "            # going to only declare that the parameters of the\n",
    "            # sigmoid_layers are parameters of the DBN. The visible\n",
    "            # biases in the RBM are parameters of those RBMs, but not\n",
    "            # of the DBN.\n",
    "            self.params.extend(sigmoid_layer.params)\n",
    "\n",
    "            # Construct an RBM that shared weights with this layer\n",
    "            rbm_layer = RBM(numpy_rng=numpy_rng,\n",
    "                            theano_rng=theano_rng,\n",
    "                            input=layer_input,\n",
    "                            n_visible=input_size,\n",
    "                            n_hidden=hidden_layers_sizes[i],\n",
    "                            W=sigmoid_layer.W,\n",
    "                            hbias=sigmoid_layer.b)\n",
    "            self.rbm_layers.append(rbm_layer)\n",
    "\n",
    "        # We now need to add a logistic layer on top of the MLP\n",
    "        self.logLayer = LogisticRegression(\n",
    "            input=self.sigmoid_layers[-1].output,\n",
    "            n_in=hidden_layers_sizes[-1],\n",
    "            n_out=n_outs)\n",
    "        self.params.extend(self.logLayer.params)\n",
    "\n",
    "        # compute the cost for second phase of training, defined as the\n",
    "        # negative log likelihood of the logistic regression (output) layer\n",
    "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        # symbolic variable that points to the number of errors made on the minibatch given by self.x and self.y\n",
    "        self.errors = self.logLayer.errors(self.y)\n",
    "\n",
    "#         Generates a list of functions, for performing one step of gradient descent at a given layer. T\n",
    "#         The function will require as input the minibatch index, and to train an RBM you just\n",
    "#         need to iterate, calling the corresponding function on all minibatch indexes.\n",
    "        \n",
    "    def pretraining_functions(self, train_set_x, batch_size, k):\n",
    "        \n",
    "        # index to a [mini]batch\n",
    "        index = T.lscalar('index')  # index to a minibatch\n",
    "        learning_rate = T.scalar('lr')  # learning rate to use\n",
    "\n",
    "        # begining of a batch, given `index`\n",
    "        batch_begin = index * batch_size\n",
    "        # ending of a batch given `index`\n",
    "        batch_end = batch_begin + batch_size\n",
    "\n",
    "        pretrain_fns = []\n",
    "        for rbm in self.rbm_layers:\n",
    "\n",
    "            # get the cost and the updates list\n",
    "            # using CD-k here (persisent=None) for training each RBM.\n",
    "            # TODO: change cost function to reconstruction error\n",
    "            cost, updates = rbm.get_cost_updates(learning_rate, persistent=None, k=k)\n",
    "\n",
    "            # compile the theano function\n",
    "            fn = theano.function(\n",
    "                inputs=[index, theano.In(learning_rate, value=0.1)],\n",
    "                outputs=cost,\n",
    "                updates=updates,\n",
    "                givens={\n",
    "                    self.x: train_set_x[batch_begin:batch_end]\n",
    "                })\n",
    "            \n",
    "            # append `fn` to the list of functions\n",
    "            pretrain_fns.append(fn)\n",
    "\n",
    "        return pretrain_fns\n",
    "\n",
    "    \n",
    "#         Generates a function `train` that implements one step of\n",
    "#         finetuning, a function `validate` that computes the error on a\n",
    "#         batch from the validation set, and a function `test` that\n",
    "#         computes the error on a batch from the testing set\n",
    "\n",
    "    def build_finetune_functions(self, datasets, batch_size, learning_rate):\n",
    "\n",
    "        (train_set_x, train_set_y) = datasets[0]\n",
    "        (valid_set_x, valid_set_y) = datasets[1]\n",
    "        (test_set_x, test_set_y) = datasets[2]\n",
    "\n",
    "        # compute number of minibatches for training, validation and testing\n",
    "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "        n_valid_batches //= batch_size\n",
    "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "        n_test_batches //= batch_size\n",
    "\n",
    "        index = T.lscalar('index')  # index to a [mini]batch\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        gparams = T.grad(self.finetune_cost, self.params)\n",
    "\n",
    "        # compute list of fine-tuning updates\n",
    "        updates = []\n",
    "        for param, gparam in zip(self.params, gparams):\n",
    "            updates.append((param, param - gparam * learning_rate))\n",
    "\n",
    "        train_fn = theano.function(\n",
    "            inputs=[index],\n",
    "            outputs=self.finetune_cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                self.x: train_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size],\n",
    "                self.y: train_set_y[index * batch_size: (index + 1) * batch_size]})\n",
    "\n",
    "        test_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: test_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: test_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]})\n",
    "\n",
    "        valid_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: valid_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: valid_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]})\n",
    "\n",
    "        # Create a function that scans the entire validation set\n",
    "        def valid_score():\n",
    "            return [valid_score_i(i) for i in range(n_valid_batches)]\n",
    "\n",
    "        # Create a function that scans the entire test set\n",
    "        def test_score():\n",
    "            return [test_score_i(i) for i in range(n_test_batches)]\n",
    "\n",
    "        return train_fn, valid_score, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_DBN:\n",
    "Train and test a Deep Belief Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_DBN(finetune_lr=0.1, pretraining_epochs=100,\n",
    "             pretrain_lr=0.01, k=1, training_epochs=1000,\n",
    "             dataset='mnist.pkl.gz', batch_size=10):\n",
    "    \n",
    "    datasets = load_data(dataset)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "    # numpy random generator\n",
    "    numpy_rng = numpy.random.RandomState(123)\n",
    "    print('... building the model')\n",
    "    # construct the Deep Belief Network\n",
    "    dbn = DBN(numpy_rng=numpy_rng, n_ins=28 * 28,\n",
    "              hidden_layers_sizes=[1000, 1000, 1000],\n",
    "              n_outs=10)\n",
    "\n",
    "    #########################\n",
    "    # PRETRAINING THE MODEL #\n",
    "    #########################\n",
    "    \n",
    "    pretraining_fns = dbn.pretraining_functions(train_set_x=train_set_x,\n",
    "                                                batch_size=batch_size,\n",
    "                                                k=k)\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "    # Pre-train layer-wise\n",
    "    for i in range(dbn.n_layers):\n",
    "        # go through pretraining epochs\n",
    "        for epoch in range(pretraining_epochs):\n",
    "            # go through the training set\n",
    "            c = []\n",
    "            for batch_index in range(n_train_batches):\n",
    "                c.append(pretraining_fns[i](index=batch_index,\n",
    "                                            lr=pretrain_lr))\n",
    "#             print('Pre-training layer %i, epoch %d, cost ' % (i, epoch), end=' ')\n",
    "            print(numpy.mean(c, dtype='float64'))\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    print('The pretraining code for file ' + os.path.split(__file__)[1] +\n",
    "          ' ran for %.2fm' % ((end_time - start_time) / 60.), sys.stderr)\n",
    "    \n",
    "    ########################\n",
    "    # FINETUNING THE MODEL #\n",
    "    ########################\n",
    "\n",
    "    # get the training, validation and testing function for the model\n",
    "    print('... getting the finetuning functions')\n",
    "    train_fn, validate_model, test_model = dbn.build_finetune_functions(\n",
    "        datasets=datasets,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=finetune_lr\n",
    "    )\n",
    "\n",
    "    # early-stopping parameters\n",
    "\n",
    "    # look as this many examples regardless\n",
    "    patience = 4 * n_train_batches\n",
    "\n",
    "    # wait this much longer when a new best is found\n",
    "    patience_increase = 2.\n",
    "\n",
    "    # a relative improvement of this much is considered significant\n",
    "    improvement_threshold = 0.995\n",
    "\n",
    "    # go through this many minibatches before checking the network on\n",
    "    # the validation set; in this case we check every epoch\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "\n",
    "    while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            train_fn(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "                validation_losses = validate_model()\n",
    "                this_validation_loss = numpy.mean(validation_losses, dtype='float64')\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' % (\n",
    "                    epoch,\n",
    "                    minibatch_index + 1,\n",
    "                    n_train_batches,\n",
    "                    this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    # improve patience if loss improvement is good enough\n",
    "                    if (this_validation_loss < best_validation_loss *\n",
    "                            improvement_threshold):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = test_model()\n",
    "                    test_score = numpy.mean(test_losses, dtype='float64')\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                          test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(('Optimization complete with best validation score of %f %%, '\n",
    "           'obtained at iteration %i, '\n",
    "           'with test performance %f %%'\n",
    "           ) % (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "    print('The fine tuning code for file ' + os.path.split(__file__)[1] +\n",
    "          ' ran for %.2fm' % ((end_time - start_time) / 60.), sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_DBN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Comment:\n",
    "\n",
    "Note that one of the advantages of DBN is that it runs in a reasonable time (as it works on small number of labels) .. this allows it to avoid vanishing gradients problem .. \n",
    "\n",
    "\n",
    "DBN takes huge amount of time while trained on PC .. this is why it is recommened to run it on a free cloud .. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
