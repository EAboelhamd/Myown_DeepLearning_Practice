{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "In this notebook, I'm gonna utilize the most basic classifier (Logistic Regression) to classify MNIST dataset using Theano library .. It is one of the most effective python libraries for implementing deep learning with the ability to run the codes on a GPU .. \n",
    "\n",
    "This implementation is steming from the following tutorial .. \n",
    "http://deeplearning.net/tutorial/deeplearning.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano import *\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import timeit\n",
    "import logistic_sgd\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import six.moves.cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... building the model\n",
      "... training the model\n",
      "epoch 1, minibatch 83/83, validation error 12.458333 %\n",
      "     epoch 1, minibatch 83/83, test error of best model 12.375000 %\n",
      "epoch 2, minibatch 83/83, validation error 11.010417 %\n",
      "     epoch 2, minibatch 83/83, test error of best model 10.958333 %\n",
      "epoch 3, minibatch 83/83, validation error 10.312500 %\n",
      "     epoch 3, minibatch 83/83, test error of best model 10.312500 %\n",
      "epoch 4, minibatch 83/83, validation error 9.875000 %\n",
      "     epoch 4, minibatch 83/83, test error of best model 9.833333 %\n",
      "epoch 5, minibatch 83/83, validation error 9.562500 %\n",
      "     epoch 5, minibatch 83/83, test error of best model 9.479167 %\n",
      "epoch 6, minibatch 83/83, validation error 9.322917 %\n",
      "     epoch 6, minibatch 83/83, test error of best model 9.291667 %\n",
      "epoch 7, minibatch 83/83, validation error 9.187500 %\n",
      "     epoch 7, minibatch 83/83, test error of best model 9.000000 %\n",
      "epoch 8, minibatch 83/83, validation error 8.989583 %\n",
      "     epoch 8, minibatch 83/83, test error of best model 8.958333 %\n",
      "epoch 9, minibatch 83/83, validation error 8.937500 %\n",
      "     epoch 9, minibatch 83/83, test error of best model 8.812500 %\n",
      "epoch 10, minibatch 83/83, validation error 8.750000 %\n",
      "     epoch 10, minibatch 83/83, test error of best model 8.666667 %\n",
      "epoch 11, minibatch 83/83, validation error 8.666667 %\n",
      "     epoch 11, minibatch 83/83, test error of best model 8.520833 %\n",
      "epoch 12, minibatch 83/83, validation error 8.583333 %\n",
      "     epoch 12, minibatch 83/83, test error of best model 8.416667 %\n",
      "epoch 13, minibatch 83/83, validation error 8.489583 %\n",
      "     epoch 13, minibatch 83/83, test error of best model 8.291667 %\n",
      "epoch 14, minibatch 83/83, validation error 8.427083 %\n",
      "     epoch 14, minibatch 83/83, test error of best model 8.281250 %\n",
      "epoch 15, minibatch 83/83, validation error 8.354167 %\n",
      "     epoch 15, minibatch 83/83, test error of best model 8.270833 %\n",
      "epoch 16, minibatch 83/83, validation error 8.302083 %\n",
      "     epoch 16, minibatch 83/83, test error of best model 8.239583 %\n",
      "epoch 17, minibatch 83/83, validation error 8.250000 %\n",
      "     epoch 17, minibatch 83/83, test error of best model 8.177083 %\n",
      "epoch 18, minibatch 83/83, validation error 8.229167 %\n",
      "     epoch 18, minibatch 83/83, test error of best model 8.062500 %\n",
      "epoch 19, minibatch 83/83, validation error 8.260417 %\n",
      "epoch 20, minibatch 83/83, validation error 8.260417 %\n",
      "epoch 21, minibatch 83/83, validation error 8.208333 %\n",
      "     epoch 21, minibatch 83/83, test error of best model 7.947917 %\n",
      "epoch 22, minibatch 83/83, validation error 8.187500 %\n",
      "     epoch 22, minibatch 83/83, test error of best model 7.927083 %\n",
      "epoch 23, minibatch 83/83, validation error 8.156250 %\n",
      "     epoch 23, minibatch 83/83, test error of best model 7.958333 %\n",
      "epoch 24, minibatch 83/83, validation error 8.114583 %\n",
      "     epoch 24, minibatch 83/83, test error of best model 7.947917 %\n",
      "epoch 25, minibatch 83/83, validation error 8.093750 %\n",
      "     epoch 25, minibatch 83/83, test error of best model 7.947917 %\n",
      "epoch 26, minibatch 83/83, validation error 8.104167 %\n",
      "epoch 27, minibatch 83/83, validation error 8.104167 %\n",
      "epoch 28, minibatch 83/83, validation error 8.052083 %\n",
      "     epoch 28, minibatch 83/83, test error of best model 7.843750 %\n",
      "epoch 29, minibatch 83/83, validation error 8.052083 %\n",
      "epoch 30, minibatch 83/83, validation error 8.031250 %\n",
      "     epoch 30, minibatch 83/83, test error of best model 7.843750 %\n",
      "epoch 31, minibatch 83/83, validation error 8.010417 %\n",
      "     epoch 31, minibatch 83/83, test error of best model 7.833333 %\n",
      "epoch 32, minibatch 83/83, validation error 7.979167 %\n",
      "     epoch 32, minibatch 83/83, test error of best model 7.812500 %\n",
      "epoch 33, minibatch 83/83, validation error 7.947917 %\n",
      "     epoch 33, minibatch 83/83, test error of best model 7.739583 %\n",
      "epoch 34, minibatch 83/83, validation error 7.875000 %\n",
      "     epoch 34, minibatch 83/83, test error of best model 7.729167 %\n",
      "epoch 35, minibatch 83/83, validation error 7.885417 %\n",
      "epoch 36, minibatch 83/83, validation error 7.843750 %\n",
      "     epoch 36, minibatch 83/83, test error of best model 7.697917 %\n",
      "epoch 37, minibatch 83/83, validation error 7.802083 %\n",
      "     epoch 37, minibatch 83/83, test error of best model 7.635417 %\n",
      "epoch 38, minibatch 83/83, validation error 7.812500 %\n",
      "epoch 39, minibatch 83/83, validation error 7.812500 %\n",
      "epoch 40, minibatch 83/83, validation error 7.822917 %\n",
      "epoch 41, minibatch 83/83, validation error 7.791667 %\n",
      "     epoch 41, minibatch 83/83, test error of best model 7.625000 %\n",
      "epoch 42, minibatch 83/83, validation error 7.770833 %\n",
      "     epoch 42, minibatch 83/83, test error of best model 7.614583 %\n",
      "epoch 43, minibatch 83/83, validation error 7.750000 %\n",
      "     epoch 43, minibatch 83/83, test error of best model 7.593750 %\n",
      "epoch 44, minibatch 83/83, validation error 7.739583 %\n",
      "     epoch 44, minibatch 83/83, test error of best model 7.593750 %\n",
      "epoch 45, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 46, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 47, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 48, minibatch 83/83, validation error 7.708333 %\n",
      "     epoch 48, minibatch 83/83, test error of best model 7.583333 %\n",
      "epoch 49, minibatch 83/83, validation error 7.677083 %\n",
      "     epoch 49, minibatch 83/83, test error of best model 7.572917 %\n",
      "epoch 50, minibatch 83/83, validation error 7.677083 %\n",
      "epoch 51, minibatch 83/83, validation error 7.677083 %\n",
      "epoch 52, minibatch 83/83, validation error 7.656250 %\n",
      "     epoch 52, minibatch 83/83, test error of best model 7.541667 %\n",
      "epoch 53, minibatch 83/83, validation error 7.656250 %\n",
      "epoch 54, minibatch 83/83, validation error 7.635417 %\n",
      "     epoch 54, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 55, minibatch 83/83, validation error 7.635417 %\n",
      "epoch 56, minibatch 83/83, validation error 7.635417 %\n",
      "epoch 57, minibatch 83/83, validation error 7.604167 %\n",
      "     epoch 57, minibatch 83/83, test error of best model 7.489583 %\n",
      "epoch 58, minibatch 83/83, validation error 7.583333 %\n",
      "     epoch 58, minibatch 83/83, test error of best model 7.458333 %\n",
      "epoch 59, minibatch 83/83, validation error 7.572917 %\n",
      "     epoch 59, minibatch 83/83, test error of best model 7.468750 %\n",
      "epoch 60, minibatch 83/83, validation error 7.572917 %\n",
      "epoch 61, minibatch 83/83, validation error 7.583333 %\n",
      "epoch 62, minibatch 83/83, validation error 7.572917 %\n",
      "     epoch 62, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 63, minibatch 83/83, validation error 7.562500 %\n",
      "     epoch 63, minibatch 83/83, test error of best model 7.510417 %\n",
      "epoch 64, minibatch 83/83, validation error 7.572917 %\n",
      "epoch 65, minibatch 83/83, validation error 7.562500 %\n",
      "epoch 66, minibatch 83/83, validation error 7.552083 %\n",
      "     epoch 66, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 67, minibatch 83/83, validation error 7.552083 %\n",
      "epoch 68, minibatch 83/83, validation error 7.531250 %\n",
      "     epoch 68, minibatch 83/83, test error of best model 7.520833 %\n",
      "epoch 69, minibatch 83/83, validation error 7.531250 %\n",
      "epoch 70, minibatch 83/83, validation error 7.510417 %\n",
      "     epoch 70, minibatch 83/83, test error of best model 7.500000 %\n",
      "epoch 71, minibatch 83/83, validation error 7.520833 %\n",
      "epoch 72, minibatch 83/83, validation error 7.510417 %\n",
      "epoch 73, minibatch 83/83, validation error 7.500000 %\n",
      "     epoch 73, minibatch 83/83, test error of best model 7.489583 %\n",
      "Optimization complete with best validation score of 7.500000 %,with test performance 7.489583 %\n",
      "The code run for 74 epochs, with 3.964741 epochs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code for file logistic_sgd.pyc ran for 18.7s\n"
     ]
    }
   ],
   "source": [
    "logistic_sgd.sgd_optimization_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the call for the already exist implementation .. \n",
    "\n",
    "Let's try to write this code bit by bit for better understanding .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input is X .. while n_in is the number of nodes\n",
    "class LogisticRegression(object):\n",
    "    def _init_(self, input, n_in, n_out):\n",
    "            ## init the weights\n",
    "        self.W = theano.shared(value = np.zeros((n_in, n_out), dtype = theano.config.floatX), name = 'W', borrow = True)\n",
    "\n",
    "            ## init the biases\n",
    "        self.b = theano.shared(value=np.zeros((n_out,), dtype=theano.config.floatX), name='b', borrow=True)\n",
    "\n",
    "            ## activition function \n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "            ## Prediction\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1) # each input will be assigned to the class that has the highest prob\n",
    "\n",
    "            ## Parameters of the model are weights and biases..\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        \n",
    "    def negative_log_likelihood(self, y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "        \n",
    "        \n",
    "    def errors(self, y):\n",
    "             # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "                )\n",
    "            # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- p_y_given_x is a symbolic variable of vector-type .. resulted from the dot product of x and W and softmax activation function ..\n",
    "\n",
    "- T.argmax returns the index at which p_y_given_x is maximal i.e. the class with maximum probability .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "argmax"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## testing the function ..\n",
    "input = [1, 20, 3, 4]\n",
    "n_in = 4\n",
    "n_out = 2\n",
    "\n",
    "_init_(input, n_in, n_out)  ## the model doesn't do anything useful so far !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Loss Function:\n",
    "The learning phase starts by minimizing the loss function .. it is very common to use the negative log-likelihood as the loss (i.e. maximizing the likelihood of the data set) ..\n",
    "\n",
    "-ve Log Likelihood ==> Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "        \n",
    "the mean is chosen instead of the sum so that the learning rate is less dependent on the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## this is a redundant implementation .. just to call the function and check it run correctly without errors ..\n",
    "def negative_log_likelihood(y):\n",
    "    return -T.mean(T.log(p_y_given_x)[T.arange(y.shape[0]), y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "errors() function ==> Return a float representing the number of errors in the minibatch\n",
    "        over the total number of examples of the minibatch ; zero one\n",
    "        loss over the size of the minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## this is a redundant implementation .. just to call the function and check it run correctly without errors ..\n",
    "def errors(y):\n",
    "     # check if y has same dimension of y_pred\n",
    "    if y.ndim != y_pred.ndim:\n",
    "        raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "    if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "        return T.mean(T.neq(y_pred, y))\n",
    "    else:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data:\n",
    "\n",
    "Note that:\n",
    "\n",
    "six.moves.cPickle is utilized here to provides simple utilities for wrapping over differences between Python 2 and Python 3. It is intended to support codebases that work on both Python 2 and 3 without modification. six consists of only one Python file, so it is painless to copy into a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):  ## input data is a string\n",
    "    dataset = '/home/eman/PhD/Deep Learning Practice/Myown practice/mnist.pkl.gz'\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "        try:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        except:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    "#     return train_set, valid_set, test_set\n",
    "\n",
    "    def shared_dataset(data_xy, borrow=True):\n",
    "        \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "        The reason we store our dataset in shared variables is to allow\n",
    "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "        is needed (the default behaviour if the data is not in a shared\n",
    "        variable) would lead to a large decrease in performance.\n",
    "        \"\"\"\n",
    "        data_x, data_y = data_xy\n",
    "        shared_x = theano.shared(np.asarray(data_x,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        shared_y = theano.shared(np.asarray(data_y,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        # When storing data on the GPU it has to be stored as floats\n",
    "        # therefore we will store the labels as ``floatX`` as well\n",
    "        # (``shared_y`` does exactly that). But during our computations\n",
    "        # we need them as ints (we use labels as index, and if they are\n",
    "        # floats it doesn't make sense) therefore instead of returning\n",
    "        # ``shared_y`` we will have to cast it to int. This little hack\n",
    "        # lets ous get around this issue\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "            (test_set_x, test_set_y)]\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent:\n",
    "For optimizing the variables of the classification model .. \n",
    "\n",
    "This function demonstrate stochastic gradient descent optimization of a log-linear\n",
    "    model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_optimization_mnist():\n",
    "    learning_rate = 0.13\n",
    "    n_epochs = 1000\n",
    "    dataset = 'mnist.pkl.gz'\n",
    "    batch_size = 600\n",
    "    this_validation_loss = 0\n",
    "    \n",
    "    datasets = load_data(dataset)\n",
    "    \n",
    "    #the dataset is divided into train, validation and test sets\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    \n",
    "#     print train_set_x\n",
    "    \n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "        \n",
    "     ####################\n",
    "    # BUILDING THE MODEL #\n",
    "    ####################\n",
    "    \n",
    "    ## We have to allocate symbolic variables for the data, inputs and outputs .. \n",
    "    \n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    # generate symbolic variables for input (x and y represent a minibatch)\n",
    "    x = T.matrix('x')  # data, presented as rasterized images\n",
    "    y = T.ivector('y')  # labels, presented as 1D vector of [int] labels\n",
    "    \n",
    "    # Initialization .. Each MNIST image has size 28*28\n",
    "    classifier = LogisticRegression() \n",
    "    classifier._init_(input=x, n_in=28 * 28, n_out=10) # each output corresponding to digit \n",
    "    \n",
    "    # Define the cost function ..\n",
    "    cost = classifier.negative_log_likelihood(y)\n",
    "\n",
    "     ## testing the model \n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs = classifier.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        })\n",
    "    \n",
    "        \n",
    "    ## Validate the model .. \n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs = classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "  \n",
    "        \n",
    "#     # compute the gradient of cost with respect to theta = (W,b)\n",
    "    g_W = T.grad(cost = cost, wrt = classifier.W)\n",
    "    g_b = T.grad(cost = cost, wrt = classifier.b)\n",
    "        \n",
    "#     ## Model updates .. \n",
    "    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "     \n",
    "#     ## Model  training .. \n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "#     ###############\n",
    "#     # TRAIN MODEL #\n",
    "#     ###############\n",
    "        \n",
    "    patience = 5000  # look as this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is considered significant\n",
    "    validation_frequency = min(n_train_batches, patience//2)\n",
    "#                                   # go through this many\n",
    "#                                   # minibatche before checking the network\n",
    "#                                   # on the validation set; in this case we\n",
    "#                                   # check every epoch\n",
    "        \n",
    "    best_validation_loss = np.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "        \n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    \n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            minibatch_avg_cost = train_model(minibatch_index)  ## i.e. minibatch avg cost: 0.291112316698\n",
    "            \n",
    "#             # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "#                 # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i) for i in range(n_valid_batches)]\n",
    "                this_validation_loss = np.mean(validation_losses)  ## i.e. this valid_loss: 0.0720833333333\n",
    "    \n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                      (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "     # if we got the best validation score until now\n",
    "            if (this_validation_loss < best_validation_loss):\n",
    "                #improve patience if loss improvement is good enough\n",
    "                if this_validation_loss < best_validation_loss *  \\\n",
    "                    improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                        \n",
    "                best_validation_loss = this_validation_loss\n",
    "                    \n",
    "            # test it on the test set\n",
    "\n",
    "                test_losses = [test_model(i) for i in range(n_test_batches)]\n",
    "                test_score = np.mean(test_losses)\n",
    "\n",
    "                print(\n",
    "                        (\n",
    "                            ' epoch %i, minibatch %i/%i, test error of'\n",
    "                            ' best model %f %%'\n",
    "                        ) %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            n_train_batches,\n",
    "                            test_score * 100.\n",
    "                        )\n",
    "                    )\n",
    "        \n",
    "#             # save the best model\n",
    "            with open('best_model.pkl', 'wb') as f:\n",
    "                pickle.dump(classifier, f)\n",
    "        \n",
    "        if patience <= iter:\n",
    "            done_looping = True\n",
    "            break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    \n",
    "    ## printting the results ..\n",
    "    print((\n",
    "            'Optimization complete with best validation score of %f %%,'\n",
    "            'with test performance %f %%'\n",
    "        )% (best_validation_loss * 100., test_score * 100.))\n",
    "    print('The code run for %d epochs, with %f epochs/sec' % (\n",
    "        epoch, 1. * epoch / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Example .. test mnist dataset\n",
    "\n",
    "def predict():\n",
    "    \n",
    "    # load the saved model\n",
    "    classifier = pickle.load(open('best_model.pkl'))\n",
    "    \n",
    "     # compile a predictor function\n",
    "    predict_model = theano.function(\n",
    "        inputs=[classifier.input],\n",
    "        outputs=classifier.y_pred)\n",
    "    \n",
    "    # We can test it on some examples from test test\n",
    "    dataset='mnist.pkl.gz'\n",
    "    datasets = load_data(dataset)\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    test_set_x = test_set_x.get_value()\n",
    "    \n",
    "    predicted_values = predict_model(test_set_x[:10])\n",
    "    print(\"Predicted values for the first 10 examples in test set:\")\n",
    "    print(predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 1, minibatch 1/83, test error of best model 41.385417 %\n",
      "epoch 1, minibatch 83/83, validation error 12.458333 %\n",
      "epoch 2, minibatch 83/83, validation error 11.010417 %\n",
      "epoch 3, minibatch 83/83, validation error 10.312500 %\n",
      "epoch 4, minibatch 83/83, validation error 9.875000 %\n",
      "epoch 5, minibatch 83/83, validation error 9.562500 %\n",
      "epoch 6, minibatch 83/83, validation error 9.322917 %\n",
      "epoch 7, minibatch 83/83, validation error 9.187500 %\n",
      "epoch 8, minibatch 83/83, validation error 8.989583 %\n",
      "epoch 9, minibatch 83/83, validation error 8.937500 %\n",
      "epoch 10, minibatch 83/83, validation error 8.750000 %\n",
      "epoch 11, minibatch 83/83, validation error 8.666667 %\n",
      "epoch 12, minibatch 83/83, validation error 8.583333 %\n",
      "epoch 13, minibatch 83/83, validation error 8.489583 %\n",
      "epoch 14, minibatch 83/83, validation error 8.427083 %\n",
      "epoch 15, minibatch 83/83, validation error 8.354167 %\n",
      "epoch 16, minibatch 83/83, validation error 8.302083 %\n",
      "epoch 17, minibatch 83/83, validation error 8.250000 %\n",
      "epoch 18, minibatch 83/83, validation error 8.229167 %\n",
      "epoch 19, minibatch 83/83, validation error 8.260417 %\n",
      "epoch 20, minibatch 83/83, validation error 8.260417 %\n",
      "epoch 21, minibatch 83/83, validation error 8.208333 %\n",
      "epoch 22, minibatch 83/83, validation error 8.187500 %\n",
      "epoch 23, minibatch 83/83, validation error 8.156250 %\n",
      "epoch 24, minibatch 83/83, validation error 8.114583 %\n",
      "epoch 25, minibatch 83/83, validation error 8.093750 %\n",
      "epoch 26, minibatch 83/83, validation error 8.104167 %\n",
      "epoch 27, minibatch 83/83, validation error 8.104167 %\n",
      "epoch 28, minibatch 83/83, validation error 8.052083 %\n",
      "epoch 29, minibatch 83/83, validation error 8.052083 %\n",
      "epoch 30, minibatch 83/83, validation error 8.031250 %\n",
      "epoch 31, minibatch 83/83, validation error 8.010417 %\n",
      "epoch 32, minibatch 83/83, validation error 7.979167 %\n",
      "epoch 33, minibatch 83/83, validation error 7.947917 %\n",
      "epoch 34, minibatch 83/83, validation error 7.875000 %\n",
      "epoch 35, minibatch 83/83, validation error 7.885417 %\n",
      "epoch 36, minibatch 83/83, validation error 7.843750 %\n",
      "epoch 37, minibatch 83/83, validation error 7.802083 %\n",
      "epoch 38, minibatch 83/83, validation error 7.812500 %\n",
      "epoch 39, minibatch 83/83, validation error 7.812500 %\n",
      "epoch 40, minibatch 83/83, validation error 7.822917 %\n",
      "epoch 41, minibatch 83/83, validation error 7.791667 %\n",
      "epoch 42, minibatch 83/83, validation error 7.770833 %\n",
      "epoch 43, minibatch 83/83, validation error 7.750000 %\n",
      "epoch 44, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 45, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 46, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 47, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 48, minibatch 83/83, validation error 7.708333 %\n",
      "epoch 49, minibatch 83/83, validation error 7.677083 %\n",
      "epoch 50, minibatch 83/83, validation error 7.677083 %\n",
      "epoch 51, minibatch 83/83, validation error 7.677083 %\n",
      "epoch 52, minibatch 83/83, validation error 7.656250 %\n",
      "epoch 53, minibatch 83/83, validation error 7.656250 %\n",
      "epoch 54, minibatch 83/83, validation error 7.635417 %\n",
      "epoch 55, minibatch 83/83, validation error 7.635417 %\n",
      "epoch 56, minibatch 83/83, validation error 7.635417 %\n",
      "epoch 57, minibatch 83/83, validation error 7.604167 %\n",
      "epoch 58, minibatch 83/83, validation error 7.583333 %\n",
      "epoch 59, minibatch 83/83, validation error 7.572917 %\n",
      "epoch 60, minibatch 83/83, validation error 7.572917 %\n",
      "epoch 61, minibatch 83/83, validation error 7.583333 %\n",
      "Optimization complete with best validation score of 0.000000 %,with test performance 41.385417 %\n",
      "The code run for 61 epochs, with 1.482160 epochs/sec\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    sgd_optimization_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment:\n",
    "\n",
    "On an Intel(R) Core(TM)2 Duo CPU E8400 @ 3.00 Ghz the code runs with approximately 1.936 epochs/sec\n",
    "and it took 75 epochs to reach a test error of 7.489%. On the GPU the code does almost 10.0 epochs/sec.\n",
    "For this instance we used a batch size of 600. \n",
    "\n",
    "However, on my laptop the code run for 61 epochs within 1.482160 eps/sec\n",
    "\n",
    "This is the end of training MNIST dataset using simple logistic regression model and SGD .. The next step is to try MLP model and compare the results of both."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
