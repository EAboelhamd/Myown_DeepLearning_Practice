{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "This notebook preesents an implementation for RESTRICTED BOLTZMANN MACHINES (RBM) using Theano tested on MNIST dataset \n",
    "\n",
    "RBM can find patterns in the data by reconstructing the inputs ..\n",
    "\n",
    "It belongs to the calss of autoencoders with only two layers (visibile and hidden) where the assessement of the network is performed on the visible layer (i.e. the backword prop. results is compared to the original visible layer)\n",
    "\n",
    "The feature extaction is performed automatically .. this is why RBM belongs to the feature extactor neural nets.\n",
    "\n",
    "Boltzmann Machines (BMs) are a particular form of energy-based model which contain hidden variables. Restricted Boltzmann Machines further restrict BMs to those without visible-visible and hidden-hidden connections (Loosly connected) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "try:\n",
    "    import PIL.Image as Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import os\n",
    "\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "import utils\n",
    "from logistic_sgd import load_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBM class consists of the following functions:\n",
    "1. init \n",
    "2. free_energy\n",
    "3. propup ==> propgates the visible units activation towards the hidden units \n",
    "4. sample_h_given_v ==> sampling step \n",
    "5. propdown  ==> propagates the hidden units activation to the visible ones\n",
    "6. sample_v_given_h ==> sampling step \n",
    "7. gibbs_hvh\n",
    "8. gibbs_vhv\n",
    "9. get_cost_updates\n",
    "10. get_pseudo_likelihood_cost\n",
    "11. get_reconstruction_cost\n",
    "\n",
    "Note that .. Block Gibbs sampling, allows the conditional distributions p(hjv) and\n",
    "p(vjh) to be used as the transition operators of the Markov chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RBM(object):\n",
    "    def __init__(self, input=None, n_visible=784, n_hidden=500, W=None, hbias=None, vbias=None, numpy_rng=None,\n",
    "                 theano_rng=None):\n",
    "        \n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        # create a number generator\n",
    "        if numpy_rng is None:\n",
    "            numpy_rng = numpy.random.RandomState(1234)\n",
    "        \n",
    "        if theano_rng is None:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "            \n",
    "        # W is initialized with `initial_W` which is uniformely\n",
    "        # sampled from -4*sqrt(6./(n_visible+n_hidden)) and\n",
    "        # 4*sqrt(6./(n_hidden+n_visible)) the output of uniform if\n",
    "        # converted using asarray to dtype theano.config.floatX so that the code is runable on GPU\n",
    "        \n",
    "        if W is None:\n",
    "            initial_W = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            # theano shared variables for weights and biases\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "            \n",
    "        if hbias is None:\n",
    "            # create shared variable for hidden units bias\n",
    "            hbias = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX),name='hbias', borrow=True)\n",
    "            \n",
    "        if vbias is None:\n",
    "            # create shared variable for hidden units bias\n",
    "            vbias = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX),name='hbias', borrow=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "        # initialize input layer for standalone RBM or layer0 of DBN\n",
    "        self.input = input\n",
    "        if not input:\n",
    "            self.input = T.matrix('input')\n",
    "        \n",
    "        ## fill in shared variables \n",
    "        self.W = W\n",
    "        self.hbias = hbias\n",
    "        self.vbias = vbias\n",
    "        self.theano_rng = theano_rng\n",
    "        \n",
    "        self.params = [self.W, self.hbias, self.vbias]\n",
    "        \n",
    "        \n",
    "     # Compute the free energy '''\n",
    "    def free_energy(self, v_sample):\n",
    "        wx_b = T.dot(v_sample, self.W) + self.hbias\n",
    "        vbias_term = T.dot(v_sample, self.vbias)\n",
    "        hidden_term = T.sum(T.log(1 + T.exp(wx_b)), axis=1)\n",
    "        return -hidden_term - vbias_term\n",
    "    \n",
    "    ## propagate from visible to hidden \n",
    "    def propup(self, vis):\n",
    "        pre_sigmoid_activation = T.dot(vis, self.W) + self.hbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]  \n",
    "    \n",
    "#     This function propagates the hidden units activation downwards to\n",
    "#         the visible units\n",
    "\n",
    "#         Note that we return also the pre_sigmoid_activation of the\n",
    "#         layer. As it will turn out later, due to how Theano deals with\n",
    "#         optimizations, this symbolic variable will be needed to write\n",
    "#         down a more stable computational graph (see details in the\n",
    "#         reconstruction cost function)\n",
    "\n",
    "    def propdown(self, hid):\n",
    "        pre_sigmoid_activation = T.dot(hid, self.W.T) + self.vbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "    \n",
    "    \n",
    "    # infers set of statistics of hidden units given visible units\n",
    "    # it computes the activation of the hidden units given a sample of pre_sigmoid_h1, h1_mean = self.propup(v0_sample)the visibles\n",
    "    def sample_h_given_v(self, v0_sample):\n",
    "        pre_sigmoid_h1, h1_mean = self.propup(v0_sample)\n",
    "        h1_sample = self.theano_rng.binomial(size=h1_mean.shape,\n",
    "                                             n=1, p=h1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "    \n",
    "    \n",
    "    #  This function infers state of visible units given hidden units\n",
    "    def sample_v_given_h(self, h0_sample): \n",
    "        # compute the activation of the visible given the hidden sample\n",
    "        pre_sigmoid_v1, v1_mean = self.propdown(h0_sample)\n",
    "        # get a sample of the visible given their activation\n",
    "        # Note that theano_rng.binomial returns a symbolic sample of dtype\n",
    "        # int64 by default. If we want to keep our computations in floatX\n",
    "        # for the GPU we need to specify to return the dtype floatX\n",
    "        v1_sample = self.theano_rng.binomial(size=v1_mean.shape,\n",
    "                                             n=1, p=v1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "    \n",
    "    ## calculate one step of Gibbs sampling, starting from the hidden state\n",
    "    def gibbs_hvh(self, h0_sample):\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample, pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "    \n",
    "    \n",
    "    ## calculate one step of Gibbs sampling, starting from the visible state\n",
    "    def gibbs_vhv(self, v0_sample):\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample, pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "    \n",
    "    \n",
    "    # update the paramters with the help of CD-k or PCD-k\n",
    "#      Returns a proxy for the cost and the updates dictionary. The\n",
    "#         dictionary contains the update rules for weights and biases but\n",
    "#         also an update of the shared variable used to store the persistent\n",
    "#         chain, if one is used.\n",
    "    def get_cost_updates(self, lr=0.1, persistent=None, k=1):\n",
    "        # compute positive phase\n",
    "        pre_sigmoid_ph, ph_mean, ph_sample = self.sample_h_given_v(self.input)\n",
    "        \n",
    "        # decide how to initialize persistent chain:\n",
    "        # for CD, we use the newly generate hidden sample\n",
    "        # for PCD, we initialize from the old state of the chain\n",
    "        if persistent is None:\n",
    "            chain_start = ph_sample\n",
    "        else:\n",
    "            chain_start = persistent\n",
    "        \n",
    "        \n",
    "        # perform actual negative phase\n",
    "        # in order to implement CD-k/PCD-k we need to scan over the\n",
    "        # function that implements one gibbs step k times.\n",
    "        \n",
    "         # the scan will return the entire Gibbs chain\n",
    "        ([\n",
    "                pre_sigmoid_nvs,\n",
    "                nv_means,\n",
    "                nv_samples,\n",
    "                pre_sigmoid_nhs,\n",
    "                nh_means,\n",
    "                nh_samples\n",
    "            ],\n",
    "            updates\n",
    "        ) = theano.scan(\n",
    "            self.gibbs_hvh,\n",
    "            # the None are place holders, saying that\n",
    "            # chain_start is the initial state corresponding to the\n",
    "            # 6th output\n",
    "            outputs_info=[None, None, None, None, None, chain_start],\n",
    "            n_steps=k,\n",
    "            name=\"gibbs_hvh\")\n",
    "        \n",
    "        \n",
    "        # determine gradients on RBM parameters\n",
    "        # note that we only need the sample at the end of the chain\n",
    "\n",
    "        chain_end = nv_samples[-1]\n",
    "        cost = T.mean(self.free_energy(self.input)) - T.mean(self.free_energy(chain_end))\n",
    "        \n",
    "        # We must not compute the gradient through the gibbs sampling\n",
    "        gparams = T.grad(cost, self.params, consider_constant=[chain_end])\n",
    "        \n",
    "        # constructs the update dictionary\n",
    "        for gparam, param in zip(gparams, self.params):\n",
    "            # make sure that the learning rate is of the right dtype\n",
    "            updates[param] = param - gparam * T.cast(lr, dtype=theano.config.floatX)\n",
    "            \n",
    "        if persistent:\n",
    "            # Note that this works only if persistent is a shared variable\n",
    "            updates[persistent] = nh_samples[-1]\n",
    "            # pseudo-likelihood is a better proxy for PCD\n",
    "            monitoring_cost = self.get_pseudo_likelihood_cost(updates)\n",
    "        else:\n",
    "            # reconstruction cross-entropy is a better proxy for CD\n",
    "            monitoring_cost = self.get_reconstruction_cost(updates, pre_sigmoid_nvs[-1])\n",
    "        \n",
    "        return monitoring_cost, updates\n",
    "\n",
    "    \n",
    "    def get_pseudo_likelihood_cost(self, updates):\n",
    "        # index of bit i in expression p(x_i | x_{\\i})\n",
    "        bit_i_idx = theano.shared(value=0, name='bit_i_idx')\n",
    "\n",
    "        # Rounding the input image to nearest integer\n",
    "        xi = T.round(self.input)\n",
    "\n",
    "        # calculate free energy for the given bit configuration\n",
    "        fe_xi = self.free_energy(xi)\n",
    "        \n",
    "        # flip bit x_i of matrix xi and preserve all other bits x_{\\i}\n",
    "        # Equivalent to xi[:,bit_i_idx] = 1-xi[:, bit_i_idx], but assigns\n",
    "        # the result to xi_flip, instead of working in place on xi.\n",
    "        xi_flip = T.set_subtensor(xi[:, bit_i_idx], 1 - xi[:, bit_i_idx])\n",
    "        \n",
    "        # calculate free energy with bit flipped\n",
    "        fe_xi_flip = self.free_energy(xi_flip)\n",
    "        \n",
    "        # equivalent to e^(-FE(x_i)) / (e^(-FE(x_i)) + e^(-FE(x_{\\i})))\n",
    "        cost = T.mean(self.n_visible * T.log(T.nnet.sigmoid(fe_xi_flip - fe_xi)))\n",
    "        \n",
    "        # increment bit_i_idx % number as part of updates\n",
    "        updates[bit_i_idx] = (bit_i_idx + 1) % self.n_visible\n",
    "        return cost\n",
    "    \n",
    "    ## Approximation to the reconstruction error\n",
    "    def get_reconstruction_cost(self, updates, pre_sigmoid_nv):\n",
    "        cross_entropy = T.mean(\n",
    "            T.sum(\n",
    "                self.input * T.log(T.nnet.sigmoid(pre_sigmoid_nv)) +\n",
    "                (1 - self.input) * T.log(1 - T.nnet.sigmoid(pre_sigmoid_nv)),\n",
    "                axis=1))\n",
    "        return cross_entropy        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RBM:\n",
    "\n",
    "The following class is devoted to train, sample RBM on MNIST dataset .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_rbm(learning_rate=0.1, training_epochs=15,\n",
    "             dataset='mnist.pkl.gz', batch_size=20,\n",
    "             n_chains=20, n_samples=10, output_folder='rbm_plots',\n",
    "             n_hidden=500):\n",
    "    \n",
    "    datasets = load_data(dataset)\n",
    "    \n",
    "    ## we do not have validation set ! :S\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    \n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    \n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()    # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "\n",
    "    rng = numpy.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    # initialize storage for the persistent chain (state = hidden layer of chain)\n",
    "    persistent_chain = theano.shared(numpy.zeros((batch_size, n_hidden),\n",
    "                                                 dtype=theano.config.floatX), borrow=True)\n",
    "    \n",
    "    # construct the RBM class\n",
    "    rbm = RBM(input=x, n_visible=28*28, n_hidden=n_hidden, numpy_rng=rng, theano_rng=theano_rng)\n",
    "\n",
    "    # get the cost and the gradient corresponding to one step of CD-15\n",
    "    cost, updates = rbm.get_cost_updates(lr=learning_rate, persistent=persistent_chain, k=15)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #################################\n",
    "    #     Training the RBM          #\n",
    "    #################################\n",
    "    \n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    os.chdir(output_folder)\n",
    "\n",
    "    # it is ok for a theano function to have no output\n",
    "    # the purpose of train_rbm is solely to update the RBM parameters\n",
    "    train_rbm = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
    "        },\n",
    "        name='train_rbm'\n",
    "    )\n",
    "\n",
    "    plotting_time = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    # go through training epochs\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # go through the training set\n",
    "        mean_cost = []\n",
    "        for batch_index in range(n_train_batches):\n",
    "            mean_cost += [train_rbm(batch_index)]\n",
    "\n",
    "        print('Training epoch %d, cost is ' % epoch, numpy.mean(mean_cost))\n",
    "\n",
    "        # Plot filters after each training epoch\n",
    "        plotting_start = timeit.default_timer()\n",
    "        # Construct image from the weight matrix\n",
    "        image = Image.fromarray(\n",
    "            utils.tile_raster_images(\n",
    "                X=rbm.W.get_value(borrow=True).T,\n",
    "                img_shape=(28, 28),\n",
    "                tile_shape=(10, 10),\n",
    "                tile_spacing=(1, 1)\n",
    "            )\n",
    "        )\n",
    "        image.save('filters_at_epoch_%i.png' % epoch)\n",
    "        plotting_stop = timeit.default_timer()\n",
    "        plotting_time += (plotting_stop - plotting_start)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    pretraining_time = (end_time - start_time) - plotting_time\n",
    "\n",
    "    print ('Training took %f minutes' % (pretraining_time / 60.))\n",
    "  \n",
    "    #################################\n",
    "    #     Sampling from the RBM     #\n",
    "    #################################\n",
    "    \n",
    "    # find out the number of test samples\n",
    "    number_of_test_samples = test_set_x.get_value(borrow=True).shape[0]\n",
    "\n",
    "    # pick random test examples, with which to initialize the persistent chain\n",
    "    test_idx = rng.randint(number_of_test_samples - n_chains)\n",
    "    persistent_vis_chain = theano.shared(\n",
    "        numpy.asarray(\n",
    "            test_set_x.get_value(borrow=True)[test_idx:test_idx + n_chains],\n",
    "            dtype=theano.config.floatX\n",
    "        ))\n",
    "    \n",
    "    plot_every = 1000\n",
    "    \n",
    "    # define one step of Gibbs sampling (mf = mean-field) define a\n",
    "    # function that does `plot_every` steps before returning the\n",
    "    # sample for plotting\n",
    "    ([\n",
    "            presig_hids,\n",
    "            hid_mfs,\n",
    "            hid_samples,\n",
    "            presig_vis,\n",
    "            vis_mfs,\n",
    "            vis_samples\n",
    "        ],\n",
    "        updates\n",
    "    ) = theano.scan(\n",
    "        rbm.gibbs_vhv,\n",
    "        outputs_info=[None, None, None, None, None, persistent_vis_chain],\n",
    "        n_steps=plot_every,\n",
    "        name=\"gibbs_vhv\")\n",
    "\n",
    "    # add to updates the shared variable that takes care of our persistent chain :.\n",
    "    updates.update({persistent_vis_chain: vis_samples[-1]})\n",
    "    # construct the function that implements our persistent chain.\n",
    "    # we generate the \"mean field\" activations for plotting and the actual\n",
    "    # samples for reinitializing the state of our persistent chain\n",
    "    sample_fn = theano.function(\n",
    "        [],\n",
    "        [vis_mfs[-1], vis_samples[-1]], updates=updates, name='sample_fn')\n",
    "\n",
    "    # create a space to store the image for plotting ( we need to leave\n",
    "    # room for the tile_spacing as well)\n",
    "    image_data = numpy.zeros((29 * n_samples + 1, 29 * n_chains - 1), dtype='uint8')\n",
    "    \n",
    "    for idx in range(n_samples):\n",
    "        # generate `plot_every` intermediate samples that we discard,\n",
    "        # because successive samples in the chain are too correlated\n",
    "        vis_mf, vis_sample = sample_fn()\n",
    "        print(' ... plotting sample %d' % idx)\n",
    "        image_data[29 * idx:29 * idx + 28, :] = utils.tile_raster_images(\n",
    "            X=vis_mf,\n",
    "            img_shape=(28, 28),\n",
    "            tile_shape=(1, n_chains),\n",
    "            tile_spacing=(1, 1)\n",
    "        )\n",
    "\n",
    "    # construct image\n",
    "    image = Image.fromarray(image_data)\n",
    "    image.save('samples.png')\n",
    "    os.chdir('/home/eman/PhD/Deep Learning Practice/Myown practice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_rbm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment:\n",
    "Although RBM already output some of the results .. it takes huge amount of time while trained on PC .. this is why it is recommened to run it on a free cloud .. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
