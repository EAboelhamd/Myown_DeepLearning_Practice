{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "Main Goals:\n",
    "\n",
    "1. Identify the recipients that will engage with the campaign.\n",
    "2. Maximise the campaignâ€™s revenue.\n",
    "\n",
    "\n",
    "Comments\n",
    "\n",
    "- The dataset contains only 5% of donors.\n",
    "- The donations are usually smaller than $20.\n",
    "- This data is quite noisy, high dimensional.\n",
    "- There is an inverse relationship between the probability to donate and the amount donated.\n",
    "\n",
    "\n",
    "Link for dataset and some analysis ==> \n",
    "\n",
    "https://github.com/rebordao/kdd98cup\n",
    "\n",
    "https://github.com/bobbyantonio/KDD98/blob/master/CleanData.py\n",
    "\n",
    "- Github solutions ==>\n",
    "https://github.com/rebordao/kdd98cup\n",
    "\n",
    "\n",
    "- Siraj notebook for a better data visualization:\n",
    "\n",
    "https://www.youtube.com/watch?v=yQsOFWqpjkE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from sklearn.model_selection import train_test_split\n",
    "from array_split import array_split, shape_split\n",
    "from sklearn import preprocessing\n",
    "# from sknn.mlp import Regressor, Layer\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "## plotting .. \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "## warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    list_of_lists = []\n",
    "\n",
    "    ## works correctly but takes too much running time ..\n",
    "    with open('cup98LRN.txt', 'r') as f:# open the file for reading\n",
    "        df = []\n",
    "        for row_num, line in enumerate(f):\n",
    "            # Remove the new line at the end and then split the string based on\n",
    "            # tabs. This creates a python list of the values.\n",
    "            values = line.strip().split(',')\n",
    "            if row_num == 0: # first line is the header\n",
    "                 header = values\n",
    "            else:\n",
    "                df.append([v for v in values])\n",
    "\n",
    "        df = pd.DataFrame(df)\n",
    "        df.columns = header\n",
    "        df.drop(df.index[0], inplace=True)\n",
    "        \n",
    "        df = df[0:200]  ## to save time .. I gonna work on only 100 records ..\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I have to run it over all the data not just 100 records\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(df['ADATE_3']) # 3 empty cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ADATE_7'].values.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only ADATE is valid .. \n",
    "\n",
    "RFA ==> only 3 characters not 4\n",
    "\n",
    "RAMNT_3, RDATE ==> are empty !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_data = ['ADATE_2', 'ADATE_3', 'ADATE_4', 'ADATE_5', 'ADATE_6', 'ADATE_7', 'ADATE_8', \n",
    "            'ADATE_9', 'ADATE_10', 'ADATE_11', 'ADATE_12', 'ADATE_13', 'ADATE_14', 'ADATE_15',\n",
    "            'ADATE_16', 'ADATE_16', 'ADATE_17', 'ADATE_18', 'ADATE_19', 'ADATE_20', 'ADATE_12',\n",
    "            'ADATE_22', 'ADATE_23', 'ADATE_24']\n",
    "df[date_data].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "temp = np.transpose(df[date_data])\n",
    "\n",
    "for j in xrange(len(temp)):\n",
    "    for i in xrange(np.shape(df)[0]):\n",
    "         t.append(temp.iloc[j].iloc[i][2:])\n",
    "\n",
    "np.shape(t) ## correct ! .. but make sure that the step size is 23 ! i.e. every 23 step .. corresponds to new ADATE \n",
    "actions = np.reshape(t, [24, 200])\n",
    "actions = pd.DataFrame(np.transpose(actions))\n",
    "actions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration:\n",
    "\n",
    "http://beyondvalence.blogspot.com.eg/2014/05/kdd-cup-profit-optimization-in-r-part-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_explore():\n",
    "    df = load_data()\n",
    "    print df.TARGET_B.value_counts().plot(x=None, y=None, kind = 'pie', autopct='%1.1f%%')\n",
    "    \n",
    "    # % of donors\n",
    "    print 'Percentage of donors: %s' % (100.0 * sum(df.TARGET_B.astype('float'))/df.shape[0]) #about only 5% of the samples are doners .. \n",
    "    plt.hist(df.TARGET_D.value_counts(), bins = 7)   \n",
    "    plt.plot(df[df.TARGET_D > 0].TARGET_D) #Histogram is not the best choice .. let's try another plot .. \n",
    "    \n",
    "    # % of donors\n",
    "    print 'Percentage of donors: %s' % (100.0 * sum(df.TARGET_D.astype('float'))/df.shape[0])\n",
    "    #about 79% of the continous predictor are doners .. are there any donation amounts of zero ?!\n",
    "    print df.TARGET_D.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender data imputation ..\n",
    "It's very strange to have gender rather than M and F !! .. \n",
    "\n",
    "Let's impute any other value with the mode of this variable .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_impute():\n",
    "    df = load_data()\n",
    "    df['GENDER'] = np.where(df['GENDER'] == 'C', df['GENDER'].mode(), df['GENDER'])\n",
    "    df['GENDER'] = np.where(df['GENDER'] == 'U', df['GENDER'].mode(), df['GENDER'])\n",
    "    df['GENDER'] = np.where(df['GENDER'] == 'J', df['GENDER'].mode(), df['GENDER'])\n",
    "    df['GENDER'] = np.where(df['GENDER'] == 'A', df['GENDER'].mode(), df['GENDER'])\n",
    "    df['GENDER'] = np.where(df['GENDER'] == ' ', df['GENDER'].mode(), df['GENDER'])\n",
    "    \n",
    "#     print df['GENDER'].unique()\n",
    "    \n",
    "    ## how donations are distributed per gender\n",
    "    df['GENDER'].value_counts().plot(kind='barh', stacked=True, fontsize=7, figsize=[9,8], colormap='gist_ncar')\n",
    "    plt.title('donations are distributed among age groups', fontsize=14, color='black') \n",
    "    plt.xlabel('Number of doners', fontsize=14, color='black') \n",
    "    plt.ylabel('Gender of doner', fontsize=14, color='black') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing:\n",
    "\n",
    "1. Gets some redundant variables .. by calculating the correlation between all the variables .. \n",
    "those of high correlation coeffecient are redundant .. \n",
    "\n",
    "__NOTE:__\n",
    "In this implementation .. \n",
    "\n",
    "https://github.com/EAboelhamd/kdd98cup/blob/master/donors.py\n",
    "\n",
    "They tried to figure out redundant variables to remove them .. to be able to decrease the dimentionality of the problem .. however, in my case, there is no need to do .. as I'm gonna implement deep learning not a shallow solution .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preperation():\n",
    "    df = load_data()\n",
    "    df = df[df.columns.difference(['TARGET_B'])]\n",
    "    data = ['TARGET_D', 'ODATEDW','OSOURCE','TCODE','STATE','ZIP','MAILCODE','PVASTATE','DOB','NOEXCH','RECINHSE','RECP3','RECPGVG','RECSWEEP','MDMAUD','DOMAIN','CLUSTER','AGE','AGEFLAG','HOMEOWNR','CHILD03','CHILD07','CHILD12',\n",
    "      'CHILD18','NUMCHLD','INCOME','GENDER','WEALTH1','HIT','MBCRAFT','MBGARDEN','MBBOOKS','MBCOLECT','MAGFAML','MAGFEM','MAGMALE',\n",
    "      'PUBGARDN','PUBCULIN','PUBHLTH','PUBDOITY','PUBNEWFN','PUBPHOTO','PUBOPP','DATASRCE','MALEMILI','MALEVET','VIETVETS','WWIIVETS','LOCALGOV','STATEGOV','FEDGOV','SOLP3','SOLIH',\n",
    "      'MAJOR','WEALTH2','GEOCODE','COLLECT1','VETERANS','BIBLE','CATLG','HOMEE','PETS','CDPLAY','STEREO','PCOWNERS','PHOTO','CRAFTS','FISHER','GARDENIN','BOATS','WALKER','KIDSTUFF','CARDS','PLATES','LIFESRC','PEPSTRFL','POP901','POP902','POP903','POP90C1','POP90C2','POP90C3','POP90C4','POP90C5','ETH1','ETH2','ETH3','ETH4','ETH5','ETH6','ETH7','ETH8','ETH9','ETH10','ETH11','ETH12','ETH13',\n",
    "      'ETH14','ETH15','ETH16','AGE901','AGE902','AGE903','AGE904','AGE905','AGE906','AGE907','CHIL1','CHIL2','CHIL3','AGEC1','AGEC2','AGEC3','AGEC4','AGEC5','AGEC6','AGEC7','CHILC1','CHILC2','CHILC3','CHILC4','CHILC5','HHAGE1','HHAGE2','HHAGE3','HHN1','HHN2','HHN3','HHN4','HHN5','HHN6','MARR1','MARR2','MARR3','MARR4','HHP1','HHP2','DW1','DW2','DW3','DW4','DW5','DW6','DW7','DW8','DW9','HV1','HV2','HV3','HV4','HU1','HU2','HU3','HU4','HU5','HHD1',\n",
    "      'HHD2','HHD3','HHD4','HHD5','HHD6','HHD7','HHD8','HHD9','HHD10','HHD11','HHD12','ETHC1','ETHC2','ETHC3','ETHC4','ETHC5','ETHC6','HVP1','HVP2','HVP3','HVP4',\n",
    "      'HVP5','HVP6','HUR1','HUR2','RHP1','RHP2','RHP3','RHP4','HUPA1','HUPA2','HUPA3','HUPA4','HUPA5','HUPA6',\n",
    "      'HUPA7','RP1','RP2', 'RP3','RP4','MSA','ADI','DMA','IC1','IC2','IC3','IC4','IC5','IC6','IC7','IC8','IC9','IC10','IC11','IC12','IC13','IC14','IC15','IC16','IC17','IC18','IC19','IC20','IC21','IC22','IC23','HHAS1','HHAS2','HHAS3','HHAS4','MC1','MC2','MC3',\n",
    "      'TPE1','TPE2','TPE3','TPE4','TPE5','TPE6','TPE7','TPE8','TPE9','PEC1','PEC2','TPE10','TPE11','TPE12','TPE13','LFC1','LFC2','LFC3','LFC4','LFC5','LFC6','LFC7','LFC8','LFC9','LFC10','OCC1','OCC2','OCC3','OCC4','OCC5','OCC6','OCC7','OCC8','OCC9',\n",
    "      'OCC10','OCC11','OCC12','OCC13','EIC1','EIC2','EIC3','EIC4','EIC5','EIC6','EIC7','EIC8','EIC9','EIC10','EIC11','EIC12','EIC13','EIC14','EIC15',\n",
    "      'EIC16','OEDC1','OEDC2','OEDC3','OEDC4','OEDC5','OEDC6','OEDC7','EC1','EC2','EC3',\n",
    "      'EC4','EC5','EC6','EC7','EC8','SEC1','SEC2','SEC3','SEC4','SEC5','AFC1','AFC2','AFC3','AFC4','AFC5','AFC6','VC1','VC2','VC3','VC4','ANC1','ANC2','ANC3','ANC4','ANC5','ANC6','ANC7','ANC8','ANC9','ANC10','ANC11','ANC12','ANC13','ANC14','ANC15','POBC1','POBC2','LSC1','LSC2','LSC3','LSC4',\n",
    "      'VOC1','VOC2', 'VOC3','HC1','HC2','HC3', 'HC4','HC5','HC6','HC7','HC8','HC9','HC10','HC11','HC12','HC13','HC14','HC15','HC16',\n",
    "      'HC17','HC18','HC19','HC20','HC21','MHUC1','MHUC2','AC1','AC2','RFA_2','RFA_3','RFA_4','RFA_5','RFA_6','RFA_7','RFA_8','RFA_9','RFA_10','RFA_11','RFA_12','RFA_13','RFA_14','RFA_15','RFA_16','RFA_17','RFA_18','RFA_19','RFA_20','RFA_21','RFA_22','RFA_23','RFA_24','CARDPROM','MAXADATE','NUMPROM',\n",
    "      'CARDPM12', 'NUMPRM12','RDATE_3','RDATE_4','RDATE_5','RDATE_6','RDATE_7','RDATE_8','RDATE_9','RDATE_10','RDATE_11',\n",
    "      'RDATE_12','RDATE_13','RDATE_14','RDATE_15','RDATE_16','RDATE_17','RDATE_18','RDATE_19','RDATE_20','RDATE_21','RDATE_22','RDATE_23','RDATE_24','RAMNT_3', 'RAMNT_4','RAMNT_5','RAMNT_6', 'RAMNT_7','RAMNT_8','RAMNT_9', 'RAMNT_10',\n",
    "      'RAMNT_11','RAMNT_12','RAMNT_13','RAMNT_14','RAMNT_15','RAMNT_16','RAMNT_17','RAMNT_18','RAMNT_19','RAMNT_20','RAMNT_21',\n",
    "      'RAMNT_22','RAMNT_23','RAMNT_24','RAMNTALL','NGIFTALL','CARDGIFT','MINRAMNT','MINRDATE', 'MAXRAMNT','MAXRDATE','LASTGIFT','LASTDATE','FISTDATE','NEXTDATE','TIMELAG','AVGGIFT','CONTROLN', 'HPHONE_D','RFA_2R','RFA_2F','RFA_2A','MDMAUD_R','MDMAUD_F','MDMAUD_A','CLUSTER2','GEOCODE2']\n",
    "    \n",
    "    for i in xrange(len(data)):\n",
    "        df[data[i]] = pd.Categorical((pd.factorize(df[data[i]])[0] + 1).astype(str))\n",
    "    \n",
    "    return df[data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_split():  \n",
    "    df = data_preperation()\n",
    "    train, test = train_test_split(df, test_size = 0.5)  # split data to 50-50 cross validate \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_split()\n",
    "rewards = ['TARGET_D']\n",
    "for i in xrange(len(rewards)):\n",
    "    train[rewards[i]] = pd.Categorical((pd.factorize(train[rewards[i]])[0] + 1).astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current State variables ..\n",
    "Recency, Frequancy, Montery variables .. ['RFA_2R', 'RFA_2F', 'RFA_2A']\n",
    "\n",
    "## Rewards:\n",
    "- Donation amount \n",
    "- These are the target variables as well .. \n",
    "- TARGET_D, TARGET_B \n",
    "\n",
    "\n",
    "## Actions: \n",
    "- 11 mailing type\n",
    "\n",
    "Actions mapping .. \n",
    "https://github.com/EAboelhamd/kdd98-1/tree/master/notebooks\n",
    "\n",
    "## Actions:\n",
    "\n",
    "- mailing types ..\n",
    "action_mapping = {\n",
    "\n",
    "                                    2: 'NK',  3: 'NK',\n",
    "                                    4: 'TK',  5: 'SK',\n",
    "                                    6: 'LL',  7: 'G1',\n",
    "                                    8: 'GK',  9: 'CC',\n",
    "                                    10: 'WL', 11: 'X1',\n",
    "                                    12: 'XK', 13: 'FS',\n",
    "                                    14: 'NK', 15: 'TK',\n",
    "                                    16: 'LL', 17: 'G1',\n",
    "                                    18: 'GK', 19: 'CC',\n",
    "                                    20: 'WL', 21: 'X1',\n",
    "                                    22: 'XK', 23: 'FS', 24: 'NK'\n",
    "                                }\n",
    "\n",
    "this list contains 11 unique values (NK, TK,LL,GK,WL,XK,SK,G1,CC,X1,FS)\n",
    "\n",
    "                                        NK ==> 2, 14, 23, 24\n",
    "                                        LL ==> 2, 16\n",
    "                                        TK ==> 4, 15\n",
    "                                        GK ==> 8, 18\n",
    "                                        WL ==> 10, 20\n",
    "                                        SK ==> 5\n",
    "                                        G1 ==> 7, 17\n",
    "                                        CC ==> 9, 19\n",
    "                                        X1 ==> 11, 21\n",
    "                                        FS ==> 13, 23\n",
    "\n",
    "Their corresponding meanings are:\n",
    "\n",
    "                                        LL mailings had labels only                                        \n",
    "                                        WL mailings had labels only\n",
    "                                        CC mailings are calendars with stickers but do\n",
    "                                           not have labels\n",
    "                                        FS mailings are blank cards that fold into\n",
    "                                           thirds with labels\n",
    "                                        NK mailings are blank cards with labels\n",
    "                                        SK mailings are blank cards with labels\n",
    "                                        TK mailings have thank you printed on the\n",
    "                                           outside with labels\n",
    "                                        GK mailings are general greeting cards (an\n",
    "                                           assortment of birthday, sympathy, blank, & get\n",
    "                                           well) with labels\n",
    "                                        XK mailings are Christmas cards with labels\n",
    "                                        X1 mailings have labels and a notepad\n",
    "                                        G1 mailings have labels and a notepad\n",
    "\n",
    "- This is why I'm gonna extract the last two digits from the sequance to represent mailing type (unique values are 11 .. NK, TK, SK, LL, G1, GK, CC, WL, X1, XK, FS) + no action .. total action are 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States:\n",
    "\n",
    "In the cited paper .. they defined the states as follows: \n",
    "\n",
    "It is a 5-dimensional vector consisting of \n",
    "\n",
    "(1) how recently the donor donated last (R)\n",
    "\n",
    "(2) how frequently she donates (F)\n",
    "\n",
    "(3) her average donation amount (M)\n",
    "\n",
    "(4) how many times PVA sends her a mail in the last six months\n",
    "\n",
    "(5) how many times PVA has sent her mails.\n",
    "\n",
    "\n",
    "This implementation is considered as a POMPD .. \n",
    "where:\n",
    "b: belief state that is a probability distribution over all states\n",
    "b(s): prob. that the agent in state s \n",
    "after taking action a and observing the state O .. the update rule for the belief state o(s) is using Bayes rule .. \n",
    "\n",
    "## Next States:\n",
    "\n",
    "As mentioned in the paper ==> the 5-dimensional observation is discrete in this problem, and individual dimensions evolve\n",
    "independently of each other. We therefore build an observation probability table for each observation dimension,\n",
    "and the sample next observations using these tables.\n",
    "\n",
    "Hence, Let's create an n*5 dim observations table .. \n",
    "\n",
    "http://www-anw.cs.umass.edu/~barto/courses/cs687/PartialObs-printable.pdf\n",
    "\n",
    "\n",
    "### These researchers proposed other types of state space ==>\n",
    "\n",
    "https://www.cs.cmu.edu/~ebrun/15889e/hw1.pdf\n",
    "\n",
    "as 9 vars (try it out) ;) .. \n",
    "and others consider (belief, observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_(actions_col):\n",
    "    \n",
    "    train, test = df_split()\n",
    "    \n",
    "    RFA = ['RFA_2']  # don't know if it is NUMPRM12 or CARDPM12 \n",
    "\n",
    "    ## next state\n",
    "    ## next_states are random selection from the current states\n",
    "    RFA_next = ['RFA_3', 'RFA_4', 'RFA_5', 'RFA_6', 'RFA_7', 'RFA_8', 'RFA_9', 'RFA_10',\n",
    "                'RFA_11', 'RFA_12', 'RFA_13', 'RFA_14', 'RFA_15', 'RFA_16', 'RFA_17', 'RFA_18',\n",
    "                'RFA_19', 'RFA_20', 'RFA_21', 'RFA_22', 'RFA_23', 'RFA_24']\n",
    "#     next_states = [[random.random() for e in train[RFA].values[0]] for e in xrange(len(train[RFA].values))]\n",
    "#     next_states = pd.DataFrame(next_states)\n",
    "\n",
    "    current_states = train['RFA_2']\n",
    "    next_states = train[RFA_next]\n",
    "    \n",
    "    rewards = ['TARGET_D']\n",
    "    \n",
    "    train_rewards = train[rewards].values\n",
    "    \n",
    "    for i in xrange(len(rewards)):\n",
    "        train[rewards[i]] = pd.Categorical((pd.factorize(train[rewards[i]])[0] + 1).astype(str))\n",
    "\n",
    "            # next actions \n",
    "    next_actions = np.zeros([len(next_states), max(actions.max(axis = 0).astype(int)) + 1])\n",
    "    next_actions = np.delete(next_actions, -1, axis=1) ## remove last column .. \n",
    "    \n",
    "    #     # fill in next_actions  \n",
    "    for i in xrange(np.shape(next_actions)[1]):\n",
    "        next_actions[:, i] = i\n",
    "    \n",
    "#     for j in xrange(np.shape(actions)[1]):\n",
    "    tuplesMx = np.column_stack((current_states.values, actions_col, next_states.values, next_actions, train_rewards))\n",
    "\n",
    "#         curr_state_current_action = pd.DataFrame(np.column_stack((current_states.values, actions[:40][j])))\n",
    "\n",
    "#         next_state_next_action = np.zeros([np.shape(next_states)[0], np.shape(next_states)[1] + max(actions.max(axis = 0).astype(int))])\n",
    "\n",
    "\n",
    "#         Mxa = np.column_stack(())  # (25,2) \n",
    "\n",
    "#         tuplesMx = np.column_stack((curr_state_current_action.values, next_states.values, next_actions, train_rewards))\n",
    "\n",
    "    \n",
    "    return tuplesMx #next_states, next_actions, curr_state_current_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuplesMx = tuple_()\n",
    "# np.shape(tuplesMx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression phase:\n",
    "\n",
    "Before performing the prediction task .. let's split the data to training and validation sets .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid any problems in prediction by having string variables .. let's binarize (catergorize) all the variables .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "Num_itrs = 2  # no loop 3leha ( w dah el sa7) .. we just have to run the whole algo. 10 times and report the avg. results\n",
    "\n",
    "num_epoch = 1 #23 #epochs are cycles of Feedforward and Backprob\n",
    "## el mafrood yeb2a feh loop 3la el epochs elli heyya el steps .. w avg. reward per step is calculated \n",
    "batch_size = 5\n",
    "chunkSize = 1\n",
    "\n",
    "n_nodes_hl1 = np.shape(train)[1]\n",
    "n_nodes_hl2 = np.shape(train)[0]\n",
    "NUM_STATES = np.shape(train)[1]\n",
    "NUM_DIM =  np.shape(train)[1]\n",
    "num_nodes = np.shape(train)[0]\n",
    "num_unrollings = 5\n",
    "\n",
    "best_actions = np.zeros([np.shape(train)[0], batch_size])\n",
    "Q_optimal = [] #np.zeros([np.shape(curr_state_current_action)[0], len(df['ACCOUNT_STATUS'].unique())])\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "## for training \n",
    "x = tf.placeholder(tf.float32, shape=[NUM_DIM, num_nodes])\n",
    "y =  tf.placeholder(tf.float32, shape=[num_nodes, 1])\n",
    "\n",
    "## for testing\n",
    "x_ = tf.placeholder(tf.float32, shape=[np.shape(test)[0], NUM_DIM])\n",
    "y_ =  tf.placeholder(tf.float32, shape=[np.shape(test)[0], 1])\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "# Input gate: input, previous output, and bias.\n",
    "ix = tf.Variable(tf.truncated_normal([NUM_DIM, num_nodes], 0, 1, dtype = tf.float32))# init_weights_RNN([n_nodes_hl1, NUM_ACTIONS])\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], 0, 1, dtype = tf.float32))\n",
    "ib = tf.Variable(tf.zeros([1, num_nodes], dtype = tf.float32))\n",
    "\n",
    "# Forget gate: input, previous output, and bias.\n",
    "fx = tf.Variable(tf.truncated_normal([NUM_DIM, num_nodes], 0, 1, dtype = tf.float32))\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], 0, 1, dtype = tf.float32))\n",
    "fb = tf.Variable(tf.zeros([1, num_nodes], dtype = tf.float32))\n",
    "\n",
    "# Memory cell: input, state and bias.                             \n",
    "cx = tf.Variable(tf.truncated_normal([NUM_DIM, num_nodes], 0, 1, dtype = tf.float32))\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], 0, 1, dtype = tf.float32))\n",
    "cb = tf.Variable(tf.zeros([1, num_nodes], dtype = tf.float32))\n",
    "\n",
    "# Output gate: input, previous output, and bias.\n",
    "ox = tf.Variable(tf.truncated_normal([NUM_DIM, num_nodes], 0, 1, dtype = tf.float32))\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], 0, 1, dtype = tf.float32))\n",
    "ob = tf.Variable(tf.zeros([1, num_nodes], dtype = tf.float32))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([1, num_nodes], dtype = tf.float32), trainable=False) #reversed\n",
    "\n",
    "saved_state = tf.Variable(tf.zeros([1, num_nodes], dtype = tf.float32), trainable=False) #reversed\n",
    "\n",
    "# Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], 0, 1, dtype = tf.float32))\n",
    "b = tf.Variable(tf.zeros([num_nodes], dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the cell computation.\n",
    "# this method takes single cell and returns single number \n",
    "def lstm_cell(i, o, state):\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(train, saved_output, saved_state):\n",
    "    # Unrolled LSTM loop.    \n",
    "    outputs = list()\n",
    "    output = saved_output  # row !\n",
    "    state = saved_state  # row !\n",
    "    \n",
    "    # astype('U') .. to convert numpy array to string ..\n",
    "    for i in xrange(np.shape(train)[0]):## el loop faydetha to copy the next line that is just for single unit \n",
    "        output_, state = lstm_cell(tf.string_to_number(train.values[i, None].astype('U')), tf.cast(output, tf.float32), tf.cast(state, tf.float32)) \n",
    "\n",
    "    ## in case the last values are saved !\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        model_output = tf.matmul(output_, w) + b # outputs single value\n",
    "        \n",
    "    return model_output  ## the output for the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model():\n",
    "    loss_RNN = []\n",
    "    model_output = lstm_model(train, saved_output, saved_state) #output here is a vector\n",
    "    model_output = tf.transpose(model_output)\n",
    "    cost = tf.reduce_mean(tf.square(y - model_output))\n",
    "    optimize = tf.train.GradientDescentOptimizer(0.01).minimize(cost) \n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()  ## updated version from initialize_all_variables :) \n",
    "    sess.run(init)\n",
    "\n",
    "    predicted_reward = sess.run(model_output, feed_dict={x:np.transpose(train.values), y:train[rewards].values})\n",
    "    \n",
    "    # Cost calculation\n",
    "    for step in xrange(1000):\n",
    "        l,_ = sess.run([cost, optimize], feed_dict={x:np.transpose(train.values), y:train[rewards]})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            loss_RNN.append(l)\n",
    "\n",
    "    Y_pred = tf.zeros([len(test[rewards]),1])\n",
    "\n",
    "    # predict \n",
    "    test_pred2 = sess.run(Y_pred, feed_dict={x_: test.values})\n",
    "\n",
    "   ## rms to test ..\n",
    "    cost_test = tf.reduce_mean(tf.square(test[rewards].values.astype(np.float32) - test_pred2))\n",
    "    \n",
    "    \n",
    "    rmse_val = sess.run(cost_test, feed_dict={x_:test.values.astype(np.float32), y_: test_pred2})\n",
    "     \n",
    "    sess.close()\n",
    "    \n",
    "    return loss_RNN, predicted_reward #saved_state, saved_output, cost_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_prediction(train):\n",
    "    loss_RNN, predicted_reward = train_lstm_model()\n",
    "    plt.plot(loss_RNN)\n",
    "    plt.xlabel('Step number')\n",
    "    plt.ylabel('Prediction Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_prediction(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the rewards:\n",
    "Now, the LSTM_RNN output (from validation phase) is considered as input for DQN model, to be able to select the action that has the maximum longtime reward for the customer (highest CLV) .. \n",
    "\n",
    "https://arxiv.org/pdf/1602.01580.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Deep Neural Network (DQN):\n",
    "\n",
    "https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Definition of the cell computation.\n",
    "# this method takes single cell and returns single number \n",
    "def lstm_cell(i, o, state):\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_model(X, w_h_1, w_h_2, w_o, bias_I_1, bias_I_2, bias_h):\n",
    "    \n",
    "#     X = np.transpose(X)\n",
    "    \n",
    "    layer_1 = tf.matmul(X, w_h_1) + bias_I_1\n",
    "    layer_1 = tf.nn.relu(layer_1)  ## el performance of softmax outperforms relu!!\n",
    "    \n",
    "    layer_2 = tf.matmul(layer_1, w_h_2) + bias_I_2\n",
    "    layer_2 = tf.nn.sigmoid(layer_2) \n",
    "\n",
    "    py_x = tf.matmul(layer_2, w_o) + bias_h\n",
    "    \n",
    "    return py_x  #predicted output\n",
    "    # note that we dont take the softmax at the end because our cost fn does that for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_training(X, output):\n",
    "    \n",
    "    w_h_1 = init_weights([np.shape(X)[1], n_nodes_hl1]) # create symbolic variables\\n\",\n",
    "    w_h_2 = init_weights([n_nodes_hl1, n_nodes_hl2]) # create symbolic variables\\n\",\n",
    "    w_o = init_weights([n_nodes_hl2, 1])\n",
    "    \n",
    "    bias_I_1=init_weights([n_nodes_hl1])\n",
    "    bias_I_2=init_weights([n_nodes_hl2])\n",
    "    bias_h=init_weights([1])\n",
    "    \n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    py_x = DQN_model(X.convert_objects(convert_numeric=True).astype(np.float32), w_h_1, w_h_2, w_o, bias_I_1, bias_I_2,bias_h)  #model training  \n",
    "    \n",
    "    cost = tf.reduce_mean(tf.square(py_x - output)) # compute costs\",  # excpect float32\n",
    "\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost) # construct an optimizer\\n\",\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)    \n",
    "    sess.close()  \n",
    "    \n",
    "    return w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction phase: \n",
    "def DQN_predict(tuplesMx, output):\n",
    "    \n",
    "#     print type(tuplesMx)\n",
    "\n",
    "    w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h = DQN_training(tuplesMx, output)\n",
    "    \n",
    "    tuplesMx = pd.DataFrame(tuplesMx)\n",
    "    predict_op_0 = DQN_model(tuplesMx.convert_objects(convert_numeric=True).astype(np.float32), w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h)\n",
    "    \n",
    "    sess2 = tf.Session()\n",
    "    init2 = tf.global_variables_initializer()\n",
    "    sess2.run(init2)\n",
    "\n",
    "    l0=sess2.run(predict_op_0)\n",
    "        \n",
    "    Q_predicted = l0  #, l1, l2, l3, l4, l5, l6]\n",
    "    \n",
    "    print Q_predicted\n",
    "    \n",
    "    sess2.close()\n",
    "    \n",
    "    return Q_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q(s,a) representing the (Quality) of action a at state is .. \n",
    "\n",
    "this Q value depends on the immediate reward r .. however, it'll be more effective if it takes the future rewards Q(s', a') into consideration .. \n",
    "\n",
    "the future rewards are discounted by probability gama .. cause the evironment is stochastic hence, it is uncertain that each time you select action a you gonna get the same reward r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_reward = np.transpose(predicted_reward) \n",
    "def Q_learning():\n",
    "    \n",
    "    nepisod = np.shape(actions)[1]  ##22\n",
    "    avg_Q = []\n",
    "    tuplesMx = []\n",
    "#     num_actions = np.shape(next_actions)[1]\n",
    "\n",
    "    \n",
    "    Q_predicted = np.zeros([np.shape(train)[0], 12])\n",
    "    best_action = list() #np.zeros([np.shape(train)[0], batch_size])\n",
    "    \n",
    "    for i in xrange(nepisod):\n",
    "        tuplesMx = tuple_(actions[:100][i])\n",
    "        _, predicted_reward = train_lstm_model()\n",
    "\n",
    "        for j in xrange(len(tuplesMx)): #(0, len(tuplesMx), batch_size):\n",
    "            Q_optimal = predicted_reward[j].astype(float) + gamma*np.max(Q_predicted) # returns max value per row !\n",
    "            Q_optimal = np.array(Q_optimal, dtype = np.float32) # convert output to float32 to match py_x\n",
    "\n",
    "            # #         Q_predicted = np.array(Q_predicted, dtype=np.float32)\n",
    "            Q_predicted = DQN_predict(tuplesMx, Q_optimal) \n",
    "        avg_Q.append(np.mean(Q_optimal))\n",
    "\n",
    "    #         print Q_predicted\n",
    "\n",
    "    return avg_Q\n",
    "        \n",
    "#     print type(Q_optimal)\n",
    "\n",
    "#             best_action.append(np.argmax(Q_predicted, axis=1))\n",
    "        #     print type(best_action)\n",
    "\n",
    "#     return np.mean(Q_optimal), best_action \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     avg_Q, best_action = Q_learning()\n",
    "# #     best_action = pd.DataFrame(best_action)\n",
    "# #     return avg_Q, best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_action = np.transpose(best_action)\n",
    "# best_action[0].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above .. \n",
    "\n",
    "the actions order (from highest to lowest) \n",
    "1. Action # 7 (Thank you printed with labels)\n",
    "2. Action # 6 \n",
    "3. Action # 4 (Blank cards that fold into thirds with labels)\n",
    "4. Action # 1 \n",
    "\n",
    "This differs a bit the order of Stanford paper in two points:\n",
    "1. Their starting index is 1 .. while Python's is zeros based .. this is why I'm adding 1 in the index .. \n",
    "2. I'm working on just a sample (with a size less than their size) this is why there are some actions I even didn't have as an output (i.e. action # 11) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "an enhancement for this implementation is to change the states to be the whole doner state instead of just RFM ..\n",
    "\n",
    "https://github.com/EAboelhamd/kdd98-1/blob/master/notebooks/exploratory.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do I apply experience replay .. \n",
    "\n",
    "The algorithm is mentioned here .. \n",
    "\n",
    "https://www.intelnervana.com/demystifying-deep-reinforcement-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
