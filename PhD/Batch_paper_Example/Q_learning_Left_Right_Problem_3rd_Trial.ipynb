{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "This notebook is devoted to implement Q learning algorithm on Left_Right problem .. a case study in Batch Learning paper (Page 14) .. \n",
    "\n",
    "Paper Link ==> http://www.jmlr.org/papers/v6/ernst05a.html\n",
    "\n",
    "Guided by this tutorial ==> http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
    "\n",
    "and also this one ==> https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "\n",
    "\n",
    "## Experimental Setup:\n",
    "\n",
    "num of simulations = 100,000\n",
    "\n",
    "s = rand(0, 10)     selected at random \n",
    "\n",
    "a = [-2, 2] ==> -2: left, 2: right\n",
    "\n",
    "r = [100, 0, 50] ==> 0: inside the interval, 50: left, 100: right\n",
    "\n",
    "num of episods = 300 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Decleration: \n",
    "\n",
    "__Episods:\n",
    "\n",
    "We'll call each exploration an episode.  \n",
    "Each episode consists of the agent moving from the initial state to the goal state.  \n",
    "Each time the agent arrives at the goal state, the program goes to the next episode.\n",
    "hence, the episod is treated as if it's the number of runs! (Each episode is equivalent to one training session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "u=[-2,2]\n",
    "gamma=0.75\n",
    "N = 10\n",
    "# M =  N + 4\n",
    "j=0\n",
    "\n",
    "s_next = np.zeros([N, 1])\n",
    "next_state = np.zeros([N, 1])\n",
    "r = np.zeros([N, 1])\n",
    "s_current = np.zeros([N, 1])\n",
    "action = list(np.zeros([N, 1]))\n",
    "reward = np.zeros([N, 1])\n",
    "\n",
    "## Q_table\n",
    "Q = np.zeros([N, 2])\n",
    "\n",
    "## Predicted Q\n",
    "Q_ = np.zeros([N, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating r matrix \n",
    "for i in xrange(N):\n",
    "    x = np.random.randint(10)\n",
    "    a = np.random.randint(2)\n",
    "    \n",
    "    while True:\n",
    "        s_next[i] = x + u[a] + np.random.rand()\n",
    "        \n",
    "        if s_next[i] > 10:\n",
    "            r[i] = np.power(gamma, (i-1))*100\n",
    "            reward[i] = r[i]\n",
    "            s_current[i] = x\n",
    "            action[i] = u[a]\n",
    "            next_state[i] = s_next[i]\n",
    "            break\n",
    "        if s_next[i] < 0:\n",
    "            r[i] = np.power(gamma, (i-1))*50\n",
    "            reward[i] = r[i]\n",
    "            s_current[i] = x\n",
    "            action[i] = u[a]\n",
    "            next_state[i] = s_next[i]\n",
    "            break\n",
    "        else: \n",
    "            r[i] = 0\n",
    "            reward[i] = r[i]\n",
    "            s_current[i] = x\n",
    "            action[i] = u[a]\n",
    "            next_state[i] = s_next[i]\n",
    "            break\n",
    "#     j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.2590229 ],\n",
       "       [  5.74077382],\n",
       "       [ 10.44190831],\n",
       "       [  5.63336002],\n",
       "       [ -0.21762443],\n",
       "       [  1.29527138],\n",
       "       [  7.20509082],\n",
       "       [ 10.71434782],\n",
       "       [  7.44950013],\n",
       "       [  5.41970504]])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 1), (10,), (10, 1), (10, 1))"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(s_current), np.shape(action), np.shape(r), np.shape(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.        ,   2.        ,   0.        ,   4.2590229 ],\n",
       "       [  7.        ,  -2.        ,   0.        ,   5.74077382],\n",
       "       [  8.        ,   2.        ,  75.        ,  10.44190831],\n",
       "       [  7.        ,  -2.        ,   0.        ,   5.63336002],\n",
       "       [  1.        ,  -2.        ,  21.09375   ,  -0.21762443],\n",
       "       [  3.        ,  -2.        ,   0.        ,   1.29527138],\n",
       "       [  9.        ,  -2.        ,   0.        ,   7.20509082],\n",
       "       [  8.        ,   2.        ,  17.79785156,  10.71434782],\n",
       "       [  9.        ,  -2.        ,   0.        ,   7.44950013],\n",
       "       [  3.        ,   2.        ,   0.        ,   5.41970504]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuplesMx = np.column_stack((s_current, action, r, next_state))\n",
    "tuplesMx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_a1 = pd.DataFrame(np.column_stack((next_state, list(np.random.choice([-2], N))))).values\n",
    "s2_a2 = pd.DataFrame(np.column_stack((next_state, list(np.random.choice([2], N))))).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 15\n",
    "n_nodes_hl2 = 25\n",
    "NUM_STATES = np.shape(Q)[0]\n",
    "NUM_ACTIONS = np.shape(Q)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "## init weights .. \n",
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, w_h, w_o,bias_I,bias_h):\n",
    "    h = tf.nn.relu(tf.matmul(X, w_h) + bias_I) \n",
    "    py_x = tf.matmul(h, w_o) + bias_h\n",
    "    return py_x # note that we dont take the softmax at the end because our cost fn does that for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(Curr_state_current_action, output):\n",
    "\n",
    "    Curr_state_current_action = Curr_state_current_action.astype(np.float32) \n",
    "    w_h = init_weights([2, n_nodes_hl1]) # create symbolic variables\\n\",\n",
    "    w_o = init_weights([n_nodes_hl1, 1])\n",
    "    bias_I=init_weights([n_nodes_hl1])\n",
    "\n",
    "    bias_h=init_weights([1])\n",
    "    py_x = model(Curr_state_current_action, w_h, w_o,bias_I,bias_h)  #model training  \n",
    "\n",
    "    cost = tf.reduce_mean(tf.square(py_x - output)) # compute costs\\n\",\n",
    "\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost) # construct an optimizer\\n\",\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    \n",
    "    return w_h, w_o,bias_I,bias_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction phase: \n",
    "def model_predict(s1_a1, s2_a2, output):\n",
    "\n",
    "    s1_a1 = s1_a1.astype(np.float32)\n",
    "    [w_h, w_o,bias_I,bias_h] = model_training(s1_a1, output[:,0])\n",
    "    predict_op_1 = model(s1_a1, w_h, w_o,bias_I,bias_h)   #optimal prediction\n",
    "      \n",
    "        \n",
    "    s2_a2 = s2_a2.astype(np.float32)\n",
    "    [w_h, w_o,bias_I,bias_h] = model_training(s2_a2, output[:,1])\n",
    "    predict_op_2 = model(s2_a2, w_h, w_o,bias_I,bias_h)   #optimal prediction\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    l1=sess.run(predict_op_1)\n",
    "    l2=sess.run(predict_op_2)\n",
    "    Q = [l1, l2]\n",
    "    return np.transpose(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.3\n",
    "for i in xrange(N):\n",
    "    Q[action] = reward + gamma*np.amax(Q_, axis = 1)[0]\n",
    "    Q_ = model_predict(s1_a1, s2_a2, Q)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.plot(Q)\n",
    "plt.title('Q value at each state in both actions')\n",
    "plt.xlabel(\"State Number\")\n",
    "plt.ylabel(\"Q(s,a)\")\n",
    "plt.legend('LR')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
