{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "Main Goals:\n",
    "\n",
    "1. Identify the recipients that will engage with the campaign.\n",
    "2. Maximise the campaignâ€™s revenue.\n",
    "\n",
    "\n",
    "Comments\n",
    "\n",
    "- The dataset contains only 5% of donors.\n",
    "- The donations are usually smaller than $20.\n",
    "- This data is quite noisy, high dimensional.\n",
    "- There is an inverse relationship between the probability to donate and the amount donated.\n",
    "\n",
    "\n",
    "Link for dataset and some analysis ==> \n",
    "\n",
    "https://github.com/rebordao/kdd98cup\n",
    "\n",
    "https://github.com/bobbyantonio/KDD98/blob/master/CleanData.py\n",
    "\n",
    "- Github solutions ==>\n",
    "https://github.com/rebordao/kdd98cup\n",
    "\n",
    "\n",
    "- Siraj notebook for a better data visualization:\n",
    "\n",
    "https://www.youtube.com/watch?v=yQsOFWqpjkE\n",
    "\n",
    "- Implementations:\n",
    "\n",
    "https://github.com/sisl/CustomerSim/blob/master/src/kdd98_preprocess.R\n",
    "\n",
    "https://github.com/ugo-nama-kun/DQN-chainer/blob/master/dqn_agent_nature.py\n",
    "\n",
    "- Yegor Tk's website => http://yegortkachenko.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# from tensorflow.python.ops import rnn, rnn_cell\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "## plotting .. \n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import random \n",
    "# random.seed(123)\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "## warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv('tuple.csv', header = 0)\n",
    "    ## read top performing action from file \n",
    "    \n",
    "    ## replace df['a'] with top performing action\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_split():\n",
    "    df = load_data()\n",
    "    train, test = train_test_split(df, test_size = 0.95)  # split data to 50-50 cross validate, Roger 1.6*1000,000\n",
    "    \n",
    "    train = train.convert_objects(convert_numeric=True)\n",
    "    train['a'] = np.random.permutation(train['a'])\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_():\n",
    "        \n",
    "    train, _ = df_split()\n",
    "\n",
    "#     train = train.convert_objects(convert_numeric=True)\n",
    "    \n",
    "    next_actions = np.zeros([len(train), 12])\n",
    "    \n",
    "    #     # fill in next_actions  \n",
    "    for i in xrange(np.shape(train)[1]):\n",
    "        next_actions[:, i] = i\n",
    "    \n",
    "    \n",
    "    # next_state_next_action\n",
    "    tuplesMx0 = np.column_stack((train, next_actions[:,0]))\n",
    "    tuplesMx1 = np.column_stack((train, next_actions[:,1]))\n",
    "    tuplesMx2 = np.column_stack((train, next_actions[:,2]))\n",
    "    tuplesMx3 = np.column_stack((train, next_actions[:,3]))\n",
    "    tuplesMx4 = np.column_stack((train, next_actions[:,4]))\n",
    "    tuplesMx5 = np.column_stack((train, next_actions[:,5]))\n",
    "    tuplesMx6 = np.column_stack((train, next_actions[:,6]))\n",
    "    tuplesMx7 = np.column_stack((train, next_actions[:,7]))\n",
    "    tuplesMx8 = np.column_stack((train, next_actions[:,8]))\n",
    "    tuplesMx9 = np.column_stack((train, next_actions[:,9]))\n",
    "    tuplesMx10 = np.column_stack((train, next_actions[:,10]))\n",
    "    tuplesMx11 = np.column_stack((train, next_actions[:,11]))\n",
    "    \n",
    "    return tuplesMx0, tuplesMx1, tuplesMx2, tuplesMx3, tuplesMx4, tuplesMx5, tuplesMx6, tuplesMx7, tuplesMx8, tuplesMx9, tuplesMx10, tuplesMx11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuplesMx0, tuplesMx1, tuplesMx2, tuplesMx3, tuplesMx4, tuplesMx5, tuplesMx6, tuplesMx7, tuplesMx8, tuplesMx9, tuplesMx10, tuplesMx11  = tuple_()\n",
    "# tuplesMx0, tuplesMx1, tuplesMx2, tuplesMx3, tuplesMx4, tuplesMx5, tuplesMx6, tuplesMx7, tuplesMx8, tuplesMx9, tuplesMx10, tuplesMx11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression phase:\n",
    "\n",
    "Before performing the prediction task .. let's split the data to training and validation sets .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid any problems in prediction by having string variables .. let's binarize (catergorize) all the variables .. \n",
    "\n",
    "Guidance ==> https://pythonprogramming.net/rnn-tensorflow-python-machine-learning-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Deep Neural Network (DQN):\n",
    "\n",
    "https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training:\n",
    "\n",
    "1. https://stackoverflow.com/questions/46832151/tensorflow-neural-network-multi-layer-perceptron-for-regression-example\n",
    "2. \n",
    "https://medium.com/mlreview/a-simple-deep-learning-model-for-stock-price-prediction-using-tensorflow-30505541d877\n",
    "\n",
    "Guidance: https://www.youtube.com/watch?v=FbJw8J0rTyQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_train(x_inputs, y_outputs):\n",
    "    \n",
    "    print np.shape(y_outputs)\n",
    "    \n",
    "    # Placeholder\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, np.shape(x_inputs)[1] ])\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "\n",
    "\n",
    "    # Model architecture parameters\n",
    "    n_stocks = np.shape(x_inputs)[1] \n",
    "    n_neurons_1 = 40\n",
    "    n_neurons_2 = 15\n",
    "#     n_neurons_3 = 256\n",
    "#     n_neurons_4 = 128\n",
    "    n_target = 12 #np.shape(x_inputs)[0]\n",
    "    \n",
    "    batch_size = 500 \n",
    "    epochs = 100\n",
    "    \n",
    "        # Initializers\n",
    "    sigma = 1\n",
    "    weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n",
    "    bias_initializer = tf.zeros_initializer()\n",
    "    \n",
    "\n",
    "    # Layer 1: Variables for hidden weights and biases\n",
    "    W_hidden_1 = tf.Variable(weight_initializer([n_stocks, n_neurons_1]))\n",
    "    bias_hidden_1 = tf.Variable(bias_initializer([n_neurons_1]))\n",
    "\n",
    "    # Layer 2: Variables for hidden weights and biases\n",
    "    W_hidden_2 = tf.Variable(weight_initializer([n_neurons_1, n_neurons_2]))\n",
    "    bias_hidden_2 = tf.Variable(bias_initializer([n_neurons_2]))\n",
    "\n",
    "    # Output layer: Variables for output weights and biases\n",
    "    W_out = tf.Variable(weight_initializer([n_neurons_2, n_target]))\n",
    "    bias_out = tf.Variable(bias_initializer([n_target]))\n",
    "\n",
    "\n",
    "    # Hidden layer\n",
    "    hidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))\n",
    "    hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))\n",
    "\n",
    "    # Output layer (must be transposed)\n",
    "    y_ = tf.add(tf.matmul(hidden_2, W_out), bias_out)\n",
    "    \n",
    "    # initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    # Cost function\n",
    "    mse = tf.reduce_mean(tf.squared_difference(y_, Y))\n",
    "\n",
    "    # Optimizer\n",
    "    opt = tf.train.AdamOptimizer().minimize(mse)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        num_itr = int(np.shape(y_outputs)[0] / batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for i in range(num_itr):\n",
    "                predicted_output = sess.run(y_, feed_dict={X: x_inputs, Y: y_outputs})\n",
    "                \n",
    "#                 _, c = sess.run([optimizer, mse], feed_dict={x: x_inputs, y: y_outputs})\n",
    "    return predicted_output[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q(s,a) representing the (Quality) of action a at state is .. \n",
    "\n",
    "this Q value depends on the immediate reward r .. however, it'll be more effective if it takes the future rewards Q(s', a') into consideration .. \n",
    "\n",
    "the future rewards are discounted by probability gama .. cause the evironment is stochastic hence, it is uncertain that each time you select action a you gonna get the same reward r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning():\n",
    "    \n",
    "    avg_Q = []  \n",
    "    gamma = 0.9\n",
    "#     actions = actions_prep(df)\n",
    "\n",
    "# nepisode > https://stats.stackexchange.com/questions/250943/what-is-the-difference-between-episode-and-epoch-in-deep-q-learning\n",
    "# means one complete path from state, action, next_s, next_a, reward upuntil terminal state\n",
    "\n",
    "    nepisod = 1 #2 #np.shape(actions)[1]  ##22+ \n",
    "    \n",
    "    tuplesMx0, tuplesMx1, tuplesMx2, tuplesMx3, tuplesMx4, tuplesMx5, tuplesMx6, tuplesMx7, tuplesMx8, tuplesMx9, tuplesMx10, tuplesMx11 = tuple_()  # nepisod ==> 46, tuplesMx[:,-1] ==> 499   \n",
    "    \n",
    "    num_rows = np.shape(tuplesMx0)[0] #-1\n",
    "    \n",
    "    Q_predicted = np.zeros([num_rows, 12]) #num_rows, num_columns\n",
    "    \n",
    "    best_action = np.zeros([num_rows, 1])\n",
    "    \n",
    "    \n",
    "    for i in xrange(nepisod):\n",
    "        Q_optimal = tuplesMx0[:,-2] + gamma*np.amax(Q_predicted, axis=1)  #np.max(Q_predicted) # returns max value per row !\n",
    "        \n",
    "        Q_predicted0 = DQN_train(tuplesMx0, Q_optimal)\n",
    "        Q_predicted1 = DQN_train(tuplesMx1, Q_optimal)\n",
    "        Q_predicted2 = DQN_train(tuplesMx2, Q_optimal)\n",
    "        Q_predicted3 = DQN_train(tuplesMx3, Q_optimal)\n",
    "        Q_predicted4 = DQN_train(tuplesMx4, Q_optimal)\n",
    "        Q_predicted5 = DQN_train(tuplesMx5, Q_optimal)\n",
    "        Q_predicted6 = DQN_train(tuplesMx6, Q_optimal)\n",
    "        Q_predicted7 = DQN_train(tuplesMx7, Q_optimal)\n",
    "        Q_predicted8 = DQN_train(tuplesMx8, Q_optimal)\n",
    "        Q_predicted9 = DQN_train(tuplesMx9, Q_optimal)\n",
    "        Q_predicted10 = DQN_train(tuplesMx10, Q_optimal)\n",
    "        Q_predicted11 = DQN_train(tuplesMx11, Q_optimal)\n",
    "        Q_predicted = np.column_stack((Q_predicted0, Q_predicted1, Q_predicted2, Q_predicted3,\n",
    "                                      Q_predicted4, Q_predicted5, Q_predicted6, Q_predicted7,\n",
    "                                      Q_predicted8, Q_predicted9, Q_predicted10, Q_predicted11))\n",
    "    \n",
    "    \n",
    "    best_action = np.argmax(Q_predicted, axis=1) \n",
    "        \n",
    "    return best_action, Q_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104954,)\n"
     ]
    }
   ],
   "source": [
    "best_action, Q_optimal = Q_learning()\n",
    "best_action, Q_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(best_action, return_counts=True)\n",
    "\n",
    "best_actions_list = np.asarray((unique, counts)).T\n",
    "\n",
    "print \"Freq.\"\n",
    "print best_actions_list[np.argsort(best_actions_list[:, 1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, _ = df_split()\n",
    "\n",
    "print \"Proportion in %\"\n",
    "# print np.asarray((unique, counts/np.float(len(best_action)))) #.T\n",
    "for i in best_actions_list[:,1]:\n",
    "    print [i, \"%.2f\" % round((i/np.float(len(train))*100), 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average by the client group:\n",
    "\n",
    "### 1. Avg. Frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train['f0'].iloc[best_action == 2]) #avg freq of clients taken action 2 \n",
    "\n",
    "# el mafrood law run-et a repeat it 3la ba2e el actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Avg. Recency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg. Monetary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation: \n",
    "\n",
    "### Policy 2: DQN model trained on the discrete action space\n",
    "\n",
    "1. Avg reponse probability and avg donnation amount (before the model) ==> BaseCase\n",
    "2. Avg reponse probability and avg donnation amount (for the model) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basecase avg donnation amount:\n",
    "print np.mean(train['rew'])\n",
    "\n",
    "## avg donnation for the model\n",
    "print np.mean(Q_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment:\n",
    "Increasing batch size .. increases the output non zero values \n",
    "\n",
    "## Further modifications:\n",
    "Roger also implemented the below code that simulates the whole customer's donnation process .. instead of applying DQN .. \n",
    "\n",
    "https://github.com/sisl/CustomerSim/blob/master/simulate_vs.py\n",
    "\n",
    "As well as these guys .. \n",
    "\n",
    "https://github.com/ugo-nama-kun/DQN-chainer/blob/master/dqn_agent_nature.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
