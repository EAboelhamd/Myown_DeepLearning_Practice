{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "Main Goals:\n",
    "\n",
    "1. Identify the recipients that will engage with the campaign.\n",
    "2. Maximise the campaignâ€™s revenue.\n",
    "\n",
    "\n",
    "Comments\n",
    "\n",
    "- The dataset contains only 5% of donors.\n",
    "- The donations are usually smaller than $20.\n",
    "- This data is quite noisy, high dimensional.\n",
    "- There is an inverse relationship between the probability to donate and the amount donated.\n",
    "\n",
    "\n",
    "Link for dataset and some analysis ==> \n",
    "\n",
    "https://github.com/rebordao/kdd98cup\n",
    "\n",
    "https://github.com/bobbyantonio/KDD98/blob/master/CleanData.py\n",
    "\n",
    "- Github solutions ==>\n",
    "https://github.com/rebordao/kdd98cup\n",
    "\n",
    "\n",
    "- Siraj notebook for a better data visualization:\n",
    "\n",
    "https://www.youtube.com/watch?v=yQsOFWqpjkE\n",
    "\n",
    "- Implementations:\n",
    "\n",
    "https://github.com/sisl/CustomerSim/blob/master/src/kdd98_preprocess.R\n",
    "\n",
    "https://github.com/ugo-nama-kun/DQN-chainer/blob/master/dqn_agent_nature.py\n",
    "\n",
    "- Yegor Tk's website => http://yegortkachenko.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# from tensorflow.python.ops import rnn, rnn_cell\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "## plotting .. \n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import random \n",
    "# random.seed(123)\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "## warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv('tuple.csv', header = 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_split():\n",
    "    df = load_data()\n",
    "    train, test = train_test_split(df, test_size = 0.9)  # split data to 50-50 cross validate \n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = df_split()\n",
    "# np.shape(train)\n",
    "# len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(train)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_():\n",
    "    \n",
    "    train, test = df_split()\n",
    "\n",
    "    train = train.convert_objects(convert_numeric=True)\n",
    "    \n",
    "    next_actions = np.zeros([len(train), 12])\n",
    "    \n",
    "    #     # fill in next_actions  \n",
    "    for i in xrange(np.shape(train)[1]):\n",
    "        next_actions[:, i] = i\n",
    "    \n",
    "    \n",
    "    # next_state_next_action\n",
    "    tuplesMx0 = np.column_stack((train, next_actions[:,0]))\n",
    "    tuplesMx1 = np.column_stack((train, next_actions[:,1]))\n",
    "    tuplesMx2 = np.column_stack((train, next_actions[:,2]))\n",
    "    tuplesMx3 = np.column_stack((train, next_actions[:,3]))\n",
    "    tuplesMx4 = np.column_stack((train, next_actions[:,4]))\n",
    "    tuplesMx5 = np.column_stack((train, next_actions[:,5]))\n",
    "    tuplesMx6 = np.column_stack((train, next_actions[:,6]))\n",
    "    tuplesMx7 = np.column_stack((train, next_actions[:,7]))\n",
    "    tuplesMx8 = np.column_stack((train, next_actions[:,8]))\n",
    "    tuplesMx9 = np.column_stack((train, next_actions[:,9]))\n",
    "    tuplesMx10 = np.column_stack((train, next_actions[:,10]))\n",
    "    tuplesMx11 = np.column_stack((train, next_actions[:,11]))\n",
    "    \n",
    "    return tuplesMx0, tuplesMx1, tuplesMx2, tuplesMx3, tuplesMx4, tuplesMx5, tuplesMx6, tuplesMx7, tuplesMx8, tuplesMx9, tuplesMx10, tuplesMx11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuplesMx0, tuplesMx1, tuplesMx2, tuplesMx3, tuplesMx4, tuplesMx5, tuplesMx6, tuplesMx7, tuplesMx8, tuplesMx9, tuplesMx10, tuplesMx11  = tuple_()\n",
    "# tuplesMx0, tuplesMx1 = tuple_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression phase:\n",
    "\n",
    "Before performing the prediction task .. let's split the data to training and validation sets .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid any problems in prediction by having string variables .. let's binarize (catergorize) all the variables .. \n",
    "\n",
    "Guidance ==> https://pythonprogramming.net/rnn-tensorflow-python-machine-learning-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Deep Neural Network (DQN):\n",
    "\n",
    "https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training:\n",
    "\n",
    "https://stackoverflow.com/questions/46832151/tensorflow-neural-network-multi-layer-perceptron-for-regression-example\n",
    "\n",
    "Guidance: https://www.youtube.com/watch?v=FbJw8J0rTyQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def DQN_train(x_inputs, y_outputs):\n",
    "    \n",
    "#     Q_optimal = [] \n",
    "#     Num_itrs = 2  \n",
    "#     batch_size = 500 \n",
    "#     n_nodes_hl1 = 40 \n",
    "#     n_nodes_hl2 = 15 #instead of 15 \n",
    "#     n_output = np.shape(x_inputs)[0] #12 #500\n",
    "#     NUM_DIM = np.shape(x_inputs)[1] \n",
    "#     target = 1\n",
    "    \n",
    "#     ######################## set learning variables ##################\n",
    "    \n",
    "#     learning_rate = 0.001  #Learning rate controls how much to change the weight to correct for the error. \n",
    "    \n",
    "# ## optimal num epochs ==> https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks\n",
    "\n",
    "#     epochs = 100 ## one complete path (forward/ backward) over all the data\n",
    "    \n",
    "#     ######################## set some variables #######################\n",
    "#     x = tf.placeholder(tf.float32)  # 3 features  # , [None, NUM_DIM], name='x'\n",
    "#     y = tf.placeholder(tf.float32)  # 3 outputs   # , [None, 1], name='y'\n",
    "\n",
    "    \n",
    "#     # hidden layer 1\n",
    "#     W1 = tf.Variable(tf.truncated_normal([NUM_DIM, n_nodes_hl1],0, 1), name='W1')\n",
    "#     b1 = tf.Variable(tf.truncated_normal([n_nodes_hl1],0,1), name='b1')\n",
    "\n",
    "#     # hidden layer 2\n",
    "#     W2 = tf.Variable(tf.truncated_normal([n_nodes_hl1, n_nodes_hl2],0,1), name='W2')\n",
    "#     b2 = tf.Variable(tf.truncated_normal([n_nodes_hl2],0,1), name='b2')\n",
    "    \n",
    "#     ## I think this one needs to be updated:\n",
    "#     ## three layers network needs only two Ws, Bs ! ..\n",
    "#     ## Link: https://github.com/jldbc/numpy_neural_net/blob/master/three_layer_network.py\n",
    "    \n",
    "#      # output\n",
    "#     W3 = tf.Variable(tf.random_uniform([n_nodes_hl2, n_output],0,1), name='W2')\n",
    "#     b3 = tf.Variable(tf.random_uniform([n_output],0,1), name='b2')\n",
    "\n",
    "#     ######################## Activations, outputs ######################\n",
    "#     # output hidden layer 1\n",
    "#     hidden_out = tf.nn.relu(tf.add(tf.matmul(x, W1), b1))\n",
    "    \n",
    "# #      total output\n",
    "#     hidden_out2 = tf.nn.relu(tf.add(tf.matmul(hidden_out, W2), b2))\n",
    "    \n",
    "    #      total output\n",
    "#     hidden_out3 = tf.nn.relu(tf.add(tf.matmul(hidden_out2, n_output), b3))\n",
    "    \n",
    "# #     #     total output\n",
    "#     y_ = tf.nn.relu(tf.add(tf.matmul(hidden_out3, 1), 1))\n",
    "    \n",
    "#     print y_\n",
    "    \n",
    "#     ####################### Loss Function  #########################\n",
    "#     mse = tf.losses.mean_squared_error(y, y_)\n",
    "\n",
    "#     ## I can try Adam optimizer ==> = GD Momentum + RMSProb \n",
    "#     ####################### Optimizer ######################### read more about RMSProb: https://www.youtube.com/watch?v=_e-LFe_igno\n",
    "#     optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=0.99).minimize(mse)  # optimizer like Gredient Descent \n",
    "\n",
    "#     ###################### Initialize, Accuracy and Run #################\n",
    "#     # initialize variables\n",
    "#     init_op = tf.global_variables_initializer()\n",
    "\n",
    "#     # accuracy for the test set\n",
    "#     cost = tf.reduce_mean(tf.square(tf.subtract(y, y_)))  # or could use tf.losses.mean_squared_error\n",
    "    \n",
    "    \n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(init_op)\n",
    "#         num_itr = int(np.shape(y_outputs)[0] / batch_size)\n",
    "        \n",
    "#         for epoch in range(epochs):\n",
    "# #             avg_cost = 0\n",
    "            \n",
    "#             for i in range(num_itr):\n",
    "# #                 batch_x, batch_y = x_inputs[i * batch_size:min(i * batch_size + batch_size, len(x_inputs))], \\\n",
    "# #                              y_outputs[i * batch_size:min(i * batch_size + batch_size, len(y_outputs))]\n",
    "\n",
    "#                 predicted_output = sess.run(y_, feed_dict={x: x_inputs, y: y_outputs})\n",
    "                \n",
    "# #                 _, c = sess.run([optimizer, mse], feed_dict={x: x_inputs, y: y_outputs})\n",
    "\n",
    "# # #                 avg_cost += c / num_itr          \n",
    "            \n",
    "# # #             if epoch % 100 == 0:\n",
    "# # #                 avg_.append(avg_cost)\n",
    "# # #                 plt.plot(avg_)\n",
    "# # #                 plt.xlabel('Step Number')\n",
    "# # #                 plt.ylabel('Prediction Loss')\n",
    "\n",
    "#         return predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_train(x_inputs, y_outputs):\n",
    "    \n",
    "    print np.shape(y_outputs)\n",
    "    \n",
    "    # Placeholder\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, np.shape(x_inputs)[1] ])\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "\n",
    "\n",
    "    # Model architecture parameters\n",
    "    n_stocks = np.shape(x_inputs)[1] \n",
    "    n_neurons_1 = 40\n",
    "    n_neurons_2 = 15\n",
    "#     n_neurons_3 = 256\n",
    "#     n_neurons_4 = 128\n",
    "    n_target = 50 #np.shape(x_inputs)[0]\n",
    "    \n",
    "    batch_size = 500 \n",
    "    epochs = 100\n",
    "    \n",
    "        # Initializers\n",
    "    sigma = 1\n",
    "    weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n",
    "    bias_initializer = tf.zeros_initializer()\n",
    "    \n",
    "\n",
    "    # Layer 1: Variables for hidden weights and biases\n",
    "    W_hidden_1 = tf.Variable(weight_initializer([n_stocks, n_neurons_1]))\n",
    "    bias_hidden_1 = tf.Variable(bias_initializer([n_neurons_1]))\n",
    "\n",
    "    # Layer 2: Variables for hidden weights and biases\n",
    "    W_hidden_2 = tf.Variable(weight_initializer([n_neurons_1, n_neurons_2]))\n",
    "    bias_hidden_2 = tf.Variable(bias_initializer([n_neurons_2]))\n",
    "\n",
    "#     # Layer 3: Variables for hidden weights and biases\n",
    "#     W_hidden_3 = tf.Variable(weight_initializer([n_neurons_2, n_neurons_3]))\n",
    "#     bias_hidden_3 = tf.Variable(bias_initializer([n_neurons_3]))\n",
    "\n",
    "#     # Layer 4: Variables for hidden weights and biases\n",
    "#     W_hidden_4 = tf.Variable(weight_initializer([n_neurons_3, n_neurons_4]))\n",
    "#     bias_hidden_4 = tf.Variable(bias_initializer([n_neurons_4]))\n",
    "\n",
    "    # Output layer: Variables for output weights and biases\n",
    "    W_out = tf.Variable(weight_initializer([n_neurons_2, n_target]))\n",
    "    bias_out = tf.Variable(bias_initializer([n_target]))\n",
    "\n",
    "\n",
    "    # Hidden layer\n",
    "    hidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))\n",
    "    hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))\n",
    "#     hidden_3 = tf.nn.relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))\n",
    "#     hidden_4 = tf.nn.relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))\n",
    "\n",
    "    # Output layer (must be transposed)\n",
    "    y_ = tf.add(tf.matmul(hidden_2, W_out), bias_out)\n",
    "    \n",
    "    # initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    # Cost function\n",
    "    mse = tf.reduce_mean(tf.squared_difference(y_, Y))\n",
    "\n",
    "    # Optimizer\n",
    "    opt = tf.train.AdamOptimizer().minimize(mse)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        num_itr = int(np.shape(y_outputs)[0] / batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for i in range(num_itr):\n",
    "                predicted_output = sess.run(y_, feed_dict={X: x_inputs, Y: y_outputs})\n",
    "                \n",
    "#                 _, c = sess.run([optimizer, mse], feed_dict={x: x_inputs, y: y_outputs})\n",
    "    return predicted_output[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q(s,a) representing the (Quality) of action a at state is .. \n",
    "\n",
    "this Q value depends on the immediate reward r .. however, it'll be more effective if it takes the future rewards Q(s', a') into consideration .. \n",
    "\n",
    "the future rewards are discounted by probability gama .. cause the evironment is stochastic hence, it is uncertain that each time you select action a you gonna get the same reward r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning():\n",
    "    \n",
    "    avg_Q = []  \n",
    "    gamma = 0.9\n",
    "#     actions = actions_prep(df)\n",
    "\n",
    "# nepisode > https://stats.stackexchange.com/questions/250943/what-is-the-difference-between-episode-and-epoch-in-deep-q-learning\n",
    "# means one complete path from state, action, next_s, next_a, reward upuntil terminal state\n",
    "\n",
    "    nepisod = 1#2 #np.shape(actions)[1]  ##22+ \n",
    "    \n",
    "    tuplesMx0, tuplesMx1, tuplesMx2, tuplesMx3, tuplesMx4, tuplesMx5, tuplesMx6, tuplesMx7, tuplesMx8, tuplesMx9, tuplesMx10, tuplesMx11 = tuple_()  # nepisod ==> 46, tuplesMx[:,-1] ==> 499   \n",
    "    num_rows = np.shape(tuplesMx0)[0] #-1\n",
    "    \n",
    "    Q_predicted = np.zeros([num_rows, 12]) #num_rows, num_columns\n",
    "    \n",
    "    best_action = np.zeros([12, 1])\n",
    "    \n",
    "    \n",
    "    for i in xrange(nepisod):\n",
    "        Q_optimal = tuplesMx0[:,-2] + gamma*np.amax(Q_predicted, axis=1)  #np.max(Q_predicted) # returns max value per row !\n",
    "        Q_predicted0 = DQN_train(tuplesMx0, Q_optimal)\n",
    "        Q_predicted1 = DQN_train(tuplesMx1, Q_optimal)\n",
    "        Q_predicted2 = DQN_train(tuplesMx2, Q_optimal)\n",
    "        Q_predicted3 = DQN_train(tuplesMx3, Q_optimal)\n",
    "        Q_predicted4 = DQN_train(tuplesMx4, Q_optimal)\n",
    "        Q_predicted5 = DQN_train(tuplesMx5, Q_optimal)\n",
    "        Q_predicted6 = DQN_train(tuplesMx6, Q_optimal)\n",
    "        Q_predicted7 = DQN_train(tuplesMx7, Q_optimal)\n",
    "        Q_predicted8 = DQN_train(tuplesMx8, Q_optimal)\n",
    "        Q_predicted9 = DQN_train(tuplesMx9, Q_optimal)\n",
    "        Q_predicted10 = DQN_train(tuplesMx10, Q_optimal)\n",
    "        Q_predicted11 = DQN_train(tuplesMx11, Q_optimal)\n",
    "        Q_predicted = np.column_stack((Q_predicted0, Q_predicted1, Q_predicted2, Q_predicted3,\n",
    "                                      Q_predicted4, Q_predicted5, Q_predicted6, Q_predicted7,\n",
    "                                      Q_predicted8, Q_predicted9, Q_predicted10, Q_predicted11))\n",
    "    \n",
    "#     for i in xrange(num_rows):\n",
    "    best_action = np.argmax(Q_predicted, axis=1) \n",
    "        \n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209908,)\n",
      "(209908,)\n",
      "(209908,)\n",
      "(209908,)\n",
      "(209908,)\n",
      "(209908,)\n",
      "(209908,)\n",
      "(209908,)\n",
      "(209908,)\n",
      "(209908,)\n",
      "(209908,)\n",
      "(209908,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 8,  7,  7, ...,  7,  9, 10])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_action = Q_learning()\n",
    "best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq.\n",
      "[[    0     3]\n",
      " [    2   209]\n",
      " [    3 94609]\n",
      " [    4  1000]\n",
      " [    5    11]\n",
      " [    7 50851]\n",
      " [    8 28150]\n",
      " [    9 15854]\n",
      " [   10 19221]]\n"
     ]
    }
   ],
   "source": [
    "# Counter(best_action)\n",
    "\n",
    "unique, counts = np.unique(best_action, return_counts=True)\n",
    "\n",
    "# print \"Relative Freq.\"\n",
    "# print np.asarray((unique, counts/np.float(len(best_action)))) #.T\n",
    "\n",
    "print \"Freq.\"\n",
    "print np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing batch size .. increases the output non zero values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
