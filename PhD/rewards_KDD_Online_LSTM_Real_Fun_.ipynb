{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "Main Goals:\n",
    "\n",
    "1. Identify the recipients that will engage with the campaign.\n",
    "2. Maximise the campaignâ€™s revenue.\n",
    "\n",
    "\n",
    "Comments\n",
    "\n",
    "- The dataset contains only 5% of donors.\n",
    "- The donations are usually smaller than $20.\n",
    "- This data is quite noisy, high dimensional.\n",
    "- There is an inverse relationship between the probability to donate and the amount donated.\n",
    "\n",
    "\n",
    "Link for dataset and some analysis ==> \n",
    "\n",
    "https://github.com/rebordao/kdd98cup\n",
    "\n",
    "https://github.com/bobbyantonio/KDD98/blob/master/CleanData.py\n",
    "\n",
    "- Github solutions ==>\n",
    "https://github.com/rebordao/kdd98cup\n",
    "\n",
    "\n",
    "- Siraj notebook for a better data visualization:\n",
    "\n",
    "https://www.youtube.com/watch?v=yQsOFWqpjkE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "\n",
    "## warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    n = 500 #number of records in file\n",
    "    s = 100 #desired sample size\n",
    "    df = pd.read_csv('tuple.csv', header = None, skiprows = sorted(random.sample(range(n),n-s)))\n",
    "    df.columns = ['r0','f0','m0','ir0','if0', 'a','r1', 'f1', 'm1','ir1','if1','rew']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r0</th>\n",
       "      <th>f0</th>\n",
       "      <th>m0</th>\n",
       "      <th>ir0</th>\n",
       "      <th>if0</th>\n",
       "      <th>a</th>\n",
       "      <th>r1</th>\n",
       "      <th>f1</th>\n",
       "      <th>m1</th>\n",
       "      <th>ir1</th>\n",
       "      <th>if1</th>\n",
       "      <th>rew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   r0  f0   m0  ir0  if0  a  r1  f1   m1  ir1  if1  rew\n",
       "0   0   0  0.0    0    0  5   1   0  0.0    0    1  0.0\n",
       "1   0   0  0.0    0    0  0   1   0  0.0    1    0  0.0\n",
       "2   0   0  0.0    0    0  5   1   0  0.0    0    1  0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## I have to run it over all the data not just 100 records\n",
    "df = load_data()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048165, 11)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df['a'].unique()\n",
    "b = np.arange(len(df['a'].unique()))\n",
    "actions_ = np.zeros([np.shape(df['a'])[0], len(df['a'].unique())])\n",
    "actions_[a, b] = 1\n",
    "np.shape(actions_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_split():\n",
    "    \n",
    "    df = load_data()\n",
    "#     train, test = train_test_split(df, test_size = 0.999)  # split data to 50-50 cross validate, Roger 1.6*1000,000\n",
    "    \n",
    "    train_x = df[df.columns.difference(['a', 'r0', 'f0', 'm0', 'ir0', 'if0'])]\n",
    "    train_y = df[df.columns.difference(['rew', 'a', 'r0', 'f0', 'm0', 'ir0', 'if0'])]\n",
    "    \n",
    "#     train_y = train_y.convert_objects(convert_numeric = True)\n",
    "    \n",
    "#     train_y = train_y.convert_objects(convert_numeric = True)\n",
    "    \n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, _ = df_split()\n",
    "# train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tuple_(predicted_states):\n",
    "        \n",
    "#     next_actions = np.zeros([len(predicted_states), 12])\n",
    "    \n",
    "#     for i in range(11):\n",
    "#         next_actions[:, i] = i\n",
    "\n",
    "#     tuplesMx0 = np.column_stack((predicted_states, next_actions[:,0]))\n",
    "#     tuplesMx1 = np.column_stack((predicted_states, next_actions[:,1]))\n",
    "#     tuplesMx2 = np.column_stack((predicted_states, next_actions[:,2]))\n",
    "#     tuplesMx3 = np.column_stack((predicted_states, next_actions[:,3]))\n",
    "#     tuplesMx4 = np.column_stack((predicted_states, next_actions[:,4]))\n",
    "#     tuplesMx5 = np.column_stack((predicted_states, next_actions[:,5]))\n",
    "#     tuplesMx6 = np.column_stack((predicted_states, next_actions[:,6]))\n",
    "#     tuplesMx7 = np.column_stack((predicted_states, next_actions[:,7]))\n",
    "#     tuplesMx8 = np.column_stack((predicted_states, next_actions[:,8]))\n",
    "#     tuplesMx9 = np.column_stack((predicted_states, next_actions[:,9]))\n",
    "#     tuplesMx10 = np.column_stack((predicted_states, next_actions[:,10]))\n",
    "#     tuplesMx11 = np.column_stack((predicted_states, next_actions[:,11]))\n",
    "    \n",
    "#     return tuplesMx0, tuplesMx1, tuplesMx2, tuplesMx3, tuplesMx4, tuplesMx5, tuplesMx6, tuplesMx7, tuplesMx8, tuplesMx9, tuplesMx10, tuplesMx11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuplesMx0, tuplesMx1, tuplesMx2, tuplesMx3, tuplesMx4, tuplesMx5, tuplesMx6, tuplesMx7, tuplesMx8, tuplesMx9, tuplesMx10, tuplesMx11  = tuple_()\n",
    "# tuplesMx0, tuplesMx1, tuplesMx2, tuplesMx3, tuplesMx4, tuplesMx5, tuplesMx6, tuplesMx7, tuplesMx8, tuplesMx9, tuplesMx10, tuplesMx11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression phase:\n",
    "\n",
    "Before performing the prediction task .. let's split the data to training and validation sets .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid any problems in prediction by having string variables .. let's binarize (catergorize) all the variables .. \n",
    "\n",
    "Guidance ==> https://pythonprogramming.net/rnn-tensorflow-python-machine-learning-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "Num_itrs = 2  # no loop 3leha ( w dah el sa7) .. we just have to run the whole algo. 10 times and report the avg. results\n",
    "\n",
    "num_epoch = 1 #23 #epochs are cycles of Feedforward and Backprob\n",
    "## el mafrood yeb2a feh loop 3la el epochs elli heyya el steps .. w avg. reward per step is calculated \n",
    "batch_size = 5\n",
    "chunkSize = 1\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 10        # Total episodes for training\n",
    "max_steps = 100              # Max possible steps in an episode\n",
    "\n",
    "possible_actions = df['a'].unique()\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "\n",
    "train_x, train_y = df_split()\n",
    "\n",
    "n_nodes_hl1 = 40 #np.shape(train)[1]\n",
    "n_nodes_hl2 = 15 #np.shape(train)[0]\n",
    "# NUM_STATES = np.shape(train)[1]\n",
    "NUM_DIM =  (np.shape(train_x)[1])\n",
    "num_nodes = np.shape(train_x)[0] \n",
    "NUM_DIM_output = np.shape(train_y)[1]\n",
    "\n",
    "num_unrollings = 5\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 10   # Number of experiences the Memory can keep\n",
    "\n",
    "# best_actions = np.zeros([np.shape(train)[0], batch_size])\n",
    "Q_optimal = [] #np.zeros([np.shape(curr_state_current_action)[0], len(df['ACCOUNT_STATUS'].unique())])\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "## for training \n",
    "x = tf.placeholder(dtype = tf.float32, shape=[num_nodes, NUM_DIM])\n",
    "y =  tf.placeholder(dtype=tf.float32, shape=[11, None])\n",
    "    \n",
    "## building the network for predicting actions\n",
    "# inputs_ = tf.placeholder(DT_FLOAT, [np.shape(train_x)[0], np.shape(train_x)[1]])\n",
    "# pred_actions = tf.placeholder(dtype = tf.int32, shape = [None, 12])\n",
    "\n",
    "    \n",
    "\n",
    "stack_size  = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((10,np.shape(train_y)[1]-1), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "# Input gate: input, previous output, and bias.\n",
    "ix = tf.Variable(tf.random_uniform([NUM_DIM, NUM_DIM_output],0,5, dtype = tf.float32))# init_weights_RNN([n_nodes_hl1, NUM_ACTIONS])\n",
    "im = tf.Variable(tf.random_uniform([NUM_DIM_output, NUM_DIM_output],0,5, dtype = tf.float32))\n",
    "ib = tf.Variable(tf.zeros([1, NUM_DIM_output], dtype = tf.float32))\n",
    "\n",
    "# Forget gate: input, previous output, and bias.\n",
    "fx = tf.Variable(tf.random_uniform([NUM_DIM, NUM_DIM_output],0,5, dtype = tf.float32))\n",
    "fm = tf.Variable(tf.random_uniform([NUM_DIM_output, NUM_DIM_output],0,5, dtype = tf.float32))\n",
    "fb = tf.Variable(tf.zeros([1, NUM_DIM_output], dtype = tf.float32))\n",
    "\n",
    "# Memory cell: input, state and bias.                             \n",
    "cx = tf.Variable(tf.random_uniform([NUM_DIM, NUM_DIM_output],0,5, dtype = tf.float32))\n",
    "cm = tf.Variable(tf.random_uniform([NUM_DIM_output, NUM_DIM_output],0,5, dtype = tf.float32))\n",
    "cb = tf.Variable(tf.zeros([1, NUM_DIM_output], dtype = tf.float32))\n",
    "\n",
    "# Output gate: input, previous output, and bias.\n",
    "ox = tf.Variable(tf.random_uniform([NUM_DIM, NUM_DIM_output],0,5, dtype = tf.float32))\n",
    "om = tf.Variable(tf.random_uniform([NUM_DIM_output, NUM_DIM_output],0,5, dtype = tf.float32))\n",
    "ob = tf.Variable(tf.zeros([NUM_DIM_output], dtype = tf.float32))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([num_nodes, NUM_DIM_output], dtype = tf.float32), trainable = False) \n",
    "\n",
    "saved_state = tf.Variable(tf.zeros([num_nodes, NUM_DIM_output], dtype = tf.float32), trainable = False) \n",
    "\n",
    "# Classifier weights and biases.\n",
    "w = tf.Variable(tf.random_uniform([NUM_DIM_output, NUM_DIM_output], 0, 5, dtype = tf.float32))\n",
    "b = tf.Variable(tf.zeros([NUM_DIM_output], dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the cell computation.\n",
    "# this method takes single cell and returns single number \n",
    "def lstm_cell(i, o, state):\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate*tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(train_x, saved_output, saved_state):\n",
    "    # Unrolled LSTM loop.     \n",
    "#     output_ = list()\n",
    "    output = saved_output  # row !\n",
    "    state = saved_state  # row !\n",
    "\n",
    "    \n",
    "    output_, state = lstm_cell(train_x.values.astype(np.float32), tf.cast(output, tf.float32), tf.cast(state, tf.float32)) \n",
    "    \n",
    "    # astype('U') .. to convert numpy array to string ..\n",
    "#     for i in range(np.shape(train_x)[0]):## el loop faydetha to copy the next line that is just for single unit \n",
    "#         output_, state = lstm_cell(train_x.values[i, None].astype(np.float32), output, state) \n",
    "\n",
    "    \n",
    "        # in case the last values are saved !\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        model_output = tf.matmul(output_, w) + b # outputs single value\n",
    "    \n",
    "    \n",
    "    return model_output ## the output for the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model():\n",
    "\n",
    "    loss_RNN = []\n",
    "    \n",
    "    model_output = lstm_model(train_x, saved_output, saved_state) #output here is a vector\n",
    "\n",
    "    cost = tf.reduce_mean(tf.square(y - model_output))\n",
    "\n",
    "    optimize = tf.train.GradientDescentOptimizer(0.001).minimize(cost) \n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()  ## updated version from initialize_all_variables :) \n",
    "    sess.run(init)\n",
    "\n",
    "    \n",
    "    Predicted_states = sess.run(model_output, feed_dict={x:train_x.values.astype(np.float32), y:train_y.values.astype(np.float32)})\n",
    "    \n",
    "    # Cost calculation\n",
    "    for step in range(1000):\n",
    "        l,_ = sess.run([cost, optimize], feed_dict={x:train_x.values, y:train_y.values})\n",
    "    \n",
    "        if step % 100 == 0:\n",
    "            loss_RNN.append(l)\n",
    "     \n",
    "    sess.close()            \n",
    "    \n",
    "#     print Predicted_states\n",
    "    \n",
    "    return Predicted_states, loss_RNN  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reward_prediction():\n",
    "#     predicted_states, loss_RNN = train_lstm_model()\n",
    "#     print predicted_states\n",
    "#     plt.plot(loss_RNN)\n",
    "#     plt.xlabel('Step number')\n",
    "#     plt.ylabel('Prediction Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the rewards:\n",
    "Now, the LSTM_RNN output (from validation phase) is considered as input for DQN model, to be able to select the action that has the maximum longtime reward for the customer (highest CLV) .. \n",
    "\n",
    "https://arxiv.org/pdf/1602.01580.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Deep Neural Network (DQN):\n",
    "\n",
    "https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## call el fun that predicts next_states and then combine them with the tuple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "## this might be converted to a loop over the esipods .. however, we might assume delw2ty ennnhom single episod \n",
    "# Render the environment\n",
    "# game.new_episode()\n",
    "\n",
    "# for i in range(pretrain_length):\n",
    "for j in range(np.shape(train_x)[0]):\n",
    "    # If it's the first step\n",
    "    if j == 0:\n",
    "        # First we need a state\n",
    "        state = train_x[train_x.columns.difference(['rew'])] #game.get_state().screen_buffer\n",
    "#         state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "        # Random action\n",
    "        action = random.choice(possible_actions)\n",
    "\n",
    "        selected_a = df['a'].loc[df['a'] == action]\n",
    "        row = np.random.choice(selected_a, 1)\n",
    "        \n",
    "        # Get the rewards\n",
    "        reward = train_x['rew'][row] #game.make_action(action)  ## select the rewards corresponding to action a randomly \n",
    "        \n",
    "    #     # Look if the episode is finished\n",
    "        done = True\n",
    "        \n",
    "            # If we're dead\n",
    "        if done:\n",
    "            # We finished the episode\n",
    "            next_state = np.zeros(state.shape)\n",
    "            \n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "            ## i think el 7etta deh might be handled b el batch \n",
    "            \n",
    "#             # Start a new episode\n",
    "#             game.new_episode()\n",
    "\n",
    "#             # First we need a state\n",
    "#             state = game.get_state().screen_buffer\n",
    "\n",
    "#             # Stack the frames\n",
    "#             state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "#         else:\n",
    "#             # Get the next state\n",
    "#             next_state = game.get_state().screen_buffer\n",
    "#             next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "#             # Add experience to memory\n",
    "#             memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "#             # Our state is now the next_state\n",
    "#             state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # i need to predict actions \n",
    "# def DQN_train(x_inputs, y_outputs):\n",
    "\n",
    "#     # Placeholder\n",
    "#     X = tf.placeholder(dtype=tf.float32, shape=[None, np.shape(x_inputs)[1]])\n",
    "#     Y = tf.placeholder(dtype=tf.float32, shape=[11, ])\n",
    "    \n",
    "\n",
    "#     # Model architecture parameters\n",
    "#     n_dim = np.shape(x_inputs)[1] \n",
    "#     n_neurons_1 = 40\n",
    "#     n_neurons_2 = 15\n",
    "#     n_target = 1 #np.shape(x_inputs)[0]\n",
    "    \n",
    "#     batch_size = 500 \n",
    "#     epochs = 100\n",
    "    \n",
    "#     pred_actions = []\n",
    "    \n",
    "#     # Initializers\n",
    "#     sigma = 1\n",
    "    \n",
    "#     #First Q Network\n",
    "#     w1 = tf.Variable(tf.random_uniform([n_dim, n_neurons_1], 0, 0.1))\n",
    "#     bias1 = tf.Variable(tf.random_uniform([n_neurons_1], 0, 0.1))\n",
    "    \n",
    "#     w2 = tf.Variable(tf.random_uniform([n_neurons_1, n_neurons_2], 0, 0.1))\n",
    "#     bias2 = tf.Variable(tf.random_uniform([n_neurons_2], 0, 0.1))\n",
    "    \n",
    "#     w3 = tf.Variable(tf.random_uniform([n_neurons_2, n_target], 0, 0.1))\n",
    "#     bias3 = tf.Variable(tf.random_uniform([n_target], 0, 0.1))\n",
    "    \n",
    "    \n",
    "#     hidden_1 = tf.nn.relu(tf.matmul(X, w1) + bias1)\n",
    "#     hidden_2 = tf.nn.relu(tf.matmul(hidden_1, w2) + bias2)\n",
    "#     y_ = tf.matmul(hidden_2, w3) + bias3\n",
    "    \n",
    "#     # initialize variables\n",
    "#     init_op = tf.global_variables_initializer()\n",
    "    \n",
    "#     # Cost function\n",
    "#     mse = tf.reduce_mean(tf.squared_difference(y_, Y))\n",
    "\n",
    "#     # Optimizer\n",
    "#     opt = tf.train.RMSPropOptimizer(0.0001, 0.99).minimize(mse)\n",
    "\n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(init_op)\n",
    "#         num_itr = int(np.shape(y_outputs)[0] / batch_size)\n",
    "#         pred_actions = sess.run(y_, feed_dict={X: x_inputs, Y: y_outputs})\n",
    "                      \n",
    "#     return pred_actions.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This function will do the part\n",
    "# With Ïµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "# \"\"\"\n",
    "# def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "#     ## EPSILON GREEDY STRATEGY\n",
    "#     # Choose action a from state s using epsilon greedy.\n",
    "#     ## First we randomize a number\n",
    "#     exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "#     # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "#     explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "#     if (explore_probability > exp_exp_tradeoff):\n",
    "#         # Make a random action (exploration)\n",
    "#         action = random.choice(possible_actions)\n",
    "        \n",
    "#     else:\n",
    "#         # Get action from Q-network (exploitation)\n",
    "#         # Estimate the Qs values state\n",
    "#         pred_actions = DQN_train(train_x.values.astype(np.float32), np.asarray(df['a'].unique())) \n",
    "# #         pred_actions = [] #tf.placeholder(dtype=tf.float32, shape=[11, None])\n",
    "#         Qs = sess.run(pred_actions, feed_dict={x:train_x.values.astype(np.float32).reshape((1, *train_x.values.shape))})\n",
    "# #     #sess.run(y, feed_dict = {x:state})\n",
    "        \n",
    "# #         Take the biggest Q value (= the best action)\n",
    "# #         choice = np.argmax(Qs)\n",
    "# #         action = possible_actions[int(choice)]\n",
    "                \n",
    "# #     return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "training = True \n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "\n",
    "        # Init the game\n",
    "#         game.init()\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "#             game.new_episode()\n",
    "#             state = game.get_state().screen_buffer\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "#             state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "                \n",
    "                exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "                # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "                explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "\n",
    "                if (explore_probability > exp_exp_tradeoff):\n",
    "                    # Make a random action (exploration)\n",
    "                    action = random.choice(possible_actions)\n",
    "\n",
    "                   #Do the action\n",
    "                selected_a = df['a'].loc[df['a'] == action]\n",
    "                row = np.random.choice(selected_a, 1)\n",
    "\n",
    "                # Get the rewards\n",
    "                reward = train_x['rew'][row]\n",
    "\n",
    "                # Look if the episode is finished\n",
    "                done = True\n",
    "                \n",
    "                # Add the reward to total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "#                 if done:\n",
    "#                     # the episode ends so no next state\n",
    "\n",
    "        # ==> here el state has to be stacked b enniha tet2a66a3 \n",
    "\n",
    "#                     next_state = #np.zeros((10,np.shape(train_y)[1]-1), dtype=np.int)\n",
    "#                     next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "#                     print('Episode: {}'.format(episode),\n",
    "#                               'Total reward: {}'.format(total_reward),\n",
    "#                               'Training loss: {:.4f}'.format(loss),\n",
    "#                               'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "#                 else:\n",
    "#                     # Get the next state\n",
    "#                     next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "#                     # Stack the frame of the next_state\n",
    "#                     next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "#                 ### LEARNING PART            \n",
    "#                 # Obtain random mini-batch from memory\n",
    "#                 batch = memory.sample(batch_size)\n",
    "#                 states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "#                 actions_mb = np.array([each[1] for each in batch])\n",
    "#                 rewards_mb = np.array([each[2] for each in batch]) \n",
    "#                 next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "#                 dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "#                 target_Qs_batch = []\n",
    "\n",
    "#                  # Get Q values for next_state \n",
    "#                 Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "#                 # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "#                 for i in range(0, len(batch)):\n",
    "#                     terminal = dones_mb[i]\n",
    "\n",
    "#                     # If we are in a terminal state, only equals reward\n",
    "#                     if terminal:\n",
    "#                         target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "#                     else:\n",
    "#                         target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "#                         target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "#                 targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "#                 loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "#                                     feed_dict={DQNetwork.inputs_: states_mb,\n",
    "#                                                DQNetwork.target_Q: targets_mb,\n",
    "#                                                DQNetwork.actions_: actions_mb})\n",
    "\n",
    "#                 # Write TF Summaries\n",
    "#                 summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "#                                                    DQNetwork.target_Q: targets_mb,\n",
    "#                                                    DQNetwork.actions_: actions_mb})\n",
    "#                 writer.add_summary(summary, episode)\n",
    "#                 writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "Name: rew, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
