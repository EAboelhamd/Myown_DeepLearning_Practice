{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "Main Goals:\n",
    "\n",
    "1. Identify the recipients that will engage with the campaign.\n",
    "2. Maximise the campaignâ€™s revenue.\n",
    "\n",
    "\n",
    "Comments\n",
    "\n",
    "- The dataset contains only 5% of donors.\n",
    "- The donations are usually smaller than $20.\n",
    "- This data is quite noisy, high dimensional.\n",
    "- There is an inverse relationship between the probability to donate and the amount donated.\n",
    "\n",
    "\n",
    "Link for dataset and some analysis ==> \n",
    "\n",
    "https://github.com/rebordao/kdd98cup\n",
    "\n",
    "https://github.com/bobbyantonio/KDD98/blob/master/CleanData.py\n",
    "\n",
    "- Github solutions ==>\n",
    "https://github.com/rebordao/kdd98cup\n",
    "\n",
    "\n",
    "- Siraj notebook for a better data visualization:\n",
    "\n",
    "https://www.youtube.com/watch?v=yQsOFWqpjkE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from sklearn.model_selection import train_test_split\n",
    "from array_split import array_split, shape_split\n",
    "from sklearn import preprocessing\n",
    "# from sknn.mlp import Regressor, Layer\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "## plotting .. \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "## warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    list_of_lists = []\n",
    "\n",
    "    ## works correctly but takes too much running time ..\n",
    "    with open('mailingData_train.txt', 'r') as f:# open the file for reading\n",
    "        df = []\n",
    "        for row_num, line in enumerate(f):\n",
    "            # Remove the new line at the end and then split the string based on\n",
    "            # tabs. This creates a python list of the values.\n",
    "            values = line.strip().split(',')\n",
    "            if row_num == 0: # first line is the header\n",
    "                 header = values\n",
    "            else:\n",
    "                df.append([v for v in values])\n",
    "\n",
    "        df = pd.DataFrame(df)\n",
    "        df.columns = header\n",
    "        df.drop(df.index[0], inplace=True)\n",
    "        \n",
    "        df = df[0:100]  ## to save time .. I gonna work on only 100 records ..\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_explore():\n",
    "    df = load_data()\n",
    "    print df.TARGET_B.value_counts().plot(x=None, y=None, kind = 'pie', autopct='%1.1f%%')\n",
    "    \n",
    "    # % of donors\n",
    "    print 'Percentage of donors: %s' % (100.0 * sum(df.TARGET_B.astype('float'))/df.shape[0]) #about only 5% of the samples are doners .. \n",
    "    plt.hist(df.TARGET_D.value_counts(), bins = 7)   \n",
    "    plt.plot(df[df.TARGET_D > 0].TARGET_D) #Histogram is not the best choice .. let's try another plot .. \n",
    "    \n",
    "    # % of donors\n",
    "    print 'Percentage of donors: %s' % (100.0 * sum(df.TARGET_D.astype('float'))/df.shape[0])\n",
    "    #about 79% of the continous predictor are doners .. are there any donation amounts of zero ?!\n",
    "    print df.TARGET_D.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender data imputation ..\n",
    "It's very strange to have gender rather than M and F !! .. \n",
    "\n",
    "Let's impute any other value with the mode of this variable .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_impute():\n",
    "    df = load_data()\n",
    "    df['GENDER'] = np.where(df['GENDER'] == 'C', df['GENDER'].mode(), df['GENDER'])\n",
    "    df['GENDER'] = np.where(df['GENDER'] == 'U', df['GENDER'].mode(), df['GENDER'])\n",
    "    df['GENDER'] = np.where(df['GENDER'] == 'J', df['GENDER'].mode(), df['GENDER'])\n",
    "    df['GENDER'] = np.where(df['GENDER'] == 'A', df['GENDER'].mode(), df['GENDER'])\n",
    "    df['GENDER'] = np.where(df['GENDER'] == ' ', df['GENDER'].mode(), df['GENDER'])\n",
    "    \n",
    "    print df['GENDER'].unique()\n",
    "    \n",
    "    ## how donations are distributed per gender\n",
    "    df['GENDER'].value_counts().plot(kind='barh', stacked=True, fontsize=7, figsize=[9,8], colormap='gist_ncar')\n",
    "    plt.title('donations are distributed among age groups', fontsize=14, color='black') \n",
    "    plt.xlabel('Number of doners', fontsize=14, color='black') \n",
    "    plt.ylabel('Gender of doner', fontsize=14, color='black') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing:\n",
    "\n",
    "1. Gets some redundant variables .. by calculating the correlation between all the variables .. \n",
    "those of high correlation coeffecient are redundant .. \n",
    "\n",
    "__NOTE:__\n",
    "In this implementation .. \n",
    "\n",
    "https://github.com/EAboelhamd/kdd98cup/blob/master/donors.py\n",
    "\n",
    "They tried to figure out redundant variables to remove them .. to be able to decrease the dimentionality of the problem .. however, in my case, there is no need to do .. as I'm gonna implement deep learning not a shallow solution .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preperation():\n",
    "    df = load_data()\n",
    "    df = df[df.columns.difference(['TARGET_B'])]\n",
    "    data = ['TARGET_D', 'ODATEDW','OSOURCE','TCODE','STATE','ZIP','MAILCODE','PVASTATE','DOB','NOEXCH','RECINHSE','RECP3','RECPGVG','RECSWEEP','MDMAUD','DOMAIN','CLUSTER','AGE','AGEFLAG','HOMEOWNR','CHILD03','CHILD07','CHILD12',\n",
    "      'CHILD18','NUMCHLD','INCOME','GENDER','WEALTH1','HIT','MBCRAFT','MBGARDEN','MBBOOKS','MBCOLECT','MAGFAML','MAGFEM','MAGMALE',\n",
    "      'PUBGARDN','PUBCULIN','PUBHLTH','PUBDOITY','PUBNEWFN','PUBPHOTO','PUBOPP','DATASRCE','MALEMILI','MALEVET','VIETVETS','WWIIVETS','LOCALGOV','STATEGOV','FEDGOV','SOLP3','SOLIH',\n",
    "      'MAJOR','WEALTH2','GEOCODE','COLLECT1','VETERANS','BIBLE','CATLG','HOMEE','PETS','CDPLAY','STEREO','PCOWNERS','PHOTO','CRAFTS','FISHER','GARDENIN','BOATS','WALKER','KIDSTUFF','CARDS','PLATES','LIFESRC','PEPSTRFL','POP901','POP902','POP903','POP90C1','POP90C2','POP90C3','POP90C4','POP90C5','ETH1','ETH2','ETH3','ETH4','ETH5','ETH6','ETH7','ETH8','ETH9','ETH10','ETH11','ETH12','ETH13',\n",
    "      'ETH14','ETH15','ETH16','AGE901','AGE902','AGE903','AGE904','AGE905','AGE906','AGE907','CHIL1','CHIL2','CHIL3','AGEC1','AGEC2','AGEC3','AGEC4','AGEC5','AGEC6','AGEC7','CHILC1','CHILC2','CHILC3','CHILC4','CHILC5','HHAGE1','HHAGE2','HHAGE3','HHN1','HHN2','HHN3','HHN4','HHN5','HHN6','MARR1','MARR2','MARR3','MARR4','HHP1','HHP2','DW1','DW2','DW3','DW4','DW5','DW6','DW7','DW8','DW9','HV1','HV2','HV3','HV4','HU1','HU2','HU3','HU4','HU5','HHD1',\n",
    "      'HHD2','HHD3','HHD4','HHD5','HHD6','HHD7','HHD8','HHD9','HHD10','HHD11','HHD12','ETHC1','ETHC2','ETHC3','ETHC4','ETHC5','ETHC6','HVP1','HVP2','HVP3','HVP4',\n",
    "      'HVP5','HVP6','HUR1','HUR2','RHP1','RHP2','RHP3','RHP4','HUPA1','HUPA2','HUPA3','HUPA4','HUPA5','HUPA6',\n",
    "      'HUPA7','RP1','RP2', 'RP3','RP4','MSA','ADI','DMA','IC1','IC2','IC3','IC4','IC5','IC6','IC7','IC8','IC9','IC10','IC11','IC12','IC13','IC14','IC15','IC16','IC17','IC18','IC19','IC20','IC21','IC22','IC23','HHAS1','HHAS2','HHAS3','HHAS4','MC1','MC2','MC3',\n",
    "      'TPE1','TPE2','TPE3','TPE4','TPE5','TPE6','TPE7','TPE8','TPE9','PEC1','PEC2','TPE10','TPE11','TPE12','TPE13','LFC1','LFC2','LFC3','LFC4','LFC5','LFC6','LFC7','LFC8','LFC9','LFC10','OCC1','OCC2','OCC3','OCC4','OCC5','OCC6','OCC7','OCC8','OCC9',\n",
    "      'OCC10','OCC11','OCC12','OCC13','EIC1','EIC2','EIC3','EIC4','EIC5','EIC6','EIC7','EIC8','EIC9','EIC10','EIC11','EIC12','EIC13','EIC14','EIC15',\n",
    "      'EIC16','OEDC1','OEDC2','OEDC3','OEDC4','OEDC5','OEDC6','OEDC7','EC1','EC2','EC3',\n",
    "      'EC4','EC5','EC6','EC7','EC8','SEC1','SEC2','SEC3','SEC4','SEC5','AFC1','AFC2','AFC3','AFC4','AFC5','AFC6','VC1','VC2','VC3','VC4','ANC1','ANC2','ANC3','ANC4','ANC5','ANC6','ANC7','ANC8','ANC9','ANC10','ANC11','ANC12','ANC13','ANC14','ANC15','POBC1','POBC2','LSC1','LSC2','LSC3','LSC4',\n",
    "      'VOC1','VOC2', 'VOC3','HC1','HC2','HC3', 'HC4','HC5','HC6','HC7','HC8','HC9','HC10','HC11','HC12','HC13','HC14','HC15','HC16',\n",
    "      'HC17','HC18','HC19','HC20','HC21','MHUC1','MHUC2','AC1','AC2','ADATE_2','ADATE_3','ADATE_4','ADATE_5','ADATE_6','ADATE_7','ADATE_8','ADATE_9','ADATE_10','ADATE_11','ADATE_12','ADATE_13','ADATE_14','ADATE_15','ADATE_16','ADATE_17','ADATE_18', 'ADATE_19','ADATE_20','ADATE_21','ADATE_22','ADATE_23',\n",
    "      'ADATE_24','RFA_2','RFA_3','RFA_4','RFA_5','RFA_6','RFA_7','RFA_8','RFA_9','RFA_10','RFA_11','RFA_12','RFA_13','RFA_14','RFA_15','RFA_16','RFA_17','RFA_18','RFA_19','RFA_20','RFA_21','RFA_22','RFA_23','RFA_24','CARDPROM','MAXADATE','NUMPROM',\n",
    "      'CARDPM12', 'NUMPRM12','RDATE_3','RDATE_4','RDATE_5','RDATE_6','RDATE_7','RDATE_8','RDATE_9','RDATE_10','RDATE_11',\n",
    "      'RDATE_12','RDATE_13','RDATE_14','RDATE_15','RDATE_16','RDATE_17','RDATE_18','RDATE_19','RDATE_20','RDATE_21','RDATE_22','RDATE_23','RDATE_24','RAMNT_3', 'RAMNT_4','RAMNT_5','RAMNT_6', 'RAMNT_7','RAMNT_8','RAMNT_9', 'RAMNT_10',\n",
    "      'RAMNT_11','RAMNT_12','RAMNT_13','RAMNT_14','RAMNT_15','RAMNT_16','RAMNT_17','RAMNT_18','RAMNT_19','RAMNT_20','RAMNT_21',\n",
    "      'RAMNT_22','RAMNT_23','RAMNT_24','RAMNTALL','NGIFTALL','CARDGIFT','MINRAMNT','MINRDATE', 'MAXRAMNT','MAXRDATE','LASTGIFT','LASTDATE','FISTDATE','NEXTDATE','TIMELAG','AVGGIFT','CONTROLN', 'HPHONE_D','RFA_2R','RFA_2F','RFA_2A','MDMAUD_R','MDMAUD_F','MDMAUD_A','CLUSTER2','GEOCODE2']\n",
    "    \n",
    "    for i in xrange(len(data)):\n",
    "        df[data[i]] = pd.Categorical((pd.factorize(df[data[i]])[0] + 1).astype(str))\n",
    "    \n",
    "    return df[data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_split():\n",
    "    df = data_preperation()\n",
    "    train, test = train_test_split(df, test_size = 0.5)  # split data to 50-50 cross validate \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_split()\n",
    "rewards = ['TARGET_D']\n",
    "for i in xrange(len(rewards)):\n",
    "    train[rewards[i]] = pd.Categorical((pd.factorize(train[rewards[i]])[0] + 1).astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current State variables ..\n",
    "Recency, Frequancy, Montery variables .. ['RFA_2R', 'RFA_2F', 'RFA_2A']\n",
    "\n",
    "## Rewards:\n",
    "- Donation amount \n",
    "- These are the target variables as well .. \n",
    "- TARGET_D, TARGET_B \n",
    "\n",
    "\n",
    "## Actions: \n",
    "- 11 mailing type\n",
    "\n",
    "Actions mapping .. \n",
    "https://github.com/EAboelhamd/kdd98-1/tree/master/notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States:\n",
    "\n",
    "In the cited paper .. they defined the states as follows: \n",
    "\n",
    "It is a 5-dimensional vector consisting of \n",
    "\n",
    "(1) how recently the donor donated last (R)\n",
    "\n",
    "(2) how frequently she donates (F)\n",
    "\n",
    "(3) her average donation amount (M)\n",
    "\n",
    "(4) how many times PVA sends her a mail in the last six months\n",
    "\n",
    "(5) how many times PVA has sent her mails.\n",
    "\n",
    "\n",
    "This implementation is considered as a POMPD .. \n",
    "where:\n",
    "b: belief state that is a probability distribution over all states\n",
    "b(s): prob. that the agent in state s \n",
    "after taking action a and observing the state O .. the update rule for the belief state o(s) is using Bayes rule .. \n",
    "\n",
    "## Next States:\n",
    "\n",
    "As mentioned in the paper ==> the 5-dimensional observation is discrete in this problem, and individual dimensions evolve\n",
    "independently of each other. We therefore build an observation probability table for each observation dimension,\n",
    "and the sample next observations using these tables.\n",
    "\n",
    "Hence, Let's create an n*5 dim observations table .. \n",
    "\n",
    "http://www-anw.cs.umass.edu/~barto/courses/cs687/PartialObs-printable.pdf\n",
    "\n",
    "\n",
    "### These researchers proposed other types of state space ==>\n",
    "\n",
    "https://www.cs.cmu.edu/~ebrun/15889e/hw1.pdf\n",
    "\n",
    "as 9 vars (try it out) ;) .. \n",
    "and others consider (belief, observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_():\n",
    "    train, test = df_split()\n",
    "    \n",
    "    ## current state ..\n",
    "    RFA = ['RFA_2R', 'RFA_2F', 'RFA_2A', 'CARDPROM', 'NUMPRM12']  # don't know if it is NUMPRM12 or CARDPM12\n",
    "#     train[RFA].head()\n",
    "\n",
    "    ## next state\n",
    "    ## next_states are random selection from the current states\n",
    "    next_states = [[random.random() for e in train[RFA].values[0]] for e in xrange(len(train[RFA].values))]\n",
    "    next_states = pd.DataFrame(next_states)\n",
    "    \n",
    "\n",
    "    rewards = ['TARGET_D']\n",
    "    \n",
    "    for i in xrange(len(rewards)):\n",
    "        train[rewards[i]] = pd.Categorical((pd.factorize(train[rewards[i]])[0] + 1).astype(str))\n",
    "    \n",
    "    ## actions\n",
    "    actions = ['RDATE_3','RDATE_4','RDATE_5','RDATE_6','RDATE_7','RDATE_8','RDATE_9','RDATE_10','RDATE_11','RDATE_12','RDATE_13','RDATE_14',\n",
    "      'RDATE_15','RDATE_16','RDATE_17','RDATE_18','RDATE_19','RDATE_20','RDATE_21','RDATE_22','RDATE_23','RDATE_24']\n",
    "#     train[actions].head()\n",
    "\n",
    "    current_states = train[RFA]\n",
    "    # next_states = train[RFA_]\n",
    "\n",
    "#     train['TARGET_D']  ## rewards \n",
    "\n",
    "    train[actions]  ## actions .. distinct actions are 12\n",
    "\n",
    "    curr_state_current_action = pd.DataFrame(np.column_stack((current_states, train[actions])))\n",
    "\n",
    "    # next actions \n",
    "    next_actions = np.zeros([len(next_states), max(train[actions].values.max(axis = 0).astype(int)) + 1])\n",
    "    next_actions = np.delete(next_actions, -1, axis=1) ## remove last column .. \n",
    "\n",
    "    next_state_next_action = np.zeros([np.shape(next_states)[0], np.shape(next_states)[1] + max(train[actions].values.max(axis = 0).astype(int)) ])\n",
    "\n",
    "    # fill in next_actions  \n",
    "    for i in xrange(np.shape(next_actions)[1]):\n",
    "        next_actions[:, i] = i\n",
    "\n",
    "    Mxa0 = Mxa1 = Mxa2 = Mxa3 = Mxa4 = Mxa5 = Mxa6 =  []\n",
    "\n",
    "    # next_state_next_action\n",
    "    Mxa0 = pd.DataFrame(np.column_stack((next_states, next_actions[:,0])))\n",
    "    Mxa1 = pd.DataFrame(np.column_stack((next_states, next_actions[:,1])))\n",
    "    Mxa2 = pd.DataFrame(np.column_stack((next_states, next_actions[:,2])))\n",
    "    Mxa3 = pd.DataFrame(np.column_stack((next_states, next_actions[:,3])))\n",
    "    Mxa4 = pd.DataFrame(np.column_stack((next_states, next_actions[:,4])))\n",
    "    Mxa5 = pd.DataFrame(np.column_stack((next_states, next_actions[:,5])))\n",
    "    Mxa6 = pd.DataFrame(np.column_stack((next_states, next_actions[:,6])))\n",
    "    # next_state_next_action7 = pd.DataFrame(np.column_stack((next_states, next_actions[:,7])))\n",
    "\n",
    "    tuplesMx = pd.DataFrame(np.column_stack((Mxa0, Mxa1, Mxa2, Mxa3, Mxa4, Mxa5, Mxa6, curr_state_current_action, train[rewards])))\n",
    "    return tuplesMx, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression phase:\n",
    "\n",
    "Before performing the prediction task .. let's split the data to training and validation sets .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid any problems in prediction by having string variables .. let's binarize (catergorize) all the variables .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "Num_itrs = 10  # no loop 3leha ( w dah el sa7) .. we just have to run the whole algo. 10 times and report the avg. results\n",
    "\n",
    "num_epoch = 23 #epochs are cycles of Feedforward and Backprob\n",
    "## el mafrood yeb2a feh loop 3la el epochs elli heyya el steps .. w avg. reward per step is calculated \n",
    "batch_size = 5\n",
    "chunkSize = 3\n",
    "\n",
    "n_nodes_hl1 = np.shape(train)[0]\n",
    "n_nodes_hl2 = np.shape(train)[0]\n",
    "NUM_STATES = np.shape(train)[1]\n",
    "NUM_DIM =  np.shape(train)[1]\n",
    "num_nodes = np.shape(train)[0]\n",
    "num_unrollings = 5\n",
    "\n",
    "best_actions = np.zeros([np.shape(train)[0], batch_size])\n",
    "Q_optimal = [] #np.zeros([np.shape(curr_state_current_action)[0], len(df['ACCOUNT_STATUS'].unique())])\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "## for training \n",
    "x = tf.placeholder(tf.float32, shape=[NUM_DIM, num_nodes])\n",
    "y =  tf.placeholder(tf.float32, shape=[num_nodes, 1])\n",
    "\n",
    "## for testing\n",
    "x_ = tf.placeholder(tf.float32, shape=[np.shape(test)[0], NUM_DIM])\n",
    "y_ =  tf.placeholder(tf.float32, shape=[np.shape(test)[0], 1])\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "# Input gate: input, previous output, and bias.\n",
    "ix = tf.Variable(tf.truncated_normal([NUM_DIM, num_nodes], 0, 1, dtype = tf.float32))# init_weights_RNN([n_nodes_hl1, NUM_ACTIONS])\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], 0, 1, dtype = tf.float32))\n",
    "ib = tf.Variable(tf.zeros([1, num_nodes], dtype = tf.float32))\n",
    "\n",
    "# Forget gate: input, previous output, and bias.\n",
    "fx = tf.Variable(tf.truncated_normal([NUM_DIM, num_nodes], 0, 1, dtype = tf.float32))\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], 0, 1, dtype = tf.float32))\n",
    "fb = tf.Variable(tf.zeros([1, num_nodes], dtype = tf.float32))\n",
    "\n",
    "# Memory cell: input, state and bias.                             \n",
    "cx = tf.Variable(tf.truncated_normal([NUM_DIM, num_nodes], 0, 1, dtype = tf.float32))\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], 0, 1, dtype = tf.float32))\n",
    "cb = tf.Variable(tf.zeros([1, num_nodes], dtype = tf.float32))\n",
    "\n",
    "# Output gate: input, previous output, and bias.\n",
    "ox = tf.Variable(tf.truncated_normal([NUM_DIM, num_nodes], 0, 1, dtype = tf.float32))\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], 0, 1, dtype = tf.float32))\n",
    "ob = tf.Variable(tf.zeros([1, num_nodes], dtype = tf.float32))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([1, num_nodes], dtype = tf.float32), trainable=False) #reversed\n",
    "\n",
    "saved_state = tf.Variable(tf.zeros([1, num_nodes], dtype = tf.float32), trainable=False) #reversed\n",
    "\n",
    "# Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], 0, 1, dtype = tf.float32))\n",
    "b = tf.Variable(tf.zeros([num_nodes], dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the cell computation.\n",
    "# this method takes single cell and returns single number \n",
    "def lstm_cell(i, o, state):\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(train, saved_output, saved_state):\n",
    "    # Unrolled LSTM loop.    \n",
    "    outputs = list()\n",
    "    output = saved_output  # row !\n",
    "    state = saved_state  # row !\n",
    "    \n",
    "    # astype('U') .. to convert numpy array to string ..\n",
    "    for i in xrange(np.shape(train)[0]):## el loop faydetha to copy the next line that is just for single unit \n",
    "        output_, state = lstm_cell(tf.string_to_number(train.values[i, None].astype('U')), tf.cast(output, tf.float32), tf.cast(state, tf.float32)) \n",
    "\n",
    "    ## in case the last values are saved !\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        model_output = tf.matmul(output_, w) + b # outputs single value\n",
    "        \n",
    "    return model_output  ## the output for the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model():\n",
    "    loss_RNN = []\n",
    "    model_output = lstm_model(train, saved_output, saved_state) #output here is a vector\n",
    "    model_output = tf.transpose(model_output)\n",
    "    cost = tf.reduce_mean(tf.square(y - model_output))\n",
    "    optimize = tf.train.GradientDescentOptimizer(0.01).minimize(cost) \n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()  ## updated version from initialize_all_variables :) \n",
    "    sess.run(init)\n",
    "\n",
    "    predicted_reward = sess.run(model_output, feed_dict={x:np.transpose(train.values), y:train[rewards]})\n",
    "    \n",
    "    # Cost calculation\n",
    "    for step in xrange(1000):\n",
    "        l,_ = sess.run([cost, optimize], feed_dict={x:np.transpose(train.values), y:train[rewards]})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            loss_RNN.append(l)\n",
    "\n",
    "    Y_pred = tf.zeros([len(test[rewards]),1])\n",
    "\n",
    "    # predict \n",
    "    test_pred2 = sess.run(Y_pred, feed_dict={x_: test.values})\n",
    "\n",
    "   ## rms to test ..\n",
    "    cost_test = tf.reduce_mean(tf.square(test[rewards].values.astype(np.float32) - test_pred2))\n",
    "    \n",
    "    \n",
    "    rmse_val = sess.run(cost_test, feed_dict={x_:test.values.astype(np.float32), y_: test_pred2})\n",
    "     \n",
    "    sess.close()\n",
    "    \n",
    "    return loss_RNN, predicted_reward #saved_state, saved_output, cost_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_prediction(train):\n",
    "    loss_RNN, predicted_reward = train_lstm_model()\n",
    "    plt.plot(loss_RNN)\n",
    "    plt.xlabel('Step number')\n",
    "    plt.ylabel('Prediction Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHyRJREFUeJzt3Xl0HGeZ7/Hv061dliW1Lce2vKgT\ngxMncbxIckKASUjYcyFwGQhcAoE7k2FNwnrCXDgwcxjmkgskwEAYk42BwGQICSGESVjDwACx5AWv\ncRy8y5u8yXJkrf3cP7ply7Ykt5dSdXf9Puf06eqq6qrHfaxfVb9d9b7m7oiISOGLhV2AiIiMDQW+\niEhEKPBFRCJCgS8iEhEKfBGRiFDgi4hEhAJfRCQiFPgiIhGhwBcRiYiisAsYauLEid7Q0BB2GSIi\neWPp0qV73b0um3VzKvAbGhpobW0NuwwRkbxhZluyXVdNOiIiEaHAFxGJCAW+iEhEKPBFRCJCgS8i\nEhEKfBGRiFDgi4hERN4Hfk//AN/67V/43Yb2sEsREclpeR/4JfEY3/6vjfx4+Y6wSxERyWl5H/hm\nRlNDgpbN+8MuRUQkp+V94AM0JRNs3d/Fro7usEsREclZBRH4zQ0JAJboLF9EZEQFEfgXTaliXGkR\nSzbtC7sUEZGcVRCBXxSPsWBmLS2bDoRdiohIziqIwAdYlEywfncnB7t6wy5FRCQnFUzgN2Xa8Vs2\n6yxfRGQ4BRP4c6dVUxKP6fJMEZERFEzglxXHmTe9hmc2KfBFRIZTMIEP0JSsZU1bBy/09IddiohI\nzimowG9OTqA/5SzfejDsUkREck5BBf6CGTXETDdgiYgMp6ACv6qsmDlTx9OidnwRkZMUVOADNDdM\nYNnWA/T2p8IuRUQkpxRe4Cdr6elPsaqtI+xSRERySsEF/uANWEvUrCMicpyCC/wJ40q5oK5SN2CJ\niJyg4AIfoDmZHhBlIOVhlyIikjMKNvA7u/tZv6sz7FJERHJGoIFvZh8xszVmttrMfmBmZUHub9Cx\njtTUrCMiMiiwwDezeuAWoNHdLwHiwA1B7W+oabUV1NeU64dbEZEhgm7SKQLKzawIqAB2BLy/o5oa\nalmyeT/uascXEYEAA9/d24AvAVuBnUCHu/88qP2dqCmZoL2zhy37usZqlyIiOS3IJp1a4I1AEpgK\nVJrZO4dZ72YzazWz1vb29nO2/0VJXY8vIjJUkE061wKb3L3d3fuAR4CXnLiSuy9290Z3b6yrqztn\nO7+gbhyJyhJ1pCYikhFk4G8FLjezCjMz4BpgXYD7O46ZpdvxdYYvIgIE24b/DPAwsAxYldnX4qD2\nN5ymhgRb93ex+1D3WO5WRCQnBXqVjrt/1t0vdPdL3P1Gd+8Jcn8nWpScAKgdX0QECvRO20EXTami\nsiSuwBcRocADvygeY8HMWt1xKyJCgQc+pC/PfHZXJwe7esMuRUQkVAUf+IP96rRuPhByJSIi4Sr4\nwL9seg0l8ZiuxxeRyCv4wC8rjnPZ9Gr9cCsikVfwgQ/pZp3VbR109faHXYqISGgiEfjNyQT9KWf5\n1oNhlyIiEppIBP7CmbXETDdgiUi0RSLwq8qKmTN1vAJfRCItEoEP6Xb85dsO0NufCrsUEZFQRCbw\nmxsSdPelWL2jI+xSRERCEZnAb9KAKCIScZEJ/InjSjm/rpIWBb6IRFRkAh/S/eq0bN5PKqWBzUUk\neiIV+E0NCQ5197N+d2fYpYiIjLnIBT6g7pJFJJIiFfjTasuZWl3GM2rHF5EIilTgmxlNyQQtm/bj\nrnZ8EYmWSAU+pPvV2dPZw5Z9XWGXIiIypqIX+Jl2fPWPLyJRE7nAnzVpHInKEl2PLyKRE7nANzMa\nZ9bqDF9EIidygQ/pdvwt+7rYfag77FJERMZMZAMf1K+OiERLJAN/zpTxVJbEdQOWiERKJAO/KB5j\nwcxaneGLSKREMvAhfXnm+t2ddHT1hV2KiMiYiGzgNyUTuEPrFp3li0g0RDbw502voSQeU7OOiERG\nZAO/rDjO3GnVuh5fRCIjsoEP6cszV23voKu3P+xSREQCF+nAb0om6E85K7YeDLsUEZHARTrwF86s\nxUwdqYlINJwy8M3sr82sKjP9aTN7xMwWBF9a8MaXFTNnynj9cCsikZDNGf5n3L3TzF4KXAvcC9yd\nzcbNrMbMHjazZ81snZldcTbFBqGpIcGyrQfo7U+FXYqISKCyCfyBzPPrgcXu/gRQkuX2vwo86e4X\nApcB606/xGAtSibo7kuxekdH2KWIiAQqm8BvM7N/Bd4G/MzMSrN5n5lVAy8n/Y0Ad+9195z7dbRx\ncGBzNeuISIHLJvDfCjwFvDoT2AngE1m8Lwm0A/eb2XIzu8fMKk9cycxuNrNWM2ttb28/ndrPibqq\nUs6fWKmO1ESk4GUT+FOAJ9x9g5ldBfw1sCSL9xUBC4C73X0+8AJw+4kruftid29098a6urrsKz+H\nmpMJWjYfIJXSwOYiUriyCfwfAQNmNgtYDEwHvp/F+7YD2939mczrh0kfAHJOU0OCjiN9PLenM+xS\nREQCk03gp9y9H3gz8HV3/wTps/5RufsuYJuZzc7MugZYe8aVBkgDoohIFGQT+H1m9nbgXcBPM/OK\ns9z+h4EHzWwlMA/4wumXGLxpteVMqS5T4ItIQSvKYp33AO8D/sndN5lZEvhuNht39xVA41nUNybM\njOZkgj9t3Ie7Y2ZhlyQics6d8gzf3dcCHwdWmdklpNvlvxh4ZWOsqSHB7kM9bN3fFXYpIiKByOZ6\n+quADcA3gG8Cz5nZywOua8ypHV9ECl02bfhfBl7l7n/l7i8HXg3cGWxZY29W3ThqK4oV+CJSsLIJ\n/GJ3Xz/4wt2fI/sfbfNGLGY0NiR0A5aIFKxsAr81c5fsVZnHt4HWoAsLw6Jkgs37uthzqDvsUkRE\nzrlsAv/9pK+fvyXzWEv6qp2C05TpV0f944tIIcrmKp0ed/+Ku78587iTLC/LzDcXTx1PRUlcHamJ\nSEE60xGvcq5f+3OhKB5j4cxanlHgi0gBivQQh8NpakiwfncnHV19YZciInJOjXin7SjDGBoFeJXO\noOZkAndo3bKfay46L+xyRETOmdG6VvjyKMuePdeF5Ip502sojhtLNivwRaSwjBj47n71WBaSK8qK\n48ydVqMfbkWk4KgNfxjNyQQrt3dwpHfg1CuLiOQJBf4wmhsS9Kec5dsOhF2KiMg5o8AfxsKGWszU\nkZqIFJZs+sPHzOqBmUPXd/f/CqqosI0vK+aiyePVr46IFJRTBr6ZfRF4G+kuFQYbtR0o2MCHdDv+\nQy3b6BtIURzXFyERyX/ZnOFfD8x2956gi8klzckED/xhM6vbOpg/ozbsckREzlo2p64bKeAbrUYy\n2JGamnVEpFBkc4bfBawws18BR8/y3f2WwKrKAXVVpZw/sZIlm/Zz88svCLscEZGzlk3g/yTziJym\nhgRPrtlFKuXEYhrYXETy2ykD392/Y2YlwIszs9a7eyR6FmtKJniodRsb9hxm9uSqsMsRETkrGsR8\nFIuODmy+L+RKRETOngYxH8W02nImjy9jyWbdcSsi+U+DmI/CzGhOJliyaR/uHnY5IiJnRYOYn0JT\nMsHuQz1s238k7FJERM7KmQ5i/v4gi8olR9vxdT2+iOS5bK7S6QG+knlEzqy6cdRUFLNk0z7esnBa\n2OWIiJyx0YY4/A93f6uZrSLdd85x3H1uoJXliFjMaJyZoEU/3IpInhvtDP/WzPN1Y1FILluUTPDL\ndbvZ09nNpKqysMsRETkjI7bhu/vOzOQH3H3L0AfwgbEpLzc0ZdrxWzbpLF9E8lc2P9q+cph5rz3X\nheSyi6eOp6IkrhuwRCSvjdaG/37SZ/IXmNnKIYuqgD8EXVguKY7HWDCjVjdgiUheG60N//vAfwL/\nDNw+ZH6nu0fuGsWmhgR3/eo5Oo70UV0eifvORKTAjNaG3+Hum4GvAvuHtN/3m9mibHdgZnEzW25m\nPz37csPTnEzgDku3RO5YJyIFIps2/LuBw0NeH87My9atwLrTKSoXzZ9RQ3HcWKIfbkUkT2UT+OZD\nOpJx9xTZD34+DXg9cM+ZlZc7yorjzJ1Wox9uRSRvZTXEoZndYmbFmcetpIc9zMZdwCeB1BlXmEOa\nGhKsauugu2/g1CuLiOSYbAL/fcBLgDZgO7AIuPlUbzKz64A97r70FOvdbGatZtba3t6eRTnhaU7W\n0jfgLN96MOxSRERO2ykD3933uPsN7j7J3c9z93e4+54stn0l8AYz2wz8O/AKM/veMNtf7O6N7t5Y\nV1d32v+AsbRwZgIzWLJJP9yKSP4Z7Tr8T7r7HWb2dYbvS2fUQczd/VPApzLbugr4uLu/8+zKDVd1\neTEXTh5Pi3rOFJE8NNqPr4NX1kSm7/tsLEomeKhlG30DKYrj2bSIiYjkhhED390fzzx/52x34u5P\nA0+f7XZyQVNDggf+sJk1Ow4xb3pN2OWIiGRttCadxxmmKWeQu78hkIpyXFOyFoCWTfsV+CKSV0Zr\nk/gS6QHMNwFHgG9nHoeBvwRfWm6aVFVGcmIlz+iHWxHJM6M16fwWwMy+7O6NQxY9bmaRbtdvaqjl\n52t3k0o5sZiFXY6ISFay+dWx0szOH3xhZkmgMriScl9zcgIHu/rYsOfwqVcWEckR2XSR8BHgaTPb\nCBgwE/i7QKvKcc0NxwY2nz25KuRqRESyk80g5k+a2YuACzOzns0MbB5Z0xPlTB5fRsum/dx4+cyw\nyxERycopm3TMrAL4BPAhd/8zMCPTbUJkmRlNyQRLNu1nSL9yIiI5LZs2/PuBXuCKzOs24POBVZQn\nmhtq2XWom+0HjoRdiohIVrIJ/Avc/Q6gD8Ddu0i35Udac3ICgC7PFJG8kU3g95pZOZmbsMzsAiDS\nbfgAL5o0juryYloU+CKSJ7K5SuezwJPAdDN7kHQvmDcFWVQ+iMWMpoYES9SRmojkiVED38wMeBZ4\nM3A56aacW9197xjUlvOak7X8ct1u9nR2M6mqLOxyRERGNWqTTmZow5+5+z53f8Ldf6qwP6Ypcz1+\n62aNcysiuS+bNvxlZtYUeCV56JL6asqL4xoQRUTyQjZt+IuAd2ZGrnqBdLOOu/vcIAvLB8XxGAtm\n1ijwRSQvZBP4rw68ijzW3DCBu371HB1H+qguLw67HBGREY3WH34Z6QHMZwGrgHvdvX+sCssXTcla\n3GHZlgNcfeGksMsRERnRaG343wEaSYf9a0n3jS8nmD+9luK46fJMEcl5ozXpzHH3SwHM7F5gydiU\nlF/KS+JcWl+tdnwRyXmjneH3DU6oKWd0TckEK7cfpLtvIOxSRERGNFrgX2ZmhzKPTmDu4LSZHRqr\nAvPBomSCvgFn+daDYZciIjKiEQPf3ePuPj7zqHL3oiHT48eyyFy3cGYCM2hRO76I5LBsbrySU6gu\nL2b2eVUKfBHJaQr8c2RRMsHSLQfoH0iFXYqIyLAU+OdIUzJBV+8Aq9o6wi5FRGRYCvxz5IrzJ1BR\nEuczj63mhR5d1CQiuUeBf45MGFfKN96xgLU7DnHLD5YzkNJYtyKSWxT459DVF07iH95wMb96dg//\n+PgaDXAuIjklm87T5DTceEUDW/d38e3fbWLGhEr+90uTYZckIgIo8APxqddexPYDR/j8E2uprynn\nNZdMDrskERE16QQhFjPufNs8LptWw20PLWfFNt2BKyLhU+AHpKw4zj3vbqSuqpS/+U4L2/Z3hV2S\niEScAj9AE8eVcv9NzfQNOO95oIWOrr5Tv0lEJCAK/IDNmjSOf71xIVv2vcD7vreU3n7diSsi4VDg\nj4HLz5/AHW+Zyx837uP2R1bqck0RCUVggW9m083sN2a21szWmNmtQe0rH7xp/jQ++soX88iyNr76\nqw1hlyMiERTkZZn9wMfcfZmZVQFLzewX7r42wH3mtA+/YhZb93dx1y83ML22gv+5cFrYJYlIhAR2\nhu/uO919WWa6E1gH1Ae1v3xgZnzhTZfykgsmcPsjK/njX/aFXZKIRMiYtOGbWQMwH3hmmGU3m1mr\nmbW2t7ePRTmhKimKcfc7F9IwoZK/+24rz+/pDLskEYmIwAPfzMYBPwJuc/eThkZ098Xu3ujujXV1\ndUGXkxOqy4u576YmSori3HR/C+2dPWGXJCIREGjgm1kx6bB/0N0fCXJf+WZ6ooJ7393I3sM9/M2/\ntXKkVwOgi0iwgrxKx4B7gXXu/pWg9pPPLptew9dumM/K7Qe57SF1qSwiwQryDP9K4EbgFWa2IvN4\nXYD7y0uvungyn3n9HJ5as5t//tm6sMsRkQIW2GWZ7v57wILafiF570uTbN3fxT2/38SMCRW864qG\nsEsSkQKk7pFzxGeum8P2A0f43E/WUF9TzjUXnRd2SSJSYNS1Qo6Ix4yvvX0eF0+t5kPfX85qDYYu\nIueYAj+HVJQUce9NjSQqS3jvAy20HTwSdkkiUkAU+DlmUlUZ97+niSO9A7z3/hYOdatLZRE5NxT4\nOejF51XxrRsX8pf2w3zwwWX0DahLZRE5ewr8HHXlrIl84c2X8rsNe/n0o6vVpbKInDVdpZPD3to4\nnW37u/j6r59nxoQKPnj1rLBLEpE8psDPcR995YvZur+L//fUeqbVlvPGeZHucFREzoICP8eZGXe8\nZS47O7r5xA9XMrWmnKaGRNhliUgeUht+HigtirP4xoVMS5Tzt//Wysb2w2GXJCJ5SIGfJ2oqSrj/\npiZiZrzngRb2HVaXyiJyehT4eWTmhEq+/a5GdnV0c/N3l9Ldpy6VRSR7Cvw8s3BmLXe+bR5Ltxzg\nYz/8Myl1qSwiWVLg56HXXTqFv3/dhTyxcid3PLU+7HJEJE/oKp089bcvO58t+7r41m//woxEBe9Y\nNCPskkQkxynw85SZ8Q9vuJi2g0f4zGOrmVpTxlWzJ4VdlojkMDXp5LGieIx/eccCZp9XxYe+v5y1\nO04aI15E5CgFfp4bV1rEfTc1Ma60iPc+0MKuju6wSxKRHKXALwCTq8u476YmOrv7eO8DLRzu6Q+7\nJBHJQQr8AjFn6ni+8b8WsH53Jx94cBk7NHiKiJxAgV9Arpo9ic9ffwm/29DOlV/8NTcs/iMPtWyl\n44gGURERsFzqZ72xsdFbW1vDLiPvbd3XxWMr2nh0eRsb975ASVGMay+axBvn1XPV7DpKi+Jhlygi\n54iZLXX3xqzWVeAXLndnVVsHjy5v4/E/72Dv4V6qy4t5/dwpvGl+PQtn1BKLWdhlishZUODLSfoH\nUvz++b38eHkbT63ZzZG+AabVlnP9vHqun1/PrEnjwi5RRM6AAl9G9UJPPz9fu4tHl+/g9xvaSTlc\nWl/N9fPr+R+XTWFSVVnYJYpIlhT4krU9nd08/uedPLaijZXbO4gZvPRFdbxp/lReNWcylaW6GVsk\nlynw5Yw8v+fw0R97tx84QnlxnFddfB7Xz6/nZbMmUhTXRV0iuUaBL2fF3Vm65QCPLm/jpyt30nGk\nj4njSrhu7lTeNL+eudOqMdOPvSK5QIEv50xvf4qn1+/hxyva+OW6PfT2pzh/YiXXz6/n+nn1zJhQ\nEXaJIpGmwJdAdBzp48nVO3l0eRt/2rgfSA/Icv38eq67dAq1lSUhVygSPQp8CVzbwSP8ZMUOHl2+\nned2H6YoZlw1u47r59dz7UXnUVasm7tExoICX8aMu7NuZyePrWjjxyva2H2oh3GlRbz2ksm8+uLJ\nzJhQwZTqMqrKisMuVaQgKfAlFAMp55mN+3h0eRv/uXrXcb12jistYkp1GZOry5haXZ5+riljSnU5\nU6rLmFJTzjhdAipy2hT4ErruvgFWt3Wwo6ObXR1H2HGwm50dR9jV0c2Ojm72Hu7hxP96VaVFTKkp\nY3J1OVNHODjovgCR451O4Af612NmrwG+CsSBe9z9/wa5P8kdZcVxGhsSIy7v7U+x+1A3uw51s+Ng\n+kCwsyN9UNjZ0c26nYdo7+w56X1VZUXHHQQmjy9nSk1Z+ltC5tuCDgoiwwvsL8PM4sA3gFcC24EW\nM/uJu68Nap+SP0qKYkxPVDA9MfJlnYMHhaEHgp0HM88d3azZcYi9h08+KIwvK0qHf+ZbQaKymLKi\nOGXFccqKY5QWZ6aLYpQVxynNPA8uTy+LU1oco7QopnsOpGAEeSrUDDzv7hsBzOzfgTcCCnzJSjYH\nhZ7+AfYc6kl/SzjUfbTpaPAgsbqtg/0v9JI6w5ZLMygtilFadPzB4MQDR+mQA8jR9YYcTEqLYpQU\nxYiZETMjHiPznH4dixlxM2InzE8/H5t34usT3xu3IfMy8804Oq3eUaMtyMCvB7YNeb0dWBTg/iSC\nSovipzwoAPQNpOjuG6C7L/3c0z90+vhl3UOX9Q3Q3Z9KP/elMsuOLe840seevoGTt9M/cNJvFLnC\nDOzotGFH56UX2HHr2dH1B9dNLxwyb8h0ZhHpyaHLBt928vqj1Tnc9OB2Rlp3sIZjy2zEZcPPGKWm\n7Fcddt8jSVSU8B/vu+I0t376Qm/sNLObgZsBZsyYEXI1UqiK4zGK4zHGqiNQd6d3IEV337GDRV8q\nhbszkEpf0ZTy9GNweiBFel7KGcjMd0+vO5CZn3KOTh+/jfR8z7zv2LLMvoZsw4/WCI5nno+9Ti8c\nnHf88vQiPzbtPuyyY/OGrnv88uM/rxNeD13jpGUnf9YjLT95u6O/dzSnffw+jTdUlY1NFAe5lzZg\n+pDX0zLzjuPui4HFkL5KJ8B6RMaMmVFaFE+PLlauexAkNwTZ/WEL8CIzS5pZCXAD8JMA9yciIqMI\n7Azf3fvN7EPAU6Qvy7zP3dcEtT8RERldoA1H7v4z4GdB7kNERLKjES1ERCJCgS8iEhEKfBGRiFDg\ni4hEhAJfRCQicqp7ZDNrB7ac4dsnAnvPYTn5TJ/F8fR5HE+fxzGF8FnMdPe6bFbMqcA/G2bWmm2f\n0IVOn8Xx9HkcT5/HMVH7LNSkIyISEQp8EZGIKKTAXxx2ATlEn8Xx9HkcT5/HMZH6LAqmDV9EREZX\nSGf4IiIyirwPfDN7jZmtN7Pnzez2sOsJk5lNN7PfmNlaM1tjZreGXVPYzCxuZsvN7Kdh1xI2M6sx\ns4fN7FkzW2dmwQ+xlMPM7COZv5PVZvYDMxuj4XHCk9eBP2Sg9NcCc4C3m9mccKsKVT/wMXefA1wO\nfDDinwfArcC6sIvIEV8FnnT3C4HLiPDnYmb1wC1Ao7tfQroL9xvCrSp4eR34DBko3d17gcGB0iPJ\n3Xe6+7LMdCfpP+j6cKsKj5lNA14P3BN2LWEzs2rg5cC9AO7e6+4Hw60qdEVAuZkVARXAjpDrCVy+\nB/5wA6VHNuCGMrMGYD7wTLiVhOou4JNAKuxCckASaAfuzzRx3WNmlWEXFRZ3bwO+BGwFdgId7v7z\ncKsKXr4HvgzDzMYBPwJuc/dDYdcTBjO7Dtjj7kvDriVHFAELgLvdfT7wAhDZ37zMrJZ0a0ASmApU\nmtk7w60qePke+FkNlB4lZlZMOuwfdPdHwq4nRFcCbzCzzaSb+l5hZt8Lt6RQbQe2u/vgN76HSR8A\noupaYJO7t7t7H/AI8JKQawpcvge+BkofwsyMdBvtOnf/Stj1hMndP+Xu09y9gfT/i1+7e8GfwY3E\n3XcB28xsdmbWNcDaEEsK21bgcjOryPzdXEMEfsQOdEzboGmg9JNcCdwIrDKzFZl5f58ZW1jkw8CD\nmZOjjcB7Qq4nNO7+jJk9DCwjfXXbciJw163utBURiYh8b9IREZEsKfBFRCJCgS8iEhEKfBGRiFDg\ni4hEhAJf8oaZ/Z9M74YrzWyFmS3KzL/NzCrCrm80ZtZgZqvDrkOiLa+vw5foyHTlex2wwN17zGwi\nUJJZfBvwPaArrPqCZmZF7t4fdh2S33SGL/liCrDX3XsA3H2vu+8ws1tI94XyGzP7DYCZvcrM/mhm\ny8zsh5m+hTCzzWZ2h5mtMrMlZjbrxJ2Y2efM7D4ze9rMNma2f9IZupl93Mw+l5l+2szuNLPWTD/z\nTWb2iJltMLPPD9l8kZk9mFnn4cFvJWa20Mx+a2ZLzewpM5syZLt3mVkr6W6eRc6KAl/yxc+B6Wb2\nnJl908z+CsDdv0a6W9ur3f3qzJn/p4Fr3X0B0Ap8dMh2Otz9UuBfSPemOZwLgVeT7n77s5n+iU6l\n190bgW8BjwEfBC4BbjKzCZl1ZgPfdPeLgEPABzLb/jrwFndfCNwH/NOQ7Za4e6O7fzmLGkRGpSYd\nyQvuftjMFgIvA64GHjKz2939gRNWvZz0YDj/ne4ihRLgj0OW/2DI850j7O6JzDeJHjPbA5yXRYmD\nfTitAta4+04AM9tIuoO/g8A2d//vzHrfIz0Ax5OkDwy/yNQbJ91d76CHsti3SFYU+JI33H0AeBp4\n2sxWAe8GHjhhNQN+4e5vH2kzI0wP1TNkeoD030k/x38jPnE4vMH3pE54f4pjf2cn7s8z9a5x95GG\nG3xhhPkip01NOpIXzGy2mb1oyKx5wJbMdCdQlZn+E3DlYPu8mVWa2YuHvO9tQ56Hnvmfym5gkplN\nMLNS0j8gn64ZQ8aRfQfwe2A9UDc438yKzeziM9i2yCnpDF/yxTjg62ZWQ/ps+3ng5syyxcCTZrYj\n045/E/CDTDBDuk3/ucx0rZmtJH0WPtK3gJO4e5+Z/SOwhPSYC8+ewb9hPelxhu8j3TXx3e7ea2Zv\nAb6WGYawiPRvC1Hu9VUCot4yJTIyg6E0uvvesGsRCYOadEREIkJn+CIiEaEzfBGRiFDgi4hEhAJf\nRCQiFPgiIhGhwBcRiQgFvohIRPx/8HLH6F7LuSMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc670767f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reward_prediction(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the rewards:\n",
    "Now, the LSTM_RNN output (from validation phase) is considered as input for DQN model, to be able to select the action that has the maximum longtime reward for the customer (highest CLV) .. \n",
    "\n",
    "https://arxiv.org/pdf/1602.01580.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Deep Neural Network (DQN):\n",
    "\n",
    "https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Definition of the cell computation.\n",
    "# this method takes single cell and returns single number \n",
    "def lstm_cell(i, o, state):\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_model(X, w_h_1, w_h_2, w_o, bias_I_1, bias_I_2, bias_h):\n",
    "\n",
    "    layer_1 = tf.matmul(X, w_h_1) + bias_I_1\n",
    "    layer_1 = tf.nn.relu(layer_1)  ## el performance of softmax outperforms relu!!\n",
    "    \n",
    "    layer_2 = tf.matmul(layer_1, w_h_2) + bias_I_2\n",
    "    layer_2 = tf.nn.sigmoid(layer_2) \n",
    "\n",
    "    py_x = tf.matmul(layer_2, w_o) + bias_h\n",
    "    \n",
    "    return py_x  #predicted output\n",
    "    # note that we dont take the softmax at the end because our cost fn does that for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_training(X, output):\n",
    " \n",
    "    w_h_1 = init_weights([np.shape(X)[1], n_nodes_hl1]) # create symbolic variables\\n\",\n",
    "    w_h_2 = init_weights([n_nodes_hl1, n_nodes_hl2]) # create symbolic variables\\n\",\n",
    "    w_o = init_weights([n_nodes_hl2, 1])\n",
    "    \n",
    "    bias_I_1=init_weights([n_nodes_hl1])\n",
    "    bias_I_2=init_weights([n_nodes_hl2])\n",
    "    bias_h=init_weights([1])\n",
    "    \n",
    "    py_x = DQN_model(tf.cast(X, tf.float32), w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h)  #model training  \n",
    "        \n",
    "    cost = tf.reduce_mean(tf.square(py_x - output)) # compute costs\",  # excpect float32\n",
    "\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost) # construct an optimizer\\n\",\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "#      # Cost calculation\n",
    "#     for step in xrange(1000):\n",
    "#         l,_ = sess.run([cost, train_op], feed_dict={x:np.transpose(X), y:np.transpose(output)})\n",
    "        \n",
    "#         if step % 100 == 0:\n",
    "#             loss_RNN.append(l)\n",
    "            \n",
    "#     plt.plot(loss_RNN)\n",
    "    \n",
    "    sess.close()    \n",
    "    return w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dataframe to smaller frames\n",
    "\n",
    "def splitDataFrameIntoSmaller(df, chunkSize = 3): \n",
    "    listOfDf = list()\n",
    "    numberChunks = len(df) // chunkSize + 1\n",
    "    for i in range(numberChunks):\n",
    "        listOfDf.append(df[i*chunkSize:(i+1)*chunkSize])\n",
    "    #listOfDf = random.sample(df, chunkSize)  ## in case df is an array not dataframe\n",
    "    return listOfDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if only RFM in the state .. \n",
    "\n",
    "Mxa0 = (tuplesMx.iloc[:,0:24]).values \n",
    "Mxa1 = (tuplesMx.iloc[:,25:48]).values\n",
    "Mxa2 = (tuplesMx.iloc[:,49:72]).values\n",
    "Mxa3 = (tuplesMx.iloc[:,73:96]).values\n",
    "Mxa4 = (tuplesMx.iloc[:,97:120]).values\n",
    "Mxa5 = (tuplesMx.iloc[:,121:144]).values\n",
    "Mxa6 = (tuplesMx.iloc[:,145:168]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction phase: \n",
    "def DQN_predict(tuplesMx, output):\n",
    "    \n",
    "    Mxa0 = (tuplesMx.iloc[:,0:6]).values \n",
    "    Mxa1 = (tuplesMx.iloc[:,6:12]).values\n",
    "    Mxa2 = (tuplesMx.iloc[:,12:18]).values\n",
    "    Mxa3 = (tuplesMx.iloc[:,18:24]).values\n",
    "    Mxa4 = (tuplesMx.iloc[:,24:30]).values\n",
    "    Mxa5 = (tuplesMx.iloc[:,30:36]).values\n",
    "    Mxa6 = (tuplesMx.iloc[:,36:42]).values\n",
    "\n",
    "    [w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h] = DQN_training(Mxa0, output)\n",
    "    predict_op_0 = DQN_model(Mxa0.astype(np.float32), w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h)\n",
    "      \n",
    "        \n",
    "#     print next_state_next_action2     \n",
    "    [w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h] = DQN_training(Mxa1, output)\n",
    "    predict_op_1 = DQN_model(Mxa1.astype(np.float32), w_h_1, w_h_2, w_o,bias_I_1, bias_I_2, bias_h)   #optimal prediction\n",
    "    \n",
    "    \n",
    "    ## print next_state_next_action3\n",
    "    [w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h] = DQN_training(Mxa2, output)\n",
    "    predict_op_2 = DQN_model(Mxa2.astype(np.float32), w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h)\n",
    "     \n",
    "    [w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h] = DQN_training(Mxa3, output)\n",
    "    predict_op_3 = DQN_model(Mxa3.astype(np.float32), w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h)\n",
    "      \n",
    "        \n",
    "#     print next_state_next_action4    \n",
    "    [w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h] = DQN_training(Mxa4, output)\n",
    "    predict_op_4 = DQN_model(Mxa4.astype(np.float32), w_h_1, w_h_2, w_o,bias_I_1, bias_I_2, bias_h)   #optimal prediction\n",
    "    \n",
    "    \n",
    "    ## print next_state_next_action5\n",
    "    [w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h] = DQN_training(Mxa5, output)\n",
    "    predict_op_5 = DQN_model(Mxa5.astype(np.float32), w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h)\n",
    "     \n",
    "    ## print next_state_next_action6    \n",
    "    [w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h] = DQN_training(Mxa6, output)\n",
    "    predict_op_6 = DQN_model(Mxa6.astype(np.float32), w_h_1, w_h_2, w_o,bias_I_1, bias_I_2,bias_h)\n",
    "\n",
    "        \n",
    "    sess2 = tf.Session()\n",
    "    init2 = tf.global_variables_initializer()\n",
    "    sess2.run(init2)\n",
    "\n",
    "    l0=sess2.run(predict_op_0)\n",
    "    l1=sess2.run(predict_op_1)\n",
    "    l2=sess2.run(predict_op_2)\n",
    "    l3=sess2.run(predict_op_3)\n",
    "    l4=sess2.run(predict_op_4)\n",
    "    l5=sess2.run(predict_op_5)\n",
    "    l6=sess2.run(predict_op_6)\n",
    "        \n",
    "    Q_predicted = [l0, l1, l2, l3, l4, l5, l6]\n",
    "    \n",
    "    sess2.close()\n",
    "    return np.transpose(Q_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q(s,a) representing the (Quality) of action a at state is .. \n",
    "\n",
    "this Q value depends on the immediate reward r .. however, it'll be more effective if it takes the future rewards Q(s', a') into consideration .. \n",
    "\n",
    "the future rewards are discounted by probability gama .. cause the evironment is stochastic hence, it is uncertain that each time you select action a you gonna get the same reward r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_reward = np.transpose(predicted_reward) \n",
    "def Q_learning():\n",
    "    listOfDf = splitDataFrameIntoSmaller(tuplesMx, chunkSize)\n",
    "    _, actions = tuple_() \n",
    "    Q_predicted = np.zeros([np.shape(train)[0], max(train[actions].values.max(axis = 0).astype(int))])\n",
    "    _, predicted_reward = train_lstm_model()\n",
    "    \n",
    "    for i in xrange(chunkSize):\n",
    "        for j in xrange(batch_size):\n",
    "            Q_optimal = predicted_reward[j].astype(float) + gamma*np.max(Q_predicted) # returns max value per row !\n",
    "\n",
    "            Q_optimal = np.array(Q_optimal, dtype=np.float32) # convert output to float32 to match py_x\n",
    "\n",
    "            Q_predicted = DQN_predict(listOfDf[i].astype(float), Q_optimal)\n",
    "\n",
    "\n",
    "    print(\"Mean Optimal Q value is:\", np.mean(Q_optimal))\n",
    "    # print(\"Q fun. is: \", Q_predicted) ## CLV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    Q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean Optimal Q value is:', 1.9275157)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "an enhancement for this implementation is to change the states to be the whole doner state instead of just RFM ..\n",
    "\n",
    "https://github.com/EAboelhamd/kdd98-1/blob/master/notebooks/exploratory.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do I apply experience replay .. \n",
    "\n",
    "The algorithm is mentioned here .. \n",
    "\n",
    "https://www.intelnervana.com/demystifying-deep-reinforcement-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
