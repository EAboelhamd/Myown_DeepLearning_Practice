{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "This is a practice for Double Deep Reinforcement Learning implemented in this notebook \n",
    "\n",
    "https://github.com/EAboelhamd/Deep_Reinforcement_Learning-CartPole/blob/master/Double_Q-Learning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-12 11:20:22,016] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return: -0.282564\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "#               Written by: Yih Kai Teh                #\n",
    "########################################################\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "seed = 600\n",
    "tf.set_random_seed(seed)\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "discount                = 0.99\t\t\t\t\t\t\t\t  # discount rate of the rewards\n",
    "batch_size              = 500\t\t\t\t\t\t\t\t  # size of mini-batch for training \n",
    "replay_size             = 500000\t\t\t\t\t\t\t  # size of Experience Replay\n",
    "maximum_episode_length  = 200\t\t\t\t\t\t\t\t  # length of episodes (same as default setting)\n",
    "replay_buffer           = deque()\t\t\t\t\t\t\t  # store the details of experience in Experience Replay\n",
    "test_episode            = 10\t\t\t\t\t\t\t\t  # number of episodes for testing\n",
    "total_episode           = 500\t\t\t\t\t\t\t\t  # number of episodes for training\n",
    "number_hidden           = 100\t\t\t\t\t\t\t\t  # number of units for the hidden layer\n",
    "learning_rate           = 0.0005\t\t\t\t\t\t\t  # learning rate for the optimizater \n",
    "state_dimension         = env.observation_space.shape[0]\t\t\t\t  # number of observations which will be received\n",
    "action_dimension        = env.action_space.n\t\t\t\t\t\t  # number of actions which can be taken\n",
    "Total_Move = Total_Loss = Total_Return = Total_Episode = cummulative_loss = np.array([])  # store the details for plotting the graph\n",
    "\n",
    "\n",
    "# epsilon greedy with fix epsilon. A decay epsilon can be implemented to reach optimal policy.\n",
    "def epsilon_greedy(obs):\n",
    "\tif float(np.random.rand(1)) <= 0.05:\n",
    "\t\taction = env.action_space.sample()\t# random action\n",
    "\telse:\n",
    "\t\tQ1_out = sess.run(Q1_output, feed_dict={state: [obs]})\n",
    "\t\tQ2_out = sess.run(Q2_output, feed_dict={state: [obs]})\n",
    "\n",
    "\t\taction = np.argmax((Q1_out + Q2_out)[0]) # optimal action\n",
    "\treturn action\n",
    "\n",
    "\n",
    "# store the transitions and details information into the experience replay\n",
    "def store_all_information(obs, action, reward, next_obs, done):\n",
    "\tglobal replay_buffer\n",
    "\treplay_buffer.append((obs, action, reward, next_obs, done)) # state, action, reward, next_state, continue\n",
    "\n",
    "\t# clear the experience replay by one to allow for new memory when it is full \n",
    "\tif len(replay_buffer) > replay_size:\n",
    "\t\treplay_buffer.popleft()\n",
    "\n",
    "\t# start training when experience replay has enough experience for one batch size\n",
    "\tif len(replay_buffer) > batch_size:\n",
    "\t\tloss = train_Q_network()\n",
    "\t\treturn loss\n",
    "\telse:\n",
    "\t\treturn 0\n",
    "\n",
    "\n",
    "# randomly sample experience from experience replay for training\n",
    "def train_Q_network():\n",
    "\tglobal replay_buffer\n",
    "\n",
    "\t# randomly sample experience\n",
    "\tminibatch        = random.sample(replay_buffer, batch_size)\n",
    "\tbatch_state      = [data[0] for data in minibatch]\n",
    "\tbatch_action     = [data[1] for data in minibatch]\n",
    "\tbatch_reward     = [data[2] for data in minibatch]\n",
    "\tbatch_next_state = [data[3] for data in minibatch]\n",
    "\n",
    "\t# randomly update either first or second Q network\n",
    "\tif np.random.rand(1) > 0.5:\n",
    "\t\tbatch_Q1_target = []\n",
    "\t\tbatch_Q1_value  = stationary_output.eval(feed_dict={state: batch_next_state})\n",
    "        \n",
    "\t\tfor i in range(0, batch_size):\n",
    "\t\t\tcontinues = minibatch[i][4]\n",
    "\t\t\tbatch_Q1_target.append(batch_reward[i] + discount * continues * np.max(batch_Q1_value[i]))\n",
    "            \n",
    "\t\tcost, _ = sess.run([loss_op_1, train_op_1], feed_dict={state: batch_state, actions: batch_action, Q_target: batch_Q1_target})\n",
    "\n",
    "\telse:\n",
    "\t\tbatch_Q2_target = []\n",
    "\t\tbatch_Q2_value  = stationary_output.eval(feed_dict={state: batch_next_state})\n",
    "\t\tfor i in range(0, batch_size):\n",
    "\t\t\tcontinues = minibatch[i][4]\n",
    "\t\t\tbatch_Q2_target.append(batch_reward[i] + discount * continues * np.max(batch_Q2_value[i]))\n",
    "\n",
    "\t\tcost, _ = sess.run([loss_op_2, train_op_2], feed_dict={state: batch_state, actions: batch_action, Q_target: batch_Q2_target})\n",
    "        \n",
    "\treturn cost\n",
    "\n",
    "\n",
    "# run the evaluation at every 20 episodes in order to monitor the performance during training \n",
    "def evaluation(episode, cummulative_loss):\n",
    "\tglobal Total_Return, Total_Move, Total_Loss, Total_Episode\n",
    "\tepisode_length = 0\n",
    "\ttotal_return   = np.array([])\n",
    "\n",
    "\t# run evaluation for 10 times because each episode is stochastic\n",
    "\tfor i in range(test_episode):\n",
    "\t\tobservation = env.reset()\n",
    "\n",
    "\t\t# step through the environment until maximum episode length is reached (default is 200)\n",
    "\t\tfor j in range(maximum_episode_length):\n",
    "\n",
    "\t\t\t# select best action using both network and step through the environment\n",
    "\t\t\tfirst_q_out                       = sess.run(Q1_output, feed_dict={state: [observation]})\n",
    "\t\t\tsecond_q_out                      = sess.run(Q2_output, feed_dict={state: [observation]})\n",
    "\t\t\tselected_action                   = np.argmax((first_q_out + second_q_out)[0])\n",
    "\t\t\tnext_observation, reward, done, _ = env.step(selected_action)\n",
    "\n",
    "\t\t\tobservation = next_observation\n",
    "\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t# calculate the return (the -1 below is because the modified rewards of -1 at terminating step)\n",
    "\t\ttotal_return    = np.append(total_return, (discount ** (j)) * -1)\n",
    "\t\tepisode_length += j+1\n",
    "\n",
    "\t# display and store all the result for plotting the graph at the end of training\n",
    "\taverage_episode_length = episode_length/test_episode\n",
    "# \tprint ('Episode: %4d Mean Episode Length: %10f Mean Return: %10f' %(episode + 1, average_episode_length, np.mean(total_return)))\n",
    "\tTotal_Return  = np.append(Total_Return, np.mean(total_return))\n",
    "\tTotal_Move    = np.append(Total_Move, average_episode_length)\n",
    "\tTotal_Loss    = np.append(Total_Loss, np.mean(cummulative_loss))\n",
    "\tTotal_Episode = np.append(Total_Episode, episode + 1)\n",
    "\n",
    "\n",
    "# initialize the weights and biases for all networks\n",
    "weights = {\t'first_Q_Learning_hidden_layer' : tf.Variable(tf.truncated_normal([state_dimension, number_hidden])),\n",
    "\t\t'first_Q_Learning_output_layer' : tf.Variable(tf.truncated_normal([number_hidden, action_dimension])),\n",
    "\t\t'second_Q_Learning_hidden_layer': tf.Variable(tf.truncated_normal([state_dimension, number_hidden])),\n",
    "\t\t'second_Q_Learning_output_layer': tf.Variable(tf.truncated_normal([number_hidden, action_dimension])),\n",
    "\t\t'stationary_target_hidden_layer': tf.Variable(tf.truncated_normal([state_dimension, number_hidden])),\n",
    "\t\t'stationary_target_output_layer': tf.Variable(tf.truncated_normal([number_hidden, action_dimension]))}\n",
    "\n",
    "biases  = {\t'first_Q_Learning_hidden_layer' : tf.Variable(tf.constant(0.01,shape = [number_hidden])),\n",
    "\t\t'first_Q_Learning_output_layer' : tf.Variable(tf.constant(0.01,shape = [action_dimension])),\n",
    "\t\t'second_Q_Learning_hidden_layer': tf.Variable(tf.constant(0.01,shape = [number_hidden])),\n",
    "\t\t'second_Q_Learning_output_layer': tf.Variable(tf.constant(0.01,shape = [action_dimension])),\n",
    "\t\t'stationary_target_hidden_layer': tf.Variable(tf.constant(0.01,shape = [number_hidden])),\n",
    "\t\t'stationary_target_output_layer': tf.Variable(tf.constant(0.01,shape = [action_dimension]))}\n",
    "\n",
    "# update weights and biases of First Q-Learning\n",
    "update_Q1_weight_hidden = weights['stationary_target_hidden_layer'].assign(weights['first_Q_Learning_hidden_layer'])\n",
    "update_Q1_weight_output = weights['stationary_target_output_layer'].assign(weights['first_Q_Learning_output_layer'])\n",
    "update_Q1_bias_hidden   = biases['stationary_target_hidden_layer'].assign(biases['first_Q_Learning_hidden_layer'])\n",
    "update_Q1_bias_output   = biases['stationary_target_output_layer'].assign(biases['first_Q_Learning_output_layer'])\n",
    "update_all_Q1           = [update_Q1_weight_hidden, update_Q1_weight_output, update_Q1_bias_hidden, update_Q1_bias_output]\n",
    "\n",
    "# update weights and biases of Second Q-Learning\n",
    "update_Q2_weight_hidden = weights['stationary_target_hidden_layer'].assign(weights['second_Q_Learning_hidden_layer'])\n",
    "update_Q2_weight_output = weights['stationary_target_output_layer'].assign(weights['second_Q_Learning_output_layer'])\n",
    "update_Q2_bias_hidden   = biases['stationary_target_hidden_layer'].assign(biases['second_Q_Learning_hidden_layer'])\n",
    "update_Q2_bias_output   = biases['stationary_target_output_layer'].assign(biases['second_Q_Learning_output_layer'])\n",
    "update_all_Q2           = [update_Q2_weight_hidden, update_Q2_weight_output, update_Q2_bias_hidden, update_Q2_bias_output]\n",
    "\n",
    "# placeholder for state, actions and target value\n",
    "state     = tf.placeholder(tf.float32, shape=[None, state_dimension])\n",
    "actions   = tf.placeholder(tf.int32, shape=[None])\n",
    "Q_target  = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "# first Q-learning network\n",
    "Q1_hidden = tf.nn.relu(tf.matmul(state, weights['first_Q_Learning_hidden_layer']) + biases['first_Q_Learning_hidden_layer'])\n",
    "Q1_output = tf.matmul(Q1_hidden, weights['first_Q_Learning_output_layer']) + biases['first_Q_Learning_output_layer']\n",
    "\n",
    "# second Q-learning network\n",
    "Q2_hidden = tf.nn.relu(tf.matmul(state, weights['second_Q_Learning_hidden_layer']) + biases['second_Q_Learning_hidden_layer'])\n",
    "Q2_output = tf.matmul(Q2_hidden, weights['second_Q_Learning_output_layer']) + biases['second_Q_Learning_output_layer']\n",
    "\n",
    "# stationary target network\n",
    "stationary_hidden = tf.nn.relu(tf.matmul(state, weights['stationary_target_hidden_layer']) + biases['stationary_target_hidden_layer'])\n",
    "stationary_output = tf.matmul(stationary_hidden, weights['stationary_target_output_layer']) + biases['stationary_target_output_layer']\n",
    "\n",
    "# compute Q from current q_output and one hot actions\n",
    "Q1 = tf.reduce_sum(tf.multiply(Q1_output, tf.one_hot(actions, action_dimension)), reduction_indices=1)\n",
    "Q2 = tf.reduce_sum(tf.multiply(Q2_output, tf.one_hot(actions, action_dimension)), reduction_indices=1)\n",
    "\n",
    "# loss operation \n",
    "loss_op_1 = tf.reduce_mean(tf.square(Q_target - Q1))\n",
    "loss_op_2 = tf.reduce_mean(tf.square(Q_target - Q2))\n",
    "\n",
    "# train operation\n",
    "train_op_1 = tf.train.AdamOptimizer(learning_rate).minimize(loss_op_1)\n",
    "train_op_2 = tf.train.AdamOptimizer(learning_rate).minimize(loss_op_2)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "\tsess.run(init)\n",
    "\tsaver = tf.train.Saver()\n",
    "\n",
    "\t# train for 1000 episodes\n",
    "\tfor episode in range(total_episode):\n",
    "\t\tobservation = env.reset()\n",
    "\n",
    "\t\t# updates the stationary network every 5 episodes\n",
    "\t\tif episode % 5 == 0:\n",
    "\t\t\tsess.run(update_all_Q1)\n",
    "\t\t\tsess.run(update_all_Q2)\n",
    "\n",
    "\t\t# step through the environment until maximum episode length is reached (default is 200)\n",
    "\t\tfor t in range(maximum_episode_length):\n",
    "\n",
    "\t\t\t# select action based on epsilon greedy and use that action in the environment\n",
    "\t\t\tselected_action                   = epsilon_greedy(observation)\n",
    "\t\t\tnext_observation, reward, done, _ = env.step(selected_action)\n",
    "\n",
    "\t\t\t# modifies the reward into sparse rewards\n",
    "\t\t\treward = -1 if done else 0\n",
    "\n",
    "\t\t\t# store all the transition and details into experience replay\n",
    "\t\t\tcost = store_all_information(observation, selected_action, reward, next_observation, 1.0 - done)\n",
    "\n",
    "\t\t\t# start accumulating the loss after experience replay has enough experience for one batch size to train\n",
    "\t\t\tif len(replay_buffer) > batch_size:\n",
    "\t\t\t\tcummulative_loss = np.append(cummulative_loss, cost)\n",
    "\n",
    "\t\t\tobservation = next_observation\n",
    "\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t# run evaluation every 20 episodes\n",
    "\t\tif (episode + 1) % 20 == 0:\n",
    "\t\t\tevaluation(episode, cummulative_loss)\n",
    "\n",
    "# \tprint(\"Average Episode Length: %f\" % (np.mean(Total_Move)))\n",
    "\tprint(\"Average Return: %f\" % (np.mean(Total_Return)))\n",
    "\n",
    "\t# Save the model\n",
    "# \tsaver.save(sess, './model_Double_Q-Learning/Double_Q-Learning')\n",
    "\n",
    "\t# Plot for the graph for the return, episodes length and loss throughout the training\n",
    "\tplt.figure(0)\n",
    "\tplt.plot(Total_Episode, Total_Return)\n",
    "\tplt.title('Plot of Performance Over %d Episodes' %(total_episode))\n",
    "\tplt.xlabel('Episodes')\n",
    "\tplt.ylabel('Performance (Mean Discounted Return)')\n",
    "# \tplt.savefig('Plot of Performance Over %d Episodes.png' %(total_episode))\n",
    "\n",
    "\tplt.figure(1)\n",
    "\tplt.plot(Total_Episode, Total_Move)\n",
    "\tplt.title('Plot of Performance Over %d Episodes' %(total_episode))\n",
    "\tplt.xlabel('Episodes')\n",
    "\tplt.ylabel('Performance (Episode Length)')\n",
    "# \tplt.savefig('Plot of Performance (Episode Length) Over %d Episodes.png' %(total_episode))\n",
    "\n",
    "\tplt.figure(2)\n",
    "\tplt.plot(Total_Episode, Total_Loss)\n",
    "\tplt.title('Plot of Loss Over %d Episodes' %(total_episode))\n",
    "\tplt.xlabel('Episodes')\n",
    "\tplt.ylabel('Training Loss')\n",
    "# plt.savefig('Plot of Loss Over %d Episodes.png' %(total_episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
