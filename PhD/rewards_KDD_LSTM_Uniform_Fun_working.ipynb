{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "Main Goals:\n",
    "\n",
    "1. Identify the recipients that will engage with the campaign.\n",
    "2. Maximise the campaignâ€™s revenue.\n",
    "\n",
    "\n",
    "Comments\n",
    "\n",
    "- The dataset contains only 5% of donors.\n",
    "- The donations are usually smaller than $20.\n",
    "- This data is quite noisy, high dimensional.\n",
    "- There is an inverse relationship between the probability to donate and the amount donated.\n",
    "\n",
    "\n",
    "Link for dataset and some analysis ==> \n",
    "\n",
    "https://github.com/rebordao/kdd98cup\n",
    "\n",
    "https://github.com/bobbyantonio/KDD98/blob/master/CleanData.py\n",
    "\n",
    "- Github solutions ==>\n",
    "https://github.com/rebordao/kdd98cup\n",
    "\n",
    "\n",
    "- Siraj notebook for a better data visualization:\n",
    "\n",
    "https://www.youtube.com/watch?v=yQsOFWqpjkE\n",
    "\n",
    "There is a notebook associated with this one contains the exploratory data analysis part .. \n",
    "http://localhost:8888/notebooks/Python%20codes/rewards_KDD_Exploratory_Analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "\n",
    "## warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    n = 10 #number of records in file\n",
    "    s = 1 #desired sample size\n",
    "    df = pd.read_csv('tuple.csv', header = None, skiprows = sorted(random.sample(range(n),n-s)))\n",
    "    df.columns = ['r0','f0','m0','ir0','if0', 'a','r1', 'f1', 'm1','ir1','if1','rew']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r0</th>\n",
       "      <th>f0</th>\n",
       "      <th>m0</th>\n",
       "      <th>ir0</th>\n",
       "      <th>if0</th>\n",
       "      <th>a</th>\n",
       "      <th>r1</th>\n",
       "      <th>f1</th>\n",
       "      <th>m1</th>\n",
       "      <th>ir1</th>\n",
       "      <th>if1</th>\n",
       "      <th>rew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   r0  f0   m0  ir0  if0  a  r1  f1   m1  ir1  if1  rew\n",
       "0   0   0  0.0    0    0  3   1   0  0.0    1    0  0.0\n",
       "1   0   0  0.0    0    0  0   1   0  0.0    1    0  0.0\n",
       "2   0   0  0.0    0    0  2   1   0  0.0    1    0  0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## I have to run it over all the data not just 100 records\n",
    "df = load_data()\n",
    "df['a'] = np.random.uniform(0,11,(df['a'].shape)).astype(int)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_split():\n",
    "    \n",
    "    ## I want to infer next_state from current_state, hence, learnt from LSTM .. I = s, O = s'\n",
    "    df = load_data()\n",
    "    train, test = train_test_split(df, test_size = 0.90)  # split data to 50-50 cross validate, Roger 1.6*1000,000\n",
    "    \n",
    "    train_x = train[train.columns.difference(['rew', 'a', 'r1', 'f1', 'm1', 'ir1', 'if1'])]\n",
    "    train_y = train[train.columns.difference(['rew', 'a', 'r0', 'f0', 'm0', 'ir0', 'if0'])]\n",
    "    \n",
    "#     train_y = train_y.convert_objects(convert_numeric = True)\n",
    "    \n",
    "#     train_y = train_y.convert_objects(convert_numeric = True)\n",
    "    \n",
    "    return train_x, train_y, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression phase:\n",
    "\n",
    "Before performing the prediction task .. let's split the data to training and validation sets .. \n",
    "\n",
    "To avoid any problems in prediction by having string variables .. let's binarize (catergorize) all the variables .. \n",
    "\n",
    "Guidance ==> https://pythonprogramming.net/rnn-tensorflow-python-machine-learning-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "Num_itrs = 2  # no loop 3leha ( w dah el sa7) .. we just have to run the whole algo. 10 times and report the avg. results\n",
    "\n",
    "num_epoch = 1 #23 #epochs are cycles of Feedforward and Backprob\n",
    "## el mafrood yeb2a feh loop 3la el epochs elli heyya el steps .. w avg. reward per step is calculated \n",
    "batch_size = 2\n",
    "chunkSize = 1\n",
    "\n",
    "train_x, train_y, train = df_split()\n",
    "\n",
    "n_nodes_hl1 = 40 #np.shape(train)[1]\n",
    "n_nodes_hl2 = 15 #np.shape(train)[0]\n",
    "# NUM_STATES = np.shape(train)[1]\n",
    "NUM_DIM =  np.shape(train_x)[1]\n",
    "num_nodes = np.shape(train_x)[0]\n",
    "NUM_DIM_output = np.shape(train_y)[1]\n",
    "########################################################################################################\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "max_steps = 2\n",
    "##########################################################################################################\n",
    "\n",
    "num_unrollings = 5\n",
    "\n",
    "# best_actions = np.zeros([np.shape(train)[0], batch_size])\n",
    "Q_optimal = [] #np.zeros([np.shape(curr_state_current_action)[0], len(df['ACCOUNT_STATUS'].unique())])\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "## for training \n",
    "x = tf.placeholder(tf.float32, shape=[num_nodes, NUM_DIM])\n",
    "y =  tf.placeholder(tf.float32, shape=[num_nodes, NUM_DIM_output])\n",
    "\n",
    "# ## for testing\n",
    "# x_ = tf.placeholder(tf.float32, shape=[NUM_DIM, None])\n",
    "# y_ =  tf.placeholder(tf.float32, shape=[NUM_DIM, None])\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "# Input gate: input, previous output, and bias.\n",
    "ix = tf.Variable(tf.random_uniform([NUM_DIM, NUM_DIM_output],0,5, dtype = tf.float32))# init_weights_RNN([n_nodes_hl1, NUM_ACTIONS])\n",
    "im = tf.Variable(tf.random_uniform([NUM_DIM_output, NUM_DIM_output],0,5, dtype = tf.float32))\n",
    "ib = tf.Variable(tf.zeros([1, NUM_DIM_output], dtype = tf.float32))\n",
    "\n",
    "# Forget gate: input, previous output, and bias.\n",
    "fx = tf.Variable(tf.random_uniform([NUM_DIM, NUM_DIM_output],0,5, dtype = tf.float32))\n",
    "fm = tf.Variable(tf.random_uniform([NUM_DIM_output, NUM_DIM_output],0,5, dtype = tf.float32))\n",
    "fb = tf.Variable(tf.zeros([1, NUM_DIM_output], dtype = tf.float32))\n",
    "\n",
    "# Memory cell: input, state and bias.                             \n",
    "cx = tf.Variable(tf.random_uniform([NUM_DIM, NUM_DIM_output],0,5, dtype = tf.float32))\n",
    "cm = tf.Variable(tf.random_uniform([NUM_DIM_output, NUM_DIM_output],0,5, dtype = tf.float32))\n",
    "cb = tf.Variable(tf.zeros([1, NUM_DIM_output], dtype = tf.float32))\n",
    "\n",
    "# Output gate: input, previous output, and bias.\n",
    "ox = tf.Variable(tf.random_uniform([NUM_DIM, NUM_DIM_output],0,5, dtype = tf.float32))\n",
    "om = tf.Variable(tf.random_uniform([NUM_DIM_output, NUM_DIM_output],0,5, dtype = tf.float32))\n",
    "ob = tf.Variable(tf.zeros([NUM_DIM_output], dtype = tf.float32))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([num_nodes, NUM_DIM_output], dtype = tf.float32), trainable = False) \n",
    "\n",
    "saved_state = tf.Variable(tf.zeros([num_nodes, NUM_DIM_output], dtype = tf.float32), trainable = False) \n",
    "\n",
    "# Classifier weights and biases.\n",
    "w = tf.Variable(tf.random_uniform([NUM_DIM_output, NUM_DIM_output], 0, 5, dtype = tf.float32))\n",
    "b = tf.Variable(tf.zeros([NUM_DIM_output], dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the cell computation.\n",
    "# this method takes single cell and returns single number \n",
    "def lstm_cell(i, o, state):\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate*tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(train_x, saved_output, saved_state):\n",
    "    # Unrolled LSTM loop.    \n",
    "#     output_ = list()\n",
    "    output = saved_output  # row !\n",
    "    state = saved_state  # row !\n",
    "\n",
    "    \n",
    "    output_, state = lstm_cell(train_x.values.astype(np.float32), tf.cast(output, tf.float32), tf.cast(state, tf.float32)) \n",
    "    \n",
    "    # astype('U') .. to convert numpy array to string ..\n",
    "#     for i in range(np.shape(train_x)[0]):## el loop faydetha to copy the next line that is just for single unit \n",
    "#         output_, state = lstm_cell(train_x.values[i, None].astype(np.float32), output, state) \n",
    "\n",
    "    \n",
    "        # in case the last values are saved !\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        model_output = tf.matmul(output_, w) + b # outputs single value\n",
    "    \n",
    "    \n",
    "    return model_output ## the output for the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model():\n",
    "\n",
    "    loss_RNN = []\n",
    "    \n",
    "    model_output = lstm_model(train_x, saved_output, saved_state) #output here is a vector\n",
    "\n",
    "    cost = tf.reduce_mean(tf.square(y - model_output))\n",
    "\n",
    "    optimize = tf.train.GradientDescentOptimizer(0.001).minimize(cost) \n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()  ## updated version from initialize_all_variables :) \n",
    "    sess.run(init)\n",
    "\n",
    "    \n",
    "    Predicted_states = sess.run(model_output, feed_dict={x:train_x.values.astype(np.float32), y:train_y.values.astype(np.float32)})\n",
    "    \n",
    "    # Cost calculation\n",
    "    for step in range(1000):\n",
    "        l,_ = sess.run([cost, optimize], feed_dict={x:train_x.values, y:train_y.values})\n",
    "    \n",
    "        if step % 100 == 0:\n",
    "            loss_RNN.append(l)\n",
    "     \n",
    "    sess.close()            \n",
    "    \n",
    "#     print Predicted_states\n",
    "    \n",
    "    return Predicted_states  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the rewards:\n",
    "Now, the LSTM_RNN output (from validation phase) is considered as input for DQN model, to be able to select the action that has the maximum longtime reward for the customer (highest CLV) .. \n",
    "\n",
    "https://arxiv.org/pdf/1602.01580.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Deep Neural Network (DQN):\n",
    "\n",
    "https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I believe enn this funtion is to predict the actions .. w neb2a t7t wanna predict max Q \n",
    "def DQN_train(x_inputs):\n",
    "\n",
    "    # Placeholder\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, np.shape(x_inputs)[1]])\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "\n",
    "\n",
    "    # Model architecture parameters\n",
    "    n_dim = np.shape(x_inputs)[1] \n",
    "    n_neurons_1 = 40\n",
    "    n_neurons_2 = 15\n",
    "    n_target = 1 #np.shape(x_inputs)[0]\n",
    "    \n",
    "    epochs = 10\n",
    "    \n",
    "#     Qs_predicted = []\n",
    "    \n",
    "        # Initializers\n",
    "    sigma = 1\n",
    "    \n",
    "    #First Q Network\n",
    "    w1 = tf.Variable(tf.random_uniform([n_dim, n_neurons_1], 0, 0.1))\n",
    "    bias1 = tf.Variable(tf.random_uniform([n_neurons_1], 0, 0.1))\n",
    "    \n",
    "    w2 = tf.Variable(tf.random_uniform([n_neurons_1, n_neurons_2], 0, 0.1))\n",
    "    bias2 = tf.Variable(tf.random_uniform([n_neurons_2], 0, 0.1))\n",
    "    \n",
    "    w3 = tf.Variable(tf.random_uniform([n_neurons_2, n_target], 0, 0.1))\n",
    "    bias3 = tf.Variable(tf.random_uniform([n_target], 0, 0.1))\n",
    "    \n",
    "    hidden_1 = tf.nn.relu(tf.matmul(X, w1) + bias1)\n",
    "    hidden_2 = tf.nn.relu(tf.matmul(hidden_1, w2) + bias2)\n",
    "    y_ = tf.matmul(hidden_2, w3) + bias3\n",
    "    \n",
    "    # initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    # Cost function\n",
    "    mse = tf.reduce_mean(tf.squared_difference(y_, Y))\n",
    "\n",
    "    # Optimizer\n",
    "    opt = tf.train.RMSPropOptimizer(0.0001, 0.99).minimize(mse)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        num_itr = int(np.shape(x_inputs)[0]/batch_size)\n",
    "\n",
    "        Qs_next_state = sess.run(y_, feed_dict={X: x_inputs})\n",
    "\n",
    "    return Qs_next_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(state, actions):\n",
    "    \n",
    "     # Placeholder\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, np.shape(state)[1]])\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "\n",
    "\n",
    "    # Model architecture parameters\n",
    "    n_dim = np.shape(state)[1] \n",
    "    n_neurons_1 = 40\n",
    "    n_neurons_2 = 15\n",
    "    n_target = 11 #np.shape(x_inputs)[0]\n",
    "    \n",
    "    epochs = 2 #100\n",
    "    \n",
    "    Qs_next_state = []\n",
    "    \n",
    "    # Initializers\n",
    "    sigma = 1\n",
    "    \n",
    "    #First Q Network\n",
    "    w1 = tf.Variable(tf.random_uniform([n_dim, n_neurons_1], 0, 0.1))\n",
    "    bias1 = tf.Variable(tf.random_uniform([n_neurons_1], 0, 0.1))\n",
    "    \n",
    "    w2 = tf.Variable(tf.random_uniform([n_neurons_1, n_neurons_2], 0, 0.1))\n",
    "    bias2 = tf.Variable(tf.random_uniform([n_neurons_2], 0, 0.1))\n",
    "    \n",
    "    w3 = tf.Variable(tf.random_uniform([n_neurons_2, n_target], 0, 0.1))\n",
    "    bias3 = tf.Variable(tf.random_uniform([n_target], 0, 0.1))\n",
    "    \n",
    "    \n",
    "    hidden_1 = tf.nn.relu(tf.matmul(X, w1) + bias1)\n",
    "    hidden_2 = tf.nn.relu(tf.matmul(hidden_1, w2) + bias2)\n",
    "    y_ = tf.matmul(hidden_2, w3) + bias3\n",
    "    # initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    # Cost function\n",
    "    mse = tf.reduce_mean(tf.squared_difference(y_, Y))\n",
    "\n",
    "    # Optimizer\n",
    "    opt = tf.train.RMSPropOptimizer(0.0001, 0.99).minimize(mse)\n",
    "\n",
    "    possible_actions = train['a'].unique() #list(range(12))\n",
    "\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "#     exp_exp_tradeoff = np.random.rand() + 0.3\n",
    "\n",
    "#     # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "#     explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    # initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)  \n",
    "#         if (explore_probability >= exp_exp_tradeoff):\n",
    "#             # Make a random action (exploration)\n",
    "#             action = random.choice(possible_actions)\n",
    "\n",
    "#         else:\n",
    "            # Get action from Q-network (exploitation)\n",
    "            # Estimate the Qs values state\n",
    "        Qs = sess.run(y_, feed_dict = {X: state})\n",
    "        # Take the biggest Q value (the best action)\n",
    "        choice = max(np.argmax(Qs, axis = 1))\n",
    "        action = possible_actions[int(choice)]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning():\n",
    "    \n",
    "    avg_Q = []  \n",
    "    gamma = 0.9\n",
    "#     actions = actions_prep(df)\n",
    "\n",
    "# nepisode > https://stats.stackexchange.com/questions/250943/what-is-the-difference-between-episode-and-epoch-in-deep-q-learning\n",
    "# means one complete path from state, action, next_s, next_a, reward upuntil terminal state\n",
    "\n",
    "    nepisod = 2 #2 #np.shape(actions)[1]  ##22+ \n",
    "    \n",
    "    predicted_states = pd.DataFrame(train_lstm_model())\n",
    "\n",
    "    \n",
    "#     tuplesMx0, tuplesMx1, tuplesMx2, tuplesMx3, tuplesMx4, tuplesMx5, tuplesMx6, tuplesMx7, tuplesMx8, tuplesMx9, tuplesMx10, tuplesMx11 = tuple_(predicted_states) \n",
    "    \n",
    "    num_rows = np.shape(train)[0] #-1\n",
    "    \n",
    "    Q_predicted = np.zeros([num_rows, 12]) #np.random.random\n",
    "    \n",
    "    best_action = np.zeros([num_rows, 1])\n",
    "#     Q_optimal = np.zeros([nepisod, num_rows])\n",
    "    decay_step = 0 \n",
    "    \n",
    "    \n",
    "#     next_state = train[train.columns.difference(['rew', 'a', 'r0', 'f0', 'm0', 'ir0', 'if0'])]\n",
    "    state = train[train.columns.difference(['rew', 'a', 'r1', 'f1', 'm1', 'ir1', 'if1'])]\n",
    "    possible_actions = train['a'].unique()\n",
    "    \n",
    "    target_Qs_batch = []\n",
    "    memory = []\n",
    "    \n",
    "    nsteps = 10\n",
    "    \n",
    "    #episod: is a complete game from start to end \n",
    "    for i in range(nepisod):\n",
    "        episode_rewards = []\n",
    "        for j in range(nsteps):\n",
    "            action = predict_action(state, possible_actions)\n",
    "            # find the reward of the corresponding action\n",
    "            selected_a = train.loc[train['a'] == action, 'a']\n",
    "            row = np.random.choice(selected_a, 1)  \n",
    "            \n",
    "            # Get the rewards corresponding to that action\n",
    "            reward = np.array(train['rew'].iloc[row]) #game.make_action(action)  ## select the rewards corresponding to action a randomly \n",
    "\n",
    "            memory.append([state.iloc[row, :], action, predicted_states.iloc[row, :], reward])\n",
    "\n",
    "            if (len(memory) <= batch_size):\n",
    "                batch = pd.DataFrame(memory)\n",
    "            else:\n",
    "                batch = pd.DataFrame(random.sample(memory, batch_size)) # I can try np.random.permutation(memory)\n",
    "                \n",
    "            ## sample Random Mini-batch from D\n",
    "#             so, this is the problem .. i wanna split batch to set of columns .. \n",
    "            states_mb = batch.iloc[:, 0:4] #np.array([each for each in batch], ndmin = 5)\n",
    "            actions_mb = batch.loc[:, 4:5] #np.array([each[1] for each in batch])\n",
    "            rewards_mb = batch.loc[:, 6:6] #np.array([each[2] for each in batch]) \n",
    "            next_states_mb = batch.loc[:, 7:11] #np.array([each[3] for each in batch], ndmin = 5)  \n",
    "            dones_mb = random.randint(0, 1) #batch.loc[:, 12:12]  ##flag variable\n",
    "            \n",
    "#             next_states_mb = states_mb\n",
    "            \n",
    "            \n",
    "            Qs_next_state = DQN_train(next_states_mb) \n",
    "            print(max(Qs_next_state))\n",
    "#         for i in range(np.shape(batch)[0]):\n",
    "#             terminal = dones_mb\n",
    "            # If we are in a terminal state, only equals reward\n",
    "#             if dones_mb:\n",
    "#                 target_Qs_batch.append(rewards_mb.values[i])            \n",
    "#             else:\n",
    "#                 print(max(Qs_next_state))\n",
    "#                 target = rewards_mb[i] + gamma * max(Qs_next_state[i])\n",
    "#                 target_Qs_batch.append(target)                   \n",
    "#                 targets_mb = np.array([each for each in Qs_next_state])\n",
    "#                 best_action = np.argmax(targets_mb, axis = 1) \n",
    "                \n",
    "#             print(target_Qs_batch)\n",
    "\n",
    "    return best_action, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method TF_Output.<lambda> of <tensorflow.python.pywrap_tensorflow_internal.TF_Output; proxy of <Swig Object of type 'TF_Output *' at 0x000002714F2D02D0> >>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Eman\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 864, in <lambda>\n",
      "    __del__ = lambda self: None\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method TF_Output.<lambda> of <tensorflow.python.pywrap_tensorflow_internal.TF_Output; proxy of <Swig Object of type 'TF_Output *' at 0x000002714F2D02D0> >>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Eman\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 864, in <lambda>\n",
      "    __del__ = lambda self: None\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method TF_Output.<lambda> of <tensorflow.python.pywrap_tensorflow_internal.TF_Output; proxy of <Swig Object of type 'TF_Output *' at 0x000002714F2D02D0> >>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Eman\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 864, in <lambda>\n",
      "    __del__ = lambda self: None\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "best_action, _ = Q_learning()\n",
    "best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
