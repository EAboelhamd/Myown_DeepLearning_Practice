{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:108: UserWarning: DEPRECATION: If x is a vector, Softmax will not automatically pad x anymore in next releases. If you need it, please do it manually. The vector case is gonna be supported soon and the output will be a vector.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... training\n",
      "epoch 1, validation error 5.670000 %\n",
      "epoch 2, validation error 4.130000 %\n",
      "epoch 3, validation error 3.600000 %\n",
      "epoch 4, validation error 3.430000 %\n",
      "epoch 5, validation error 3.330000 %\n",
      "epoch 6, validation error 3.150000 %\n",
      "epoch 7, validation error 3.010000 %\n",
      "epoch 8, validation error 2.900000 %\n",
      "epoch 9, validation error 2.750000 %\n",
      "epoch 10, validation error 2.590000 %\n",
      "epoch 11, validation error 2.420000 %\n",
      "epoch 12, validation error 2.350000 %\n",
      "epoch 13, validation error 2.310000 %\n",
      "epoch 14, validation error 2.250000 %\n",
      "epoch 15, validation error 2.260000 %\n",
      "epoch 16, validation error 2.240000 %\n",
      "epoch 17, validation error 2.230000 %\n",
      "epoch 18, validation error 2.230000 %\n",
      "epoch 19, validation error 2.240000 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ef6c8dbb4f03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-ef6c8dbb4f03>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(learning_rate, L1_reg, L2_reg, n_epochs, dataset, n_hidden)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;31m# compute zero-one loss on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdev_examples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"A stripped-down MLP example, using Theano.\n",
    "\n",
    "Based on the tutorial here: http://deeplearning.net/tutorial/mlp.html\n",
    "\n",
    "This example trims away some complexities, and makes it easier to see how Theano works.\n",
    "\n",
    "Design changes:\n",
    "\n",
    "* Model compiled in a distinct function, so that symbolic variables are not in run-time scope.\n",
    "* No classes. Network shown by chained function calls.\n",
    "\n",
    "Some features of original have been dropped:\n",
    "\n",
    "* Inputs streamed to model, not pre-loaded as given\n",
    "* Minibatch size 1, i.e. `true' stochastic update\n",
    "* No early stopping\n",
    "\n",
    "Released under MIT license\n",
    "Copyright Matthew Honnibal, 2015.\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from os import path\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "\n",
    "def load_data(dataset):\n",
    "    ''' Loads the dataset\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: the path to the dataset (here MNIST)\n",
    "    '''\n",
    "    # Download the MNIST dataset if it is not present\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the data directory.\n",
    "        data_dir = os.path.join(os.path.split('__file__')[0], \"..\", \"data\")\n",
    "        if not path.exists(data_dir):\n",
    "            print \"No data directory to save data to. Try:\"\n",
    "            print \"mkdir ../data\"\n",
    "            sys.exit(1)\n",
    "        new_path = path.join(data_dir, data_file)\n",
    "        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "            dataset = new_path\n",
    "\n",
    "    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "        import urllib\n",
    "        url = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        print 'Downloading data from %s' % url\n",
    "        urllib.urlretrieve(url, dataset)\n",
    "\n",
    "    print '... loading data'\n",
    "\n",
    "    # Load the dataset\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "        train_set, valid_set, test_set = cPickle.load(f)\n",
    "    return _make_array(train_set), _make_array(valid_set), _make_array(test_set)\n",
    "\n",
    "\n",
    "def _make_array(xy):\n",
    "    data_x, data_y = xy\n",
    "    return zip(\n",
    "        numpy.asarray(data_x, dtype=theano.config.floatX),\n",
    "        numpy.asarray(data_y, dtype='int32'))\n",
    "\n",
    "\n",
    "def _init_logreg_weights(n_hidden, n_out):\n",
    "    weights = numpy.zeros((n_hidden, n_out), dtype=theano.config.floatX)\n",
    "    bias = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "    return (\n",
    "        theano.shared(name='W', borrow=True, value=weights),\n",
    "        theano.shared(name='b', borrow=True, value=bias)\n",
    "    )\n",
    "\n",
    "\n",
    "def _init_hidden_weights(n_in, n_out):\n",
    "    rng = numpy.random.RandomState(1234)\n",
    "    weights = numpy.asarray(\n",
    "        rng.uniform(\n",
    "            low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "            high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "            size=(n_in, n_out)\n",
    "        ),\n",
    "        dtype=theano.config.floatX\n",
    "    )\n",
    "    bias = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "    return (\n",
    "        theano.shared(value=weights, name='W', borrow=True),\n",
    "        theano.shared(value=bias, name='b', borrow=True)\n",
    "    )\n",
    "\n",
    "\n",
    "# Define how an input is fed through a layer of the network, and how a step of\n",
    "# the stochastic gradient descent is computed.\n",
    "    \n",
    "# Note that these are *symbolic expressions* --- we are just compiling code here.\n",
    "# These functions are only called during compile_model.  The *actual* feed-forward\n",
    "# and SGD update procedures, which happen iteratively on each example, are \n",
    "# Theano-internal.\n",
    "def feed_forward(activation, weights, bias, input_):\n",
    "    return activation(T.dot(input_, weights) + bias)\n",
    "\n",
    "def sgd_step(param, cost, learning_rate):\n",
    "    return param - (learning_rate * T.grad(cost, param))\n",
    "\n",
    "# These are also symbolic.\n",
    "def L1(L1_reg, w1, w2):\n",
    "    return L1_reg * (abs(w1).sum() + abs(w2).sum())\n",
    "\n",
    "\n",
    "def L2(L2_reg, w1, w2):\n",
    "    return L2_reg * ((w1 ** 2).sum() + (w2 ** 2).sum())\n",
    " \n",
    "\n",
    "def compile_model(n_in, n_classes, n_hidden, learning_rate, L1_reg, L2_reg):\n",
    "    '''Compile train and evaluation functions, which we'll then call iteratively\n",
    "    to train the parameters.  This function is called exactly once --- think of\n",
    "    it like a compiler.  We declare variables, allocate memory, and define some\n",
    "    computation.\n",
    "    '''\n",
    "    # allocate symbolic variables for the data\n",
    "    x = T.vector('x')  # Features\n",
    "    y = T.iscalar('y') # (Gold) Label\n",
    "    \n",
    "    # Allocate and initialize weights.  These are stored internally, and updated.\n",
    "    hidden_W, hidden_b = _init_hidden_weights(n_in, n_hidden)\n",
    "    logreg_W, logreg_b = _init_logreg_weights(n_hidden, n_classes)\n",
    "\n",
    "    # Estimate P(y | x) given the current weights\n",
    "    p_y_given_x = feed_forward(\n",
    "                      T.nnet.softmax,\n",
    "                      logreg_W,\n",
    "                      logreg_b,\n",
    "                      feed_forward(\n",
    "                          T.tanh,\n",
    "                          hidden_W,\n",
    "                          hidden_b,\n",
    "                          x)) # <--- Our input variable (the features)\n",
    "\n",
    "    cost = (\n",
    "        -T.log(p_y_given_x[0, y]) # <-- Negative log likelihood of gold label\n",
    "        + L1(L1_reg, logreg_W, hidden_W)\n",
    "        + L2(L2_reg, logreg_W, hidden_W)\n",
    "    )\n",
    "\n",
    "    # Compile the training function.  Successive calls to this update the weights.\n",
    "    # Internal state is maintained.\n",
    "    # The output is \"cost\", which requires the computation of p_y_given_x.  We\n",
    "    # also define how to update the weights based on the input label.\n",
    "    train_model = theano.function(\n",
    "        inputs=[x, y],\n",
    "        outputs=cost, # <-- Output depends on cost, which depends on P(y | x)\n",
    "        updates=[\n",
    "            (logreg_W, sgd_step(logreg_W, cost, learning_rate)),\n",
    "            (logreg_b, sgd_step(logreg_b, cost, learning_rate)),\n",
    "            (hidden_W, sgd_step(hidden_W, cost, learning_rate)),\n",
    "            (hidden_b, sgd_step(hidden_b, cost, learning_rate)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Compile the evaluation function, which returns a 0/1 loss wrt the true\n",
    "    # label.  Note that the output depends on p_y_given_x, so the program must\n",
    "    # compute it.\n",
    "    evaluate_model = theano.function(\n",
    "        inputs=[x, y],\n",
    "        outputs=T.neq(y, T.argmax(p_y_given_x[0])),\n",
    "    )\n",
    "    return train_model, evaluate_model\n",
    "\n",
    "\n",
    "def main(learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000,\n",
    "             dataset='mnist.pkl.gz', n_hidden=500):\n",
    "    train_examples, dev_examples, test_examples = load_data(dataset)\n",
    "    print '... building the model'\n",
    "    train_model, evaluate_model = compile_model(28*28, 10, n_hidden, learning_rate, L1_reg, L2_reg)\n",
    "    print '... training'\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        for x, y in train_examples:\n",
    "            train_model(x, y)\n",
    "        # compute zero-one loss on validation set\n",
    "        error = numpy.mean([evaluate_model(x, y) for x, y in dev_examples])\n",
    "        print('epoch %i, validation error %f %%' % (epoch, error * 100))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
